
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>kubelet: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">k8s.io/kubernetes/pkg/kubelet/active_deadline.go (77.3%)</option>
				
				<option value="file1">k8s.io/kubernetes/pkg/kubelet/kubelet.go (40.8%)</option>
				
				<option value="file2">k8s.io/kubernetes/pkg/kubelet/kubelet_getters.go (32.2%)</option>
				
				<option value="file3">k8s.io/kubernetes/pkg/kubelet/kubelet_network.go (40.0%)</option>
				
				<option value="file4">k8s.io/kubernetes/pkg/kubelet/kubelet_network_linux.go (0.0%)</option>
				
				<option value="file5">k8s.io/kubernetes/pkg/kubelet/kubelet_node_status.go (19.8%)</option>
				
				<option value="file6">k8s.io/kubernetes/pkg/kubelet/kubelet_node_status_others.go (0.0%)</option>
				
				<option value="file7">k8s.io/kubernetes/pkg/kubelet/kubelet_pods.go (50.7%)</option>
				
				<option value="file8">k8s.io/kubernetes/pkg/kubelet/kubelet_resources.go (0.0%)</option>
				
				<option value="file9">k8s.io/kubernetes/pkg/kubelet/kubelet_server_journal.go (0.0%)</option>
				
				<option value="file10">k8s.io/kubernetes/pkg/kubelet/kubelet_server_journal_linux.go (0.0%)</option>
				
				<option value="file11">k8s.io/kubernetes/pkg/kubelet/kubelet_volumes.go (44.4%)</option>
				
				<option value="file12">k8s.io/kubernetes/pkg/kubelet/pod_container_deletor.go (16.2%)</option>
				
				<option value="file13">k8s.io/kubernetes/pkg/kubelet/pod_workers.go (1.1%)</option>
				
				<option value="file14">k8s.io/kubernetes/pkg/kubelet/reason_cache.go (84.0%)</option>
				
				<option value="file15">k8s.io/kubernetes/pkg/kubelet/runonce.go (0.0%)</option>
				
				<option value="file16">k8s.io/kubernetes/pkg/kubelet/runtime.go (53.2%)</option>
				
				<option value="file17">k8s.io/kubernetes/pkg/kubelet/volume_host.go (14.8%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "fmt"
        "time"

        v1 "k8s.io/api/core/v1"
        "k8s.io/client-go/tools/record"
        "k8s.io/kubernetes/pkg/kubelet/lifecycle"
        "k8s.io/kubernetes/pkg/kubelet/status"
        "k8s.io/utils/clock"
)

const (
        reason  = "DeadlineExceeded"
        message = "Pod was active on the node longer than the specified deadline"
)

// activeDeadlineHandler knows how to enforce active deadlines on pods.
type activeDeadlineHandler struct {
        // the clock to use for deadline enforcement
        clock clock.Clock
        // the provider of pod status
        podStatusProvider status.PodStatusProvider
        // the recorder to dispatch events when we identify a pod has exceeded active deadline
        recorder record.EventRecorder
}

// newActiveDeadlineHandler returns an active deadline handler that can enforce pod active deadline
func newActiveDeadlineHandler(
        podStatusProvider status.PodStatusProvider,
        recorder record.EventRecorder,
        clock clock.Clock,
) (*activeDeadlineHandler, error) <span class="cov8" title="1">{

        // check for all required fields
        if clock == nil || podStatusProvider == nil || recorder == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("required arguments must not be nil: %v, %v, %v", clock, podStatusProvider, recorder)
        }</span>
        <span class="cov8" title="1">return &amp;activeDeadlineHandler{
                clock:             clock,
                podStatusProvider: podStatusProvider,
                recorder:          recorder,
        }, nil</span>
}

// NewActiveDeadlineHandler returns a new active deadline handler.
func NewActiveDeadlineHandler(
        podStatusProvider status.PodStatusProvider,
        recorder record.EventRecorder,
        clock clock.Clock,
) (*activeDeadlineHandler, error) <span class="cov0" title="0">{

        // check for all required fields
        if clock == nil || podStatusProvider == nil || recorder == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("required arguments must not be nil: %v, %v, %v", clock, podStatusProvider, recorder)
        }</span>
        <span class="cov0" title="0">return &amp;activeDeadlineHandler{
                clock:             clock,
                podStatusProvider: podStatusProvider,
                recorder:          recorder,
        }, nil</span>
}

// ShouldSync returns true if the pod is past its active deadline.
func (m *activeDeadlineHandler) ShouldSync(pod *v1.Pod) bool <span class="cov8" title="1">{
        return m.pastActiveDeadline(pod)
}</span>

// ShouldEvict returns true if the pod is past its active deadline.
// It dispatches an event that the pod should be evicted if it is past its deadline.
func (m *activeDeadlineHandler) ShouldEvict(pod *v1.Pod) lifecycle.ShouldEvictResponse <span class="cov8" title="1">{
        if !m.pastActiveDeadline(pod) </span><span class="cov8" title="1">{
                return lifecycle.ShouldEvictResponse{Evict: false}
        }</span>
        <span class="cov8" title="1">m.recorder.Eventf(pod, v1.EventTypeNormal, reason, message)
        return lifecycle.ShouldEvictResponse{Evict: true, Reason: reason, Message: message}</span>
}

// pastActiveDeadline returns true if the pod has been active for more than its ActiveDeadlineSeconds
func (m *activeDeadlineHandler) pastActiveDeadline(pod *v1.Pod) bool <span class="cov8" title="1">{
        // no active deadline was specified
        if pod.Spec.ActiveDeadlineSeconds == nil </span><span class="cov8" title="1">{
                return false
        }</span>
        // get the latest status to determine if it was started
        <span class="cov8" title="1">podStatus, ok := m.podStatusProvider.GetPodStatus(pod.UID)
        if !ok </span><span class="cov8" title="1">{
                podStatus = pod.Status
        }</span>
        // we have no start time so just return
        <span class="cov8" title="1">if podStatus.StartTime.IsZero() </span><span class="cov0" title="0">{
                return false
        }</span>
        // determine if the deadline was exceeded
        <span class="cov8" title="1">start := podStatus.StartTime.Time
        duration := m.clock.Since(start)
        allowedDuration := time.Duration(*pod.Spec.ActiveDeadlineSeconds) * time.Second
        return duration &gt;= allowedDuration</span>
}
</pre>
		
		<pre class="file" id="file1" style="display: none">/*
Copyright 2015 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "context"
        "crypto/tls"
        "fmt"
        "math"
        "net"
        "net/http"
        "os"
        "path/filepath"
        sysruntime "runtime"
        "sort"
        "sync"
        "sync/atomic"
        "time"

        cadvisorapi "github.com/google/cadvisor/info/v1"
        "github.com/google/go-cmp/cmp"
        "github.com/opencontainers/selinux/go-selinux"
        "go.opentelemetry.io/otel/attribute"
        semconv "go.opentelemetry.io/otel/semconv/v1.12.0"
        "go.opentelemetry.io/otel/trace"
        "k8s.io/client-go/informers"

        "k8s.io/mount-utils"
        netutils "k8s.io/utils/net"

        v1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/fields"
        "k8s.io/apimachinery/pkg/labels"
        "k8s.io/apimachinery/pkg/types"
        utilruntime "k8s.io/apimachinery/pkg/util/runtime"
        "k8s.io/apimachinery/pkg/util/sets"
        "k8s.io/apimachinery/pkg/util/wait"
        utilfeature "k8s.io/apiserver/pkg/util/feature"
        clientset "k8s.io/client-go/kubernetes"
        v1core "k8s.io/client-go/kubernetes/typed/core/v1"
        corelisters "k8s.io/client-go/listers/core/v1"
        "k8s.io/client-go/tools/cache"
        "k8s.io/client-go/tools/record"
        "k8s.io/client-go/util/certificate"
        "k8s.io/client-go/util/flowcontrol"
        cloudprovider "k8s.io/cloud-provider"
        "k8s.io/component-helpers/apimachinery/lease"
        internalapi "k8s.io/cri-api/pkg/apis"
        runtimeapi "k8s.io/cri-api/pkg/apis/runtime/v1"
        "k8s.io/klog/v2"
        pluginwatcherapi "k8s.io/kubelet/pkg/apis/pluginregistration/v1"
        statsapi "k8s.io/kubelet/pkg/apis/stats/v1alpha1"
        podutil "k8s.io/kubernetes/pkg/api/v1/pod"
        "k8s.io/kubernetes/pkg/api/v1/resource"
        "k8s.io/kubernetes/pkg/features"
        kubeletconfiginternal "k8s.io/kubernetes/pkg/kubelet/apis/config"
        "k8s.io/kubernetes/pkg/kubelet/apis/podresources"
        "k8s.io/kubernetes/pkg/kubelet/cadvisor"
        kubeletcertificate "k8s.io/kubernetes/pkg/kubelet/certificate"
        "k8s.io/kubernetes/pkg/kubelet/cloudresource"
        "k8s.io/kubernetes/pkg/kubelet/clustertrustbundle"
        "k8s.io/kubernetes/pkg/kubelet/cm"
        draplugin "k8s.io/kubernetes/pkg/kubelet/cm/dra/plugin"
        "k8s.io/kubernetes/pkg/kubelet/config"
        "k8s.io/kubernetes/pkg/kubelet/configmap"
        kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
        "k8s.io/kubernetes/pkg/kubelet/cri/remote"
        "k8s.io/kubernetes/pkg/kubelet/events"
        "k8s.io/kubernetes/pkg/kubelet/eviction"
        "k8s.io/kubernetes/pkg/kubelet/images"
        "k8s.io/kubernetes/pkg/kubelet/kuberuntime"
        "k8s.io/kubernetes/pkg/kubelet/lifecycle"
        "k8s.io/kubernetes/pkg/kubelet/logs"
        "k8s.io/kubernetes/pkg/kubelet/metrics"
        "k8s.io/kubernetes/pkg/kubelet/metrics/collectors"
        "k8s.io/kubernetes/pkg/kubelet/network/dns"
        "k8s.io/kubernetes/pkg/kubelet/nodeshutdown"
        oomwatcher "k8s.io/kubernetes/pkg/kubelet/oom"
        "k8s.io/kubernetes/pkg/kubelet/pleg"
        "k8s.io/kubernetes/pkg/kubelet/pluginmanager"
        plugincache "k8s.io/kubernetes/pkg/kubelet/pluginmanager/cache"
        kubepod "k8s.io/kubernetes/pkg/kubelet/pod"
        "k8s.io/kubernetes/pkg/kubelet/preemption"
        "k8s.io/kubernetes/pkg/kubelet/prober"
        proberesults "k8s.io/kubernetes/pkg/kubelet/prober/results"
        "k8s.io/kubernetes/pkg/kubelet/runtimeclass"
        "k8s.io/kubernetes/pkg/kubelet/secret"
        "k8s.io/kubernetes/pkg/kubelet/server"
        servermetrics "k8s.io/kubernetes/pkg/kubelet/server/metrics"
        serverstats "k8s.io/kubernetes/pkg/kubelet/server/stats"
        "k8s.io/kubernetes/pkg/kubelet/stats"
        "k8s.io/kubernetes/pkg/kubelet/status"
        "k8s.io/kubernetes/pkg/kubelet/sysctl"
        "k8s.io/kubernetes/pkg/kubelet/token"
        kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
        "k8s.io/kubernetes/pkg/kubelet/userns"
        "k8s.io/kubernetes/pkg/kubelet/userns/inuserns"
        "k8s.io/kubernetes/pkg/kubelet/util"
        "k8s.io/kubernetes/pkg/kubelet/util/manager"
        "k8s.io/kubernetes/pkg/kubelet/util/queue"
        "k8s.io/kubernetes/pkg/kubelet/util/sliceutils"
        "k8s.io/kubernetes/pkg/kubelet/volumemanager"
        httpprobe "k8s.io/kubernetes/pkg/probe/http"
        "k8s.io/kubernetes/pkg/security/apparmor"
        "k8s.io/kubernetes/pkg/util/oom"
        "k8s.io/kubernetes/pkg/volume"
        "k8s.io/kubernetes/pkg/volume/csi"
        "k8s.io/kubernetes/pkg/volume/util/hostutil"
        "k8s.io/kubernetes/pkg/volume/util/subpath"
        "k8s.io/kubernetes/pkg/volume/util/volumepathhandler"
        "k8s.io/utils/clock"
)

const (
        // Max amount of time to wait for the container runtime to come up.
        maxWaitForContainerRuntime = 30 * time.Second

        // MaxWaitForContainerRuntime for Max amount of time to wait for the container runtime to come up.
        MaxWaitForContainerRuntime = 30 * time.Second
        // nodeStatusUpdateRetry specifies how many times kubelet retries when posting node status failed.
        nodeStatusUpdateRetry = 5

        // nodeReadyGracePeriod is the period to allow for before fast status update is
        // terminated and container runtime not being ready is logged without verbosity guard.
        nodeReadyGracePeriod = 120 * time.Second

        // DefaultContainerLogsDir is the location of container logs.
        DefaultContainerLogsDir = "/var/log/containers"

        // MaxContainerBackOff is the max backoff period, exported for the e2e test
        MaxContainerBackOff = 300 * time.Second

        // Period for performing global cleanup tasks.
        housekeepingPeriod = time.Second * 2

        // Duration at which housekeeping failed to satisfy the invariant that
        // housekeeping should be fast to avoid blocking pod config (while
        // housekeeping is running no new pods are started or deleted).
        housekeepingWarningDuration = time.Second * 1

        // Period after which the runtime cache expires - set to slightly longer than
        // the expected length between housekeeping periods, which explicitly refreshes
        // the cache.
        runtimeCacheRefreshPeriod = housekeepingPeriod + housekeepingWarningDuration

        // Period for performing eviction monitoring.
        // ensure this is kept in sync with internal cadvisor housekeeping.
        evictionMonitoringPeriod = time.Second * 10

        // The path in containers' filesystems where the hosts file is mounted.
        linuxEtcHostsPath   = "/etc/hosts"
        windowsEtcHostsPath = "C:\\Windows\\System32\\drivers\\etc\\hosts"

        // Capacity of the channel for receiving pod lifecycle events. This number
        // is a bit arbitrary and may be adjusted in the future.
        plegChannelCapacity = 1000

        // Generic PLEG relies on relisting for discovering container events.
        // A longer period means that kubelet will take longer to detect container
        // changes and to update pod status. On the other hand, a shorter period
        // will cause more frequent relisting (e.g., container runtime operations),
        // leading to higher cpu usage.
        // Note that even though we set the period to 1s, the relisting itself can
        // take more than 1s to finish if the container runtime responds slowly
        // and/or when there are many container changes in one cycle.
        genericPlegRelistPeriod    = time.Second * 1
        genericPlegRelistThreshold = time.Minute * 3

        // Generic PLEG relist period and threshold when used with Evented PLEG.
        eventedPlegRelistPeriod     = time.Second * 300
        eventedPlegRelistThreshold  = time.Minute * 10
        eventedPlegMaxStreamRetries = 5

        // backOffPeriod is the period to back off when pod syncing results in an
        // error. It is also used as the base period for the exponential backoff
        // container restarts and image pulls.
        backOffPeriod = time.Second * 10

        // ContainerGCPeriod is the period for performing container garbage collection.
        ContainerGCPeriod = time.Minute
        // ImageGCPeriod is the period for performing image garbage collection.
        ImageGCPeriod = 5 * time.Minute

        // Minimum number of dead containers to keep in a pod
        minDeadContainerInPod = 1

        // nodeLeaseRenewIntervalFraction is the fraction of lease duration to renew the lease
        nodeLeaseRenewIntervalFraction = 0.25

        // instrumentationScope is the name of OpenTelemetry instrumentation scope
        instrumentationScope = "k8s.io/kubernetes/pkg/kubelet"
)

var (
        // ContainerLogsDir can be overwritten for testing usage
        ContainerLogsDir = DefaultContainerLogsDir
        etcHostsPath     = getContainerEtcHostsPath()
)

func getContainerEtcHostsPath() string <span class="cov8" title="1">{
        if sysruntime.GOOS == "windows" </span><span class="cov0" title="0">{
                return windowsEtcHostsPath
        }</span>
        <span class="cov8" title="1">return linuxEtcHostsPath</span>
}

// SyncHandler is an interface implemented by Kubelet, for testability
type SyncHandler interface {
        HandlePodAdditions(pods []*v1.Pod)
        HandlePodUpdates(pods []*v1.Pod)
        HandlePodRemoves(pods []*v1.Pod)
        HandlePodReconcile(pods []*v1.Pod)
        HandlePodSyncs(pods []*v1.Pod)
        HandlePodCleanups(ctx context.Context) error
}

// Option is a functional option type for Kubelet
type Option func(*Kubelet)

// Bootstrap is a bootstrapping interface for kubelet, targets the initialization protocol
type Bootstrap interface {
        GetConfiguration() kubeletconfiginternal.KubeletConfiguration
        BirthCry()
        StartGarbageCollection()
        ListenAndServe(kubeCfg *kubeletconfiginternal.KubeletConfiguration, tlsOptions *server.TLSOptions, auth server.AuthInterface, tp trace.TracerProvider)
        ListenAndServeReadOnly(address net.IP, port uint)
        ListenAndServePodResources()
        Run(&lt;-chan kubetypes.PodUpdate)
        RunOnce(&lt;-chan kubetypes.PodUpdate) ([]RunPodResult, error)
}

// Dependencies is a bin for things we might consider "injected dependencies" -- objects constructed
// at runtime that are necessary for running the Kubelet. This is a temporary solution for grouping
// these objects while we figure out a more comprehensive dependency injection story for the Kubelet.
type Dependencies struct {
        Options []Option

        // Injected Dependencies
        Auth                      server.AuthInterface
        CAdvisorInterface         cadvisor.Interface
        Cloud                     cloudprovider.Interface
        ContainerManager          cm.ContainerManager
        EventClient               v1core.EventsGetter
        HeartbeatClient           clientset.Interface
        OnHeartbeatFailure        func()
        KubeClient                clientset.Interface
        Mounter                   mount.Interface
        HostUtil                  hostutil.HostUtils
        OOMAdjuster               *oom.OOMAdjuster
        OSInterface               kubecontainer.OSInterface
        PodConfig                 *config.PodConfig
        ProbeManager              prober.Manager
        Recorder                  record.EventRecorder
        Subpather                 subpath.Interface
        TracerProvider            trace.TracerProvider
        VolumePlugins             []volume.VolumePlugin
        DynamicPluginProber       volume.DynamicPluginProber
        TLSOptions                *server.TLSOptions
        RemoteRuntimeService      internalapi.RuntimeService
        RemoteImageService        internalapi.ImageManagerService
        PodStartupLatencyTracker  util.PodStartupLatencyTracker
        NodeStartupLatencyTracker util.NodeStartupLatencyTracker
        // remove it after cadvisor.UsingLegacyCadvisorStats dropped.
        useLegacyCadvisorStats bool
}

// makePodSourceConfig creates a config.PodConfig from the given
// KubeletConfiguration or returns an error.
func makePodSourceConfig(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName, nodeHasSynced func() bool) (*config.PodConfig, error) <span class="cov8" title="1">{
        manifestURLHeader := make(http.Header)
        if len(kubeCfg.StaticPodURLHeader) &gt; 0 </span><span class="cov0" title="0">{
                for k, v := range kubeCfg.StaticPodURLHeader </span><span class="cov0" title="0">{
                        for i := range v </span><span class="cov0" title="0">{
                                manifestURLHeader.Add(k, v[i])
                        }</span>
                }
        }

        // source of all configuration
        <span class="cov8" title="1">cfg := config.NewPodConfig(config.PodConfigNotificationIncremental, kubeDeps.Recorder, kubeDeps.PodStartupLatencyTracker)

        // TODO:  it needs to be replaced by a proper context in the future
        ctx := context.TODO()

        // define file config source
        if kubeCfg.StaticPodPath != "" </span><span class="cov0" title="0">{
                klog.InfoS("Adding static pod path", "path", kubeCfg.StaticPodPath)
                config.NewSourceFile(kubeCfg.StaticPodPath, nodeName, kubeCfg.FileCheckFrequency.Duration, cfg.Channel(ctx, kubetypes.FileSource))
        }</span>

        // define url config source
        <span class="cov8" title="1">if kubeCfg.StaticPodURL != "" </span><span class="cov0" title="0">{
                klog.InfoS("Adding pod URL with HTTP header", "URL", kubeCfg.StaticPodURL, "header", manifestURLHeader)
                config.NewSourceURL(kubeCfg.StaticPodURL, manifestURLHeader, nodeName, kubeCfg.HTTPCheckFrequency.Duration, cfg.Channel(ctx, kubetypes.HTTPSource))
        }</span>

        <span class="cov8" title="1">if kubeDeps.KubeClient != nil </span><span class="cov0" title="0">{
                klog.InfoS("Adding apiserver pod source")
                config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, nodeHasSynced, cfg.Channel(ctx, kubetypes.ApiserverSource))
        }</span>
        <span class="cov8" title="1">return cfg, nil</span>
}

// PreInitRuntimeService will init runtime service before RunKubelet.
func PreInitRuntimeService(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies) error <span class="cov0" title="0">{
        remoteImageEndpoint := kubeCfg.ImageServiceEndpoint
        if remoteImageEndpoint == "" &amp;&amp; kubeCfg.ContainerRuntimeEndpoint != "" </span><span class="cov0" title="0">{
                remoteImageEndpoint = kubeCfg.ContainerRuntimeEndpoint
        }</span>
        <span class="cov0" title="0">var err error
        if kubeDeps.RemoteRuntimeService, err = remote.NewRemoteRuntimeService(kubeCfg.ContainerRuntimeEndpoint, kubeCfg.RuntimeRequestTimeout.Duration, kubeDeps.TracerProvider); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">if kubeDeps.RemoteImageService, err = remote.NewRemoteImageService(remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout.Duration, kubeDeps.TracerProvider); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">kubeDeps.useLegacyCadvisorStats = cadvisor.UsingLegacyCadvisorStats(kubeCfg.ContainerRuntimeEndpoint)

        return nil</span>
}

// NewMainKubelet instantiates a new Kubelet object along with all the required internal modules.
// No initialization of Kubelet and its modules should happen here.
func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,
        kubeDeps *Dependencies,
        crOptions *config.ContainerRuntimeOptions,
        hostname string,
        hostnameOverridden bool,
        nodeName types.NodeName,
        nodeIPs []net.IP,
        providerID string,
        cloudProvider string,
        certDirectory string,
        rootDirectory string,
        imageCredentialProviderConfigFile string,
        imageCredentialProviderBinDir string,
        registerNode bool,
        registerWithTaints []v1.Taint,
        allowedUnsafeSysctls []string,
        experimentalMounterPath string,
        kernelMemcgNotification bool,
        experimentalNodeAllocatableIgnoreEvictionThreshold bool,
        minimumGCAge metav1.Duration,
        maxPerPodContainerCount int32,
        maxContainerCount int32,
        registerSchedulable bool,
        keepTerminatedPodVolumes bool,
        nodeLabels map[string]string,
        nodeStatusMaxImages int32,
        seccompDefault bool,
) (*Kubelet, error) <span class="cov8" title="1">{
        ctx := context.Background()
        logger := klog.TODO()

        if rootDirectory == "" </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid root directory %q", rootDirectory)
        }</span>
        <span class="cov8" title="1">if kubeCfg.SyncFrequency.Duration &lt;= 0 </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("invalid sync frequency %d", kubeCfg.SyncFrequency.Duration)
        }</span>

        <span class="cov8" title="1">if utilfeature.DefaultFeatureGate.Enabled(features.DisableCloudProviders) &amp;&amp; cloudprovider.IsDeprecatedInternal(cloudProvider) </span><span class="cov0" title="0">{
                cloudprovider.DisableWarningForProvider(cloudProvider)
                return nil, fmt.Errorf("cloud provider %q was specified, but built-in cloud providers are disabled. Please set --cloud-provider=external and migrate to an external cloud provider", cloudProvider)
        }</span>

        <span class="cov8" title="1">var nodeHasSynced cache.InformerSynced
        var nodeLister corelisters.NodeLister

        // If kubeClient == nil, we are running in standalone mode (i.e. no API servers)
        // If not nil, we are running as part of a cluster and should sync w/API
        if kubeDeps.KubeClient != nil </span><span class="cov0" title="0">{
                kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0, informers.WithTweakListOptions(func(options *metav1.ListOptions) </span><span class="cov0" title="0">{
                        options.FieldSelector = fields.Set{metav1.ObjectNameField: string(nodeName)}.String()
                }</span>))
                <span class="cov0" title="0">nodeLister = kubeInformers.Core().V1().Nodes().Lister()
                nodeHasSynced = func() bool </span><span class="cov0" title="0">{
                        return kubeInformers.Core().V1().Nodes().Informer().HasSynced()
                }</span>
                <span class="cov0" title="0">kubeInformers.Start(wait.NeverStop)
                klog.InfoS("Attempting to sync node with API server")</span>
        } else<span class="cov8" title="1"> {
                // we don't have a client to sync!
                nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{})
                nodeLister = corelisters.NewNodeLister(nodeIndexer)
                nodeHasSynced = func() bool </span><span class="cov0" title="0">{ return true }</span>
                <span class="cov8" title="1">klog.InfoS("Kubelet is running in standalone mode, will skip API server sync")</span>
        }

        <span class="cov8" title="1">if kubeDeps.PodConfig == nil </span><span class="cov8" title="1">{
                var err error
                kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, nodeHasSynced)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
        }

        <span class="cov8" title="1">containerGCPolicy := kubecontainer.GCPolicy{
                MinAge:             minimumGCAge.Duration,
                MaxPerPodContainer: int(maxPerPodContainerCount),
                MaxContainers:      int(maxContainerCount),
        }

        daemonEndpoints := &amp;v1.NodeDaemonEndpoints{
                KubeletEndpoint: v1.DaemonEndpoint{Port: kubeCfg.Port},
        }

        imageGCPolicy := images.ImageGCPolicy{
                MinAge:               kubeCfg.ImageMinimumGCAge.Duration,
                HighThresholdPercent: int(kubeCfg.ImageGCHighThresholdPercent),
                LowThresholdPercent:  int(kubeCfg.ImageGCLowThresholdPercent),
        }

        if utilfeature.DefaultFeatureGate.Enabled(features.ImageMaximumGCAge) </span><span class="cov0" title="0">{
                imageGCPolicy.MaxAge = kubeCfg.ImageMaximumGCAge.Duration
        }</span> else<span class="cov8" title="1"> if kubeCfg.ImageMaximumGCAge.Duration != 0 </span><span class="cov0" title="0">{
                klog.InfoS("ImageMaximumGCAge flag enabled, but corresponding feature gate is not enabled. Ignoring flag.")
        }</span>

        <span class="cov8" title="1">enforceNodeAllocatable := kubeCfg.EnforceNodeAllocatable
        if experimentalNodeAllocatableIgnoreEvictionThreshold </span><span class="cov0" title="0">{
                // Do not provide kubeCfg.EnforceNodeAllocatable to eviction threshold parsing if we are not enforcing Evictions
                enforceNodeAllocatable = []string{}
        }</span>
        <span class="cov8" title="1">thresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">evictionConfig := eviction.Config{
                PressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration,
                MaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod),
                Thresholds:               thresholds,
                KernelMemcgNotification:  kernelMemcgNotification,
                PodCgroupRoot:            kubeDeps.ContainerManager.GetPodCgroupRoot(),
        }

        var serviceLister corelisters.ServiceLister
        var serviceHasSynced cache.InformerSynced
        if kubeDeps.KubeClient != nil </span><span class="cov0" title="0">{
                kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0)
                serviceLister = kubeInformers.Core().V1().Services().Lister()
                serviceHasSynced = kubeInformers.Core().V1().Services().Informer().HasSynced
                kubeInformers.Start(wait.NeverStop)
        }</span> else<span class="cov8" title="1"> {
                serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc})
                serviceLister = corelisters.NewServiceLister(serviceIndexer)
                serviceHasSynced = func() bool </span><span class="cov0" title="0">{ return true }</span>
        }

        // construct a node reference used for events
        <span class="cov8" title="1">nodeRef := &amp;v1.ObjectReference{
                Kind:      "Node",
                Name:      string(nodeName),
                UID:       types.UID(nodeName),
                Namespace: "",
        }

        oomWatcher, err := oomwatcher.NewWatcher(kubeDeps.Recorder)
        if err != nil </span><span class="cov0" title="0">{
                if inuserns.RunningInUserNS() </span><span class="cov0" title="0">{
                        if utilfeature.DefaultFeatureGate.Enabled(features.KubeletInUserNamespace) </span><span class="cov0" title="0">{
                                // oomwatcher.NewWatcher returns "open /dev/kmsg: operation not permitted" error,
                                // when running in a user namespace with sysctl value `kernel.dmesg_restrict=1`.
                                klog.V(2).InfoS("Failed to create an oomWatcher (running in UserNS, ignoring)", "err", err)
                                oomWatcher = nil
                        }</span> else<span class="cov0" title="0"> {
                                klog.ErrorS(err, "Failed to create an oomWatcher (running in UserNS, Hint: enable KubeletInUserNamespace feature flag to ignore the error)")
                                return nil, err
                        }</span>
                } else<span class="cov0" title="0"> {
                        return nil, err
                }</span>
        }

        <span class="cov8" title="1">clusterDNS := make([]net.IP, 0, len(kubeCfg.ClusterDNS))
        for _, ipEntry := range kubeCfg.ClusterDNS </span><span class="cov0" title="0">{
                ip := netutils.ParseIPSloppy(ipEntry)
                if ip == nil </span><span class="cov0" title="0">{
                        klog.InfoS("Invalid clusterDNS IP", "IP", ipEntry)
                }</span> else<span class="cov0" title="0"> {
                        clusterDNS = append(clusterDNS, ip)
                }</span>
        }

        // A TLS transport is needed to make HTTPS-based container lifecycle requests,
        // but we do not have the information necessary to do TLS verification.
        //
        // This client must not be modified to include credentials, because it is
        // critical that credentials not leak from the client to arbitrary hosts.
        <span class="cov8" title="1">insecureContainerLifecycleHTTPClient := &amp;http.Client{
                Transport: &amp;http.Transport{
                        TLSClientConfig: &amp;tls.Config{InsecureSkipVerify: true},
                },
                CheckRedirect: httpprobe.RedirectChecker(false),
        }

        tracer := kubeDeps.TracerProvider.Tracer(instrumentationScope)

        klet := &amp;Kubelet{
                hostname:                       hostname,
                hostnameOverridden:             hostnameOverridden,
                nodeName:                       nodeName,
                kubeClient:                     kubeDeps.KubeClient,
                heartbeatClient:                kubeDeps.HeartbeatClient,
                onRepeatedHeartbeatFailure:     kubeDeps.OnHeartbeatFailure,
                rootDirectory:                  filepath.Clean(rootDirectory),
                resyncInterval:                 kubeCfg.SyncFrequency.Duration,
                sourcesReady:                   config.NewSourcesReady(kubeDeps.PodConfig.SeenAllSources),
                registerNode:                   registerNode,
                registerWithTaints:             registerWithTaints,
                registerSchedulable:            registerSchedulable,
                dnsConfigurer:                  dns.NewConfigurer(kubeDeps.Recorder, nodeRef, nodeIPs, clusterDNS, kubeCfg.ClusterDomain, kubeCfg.ResolverConfig),
                serviceLister:                  serviceLister,
                serviceHasSynced:               serviceHasSynced,
                nodeLister:                     nodeLister,
                nodeHasSynced:                  nodeHasSynced,
                streamingConnectionIdleTimeout: kubeCfg.StreamingConnectionIdleTimeout.Duration,
                recorder:                       kubeDeps.Recorder,
                cadvisor:                       kubeDeps.CAdvisorInterface,
                cloud:                          kubeDeps.Cloud,
                externalCloudProvider:          cloudprovider.IsExternal(cloudProvider),
                providerID:                     providerID,
                nodeRef:                        nodeRef,
                nodeLabels:                     nodeLabels,
                nodeStatusUpdateFrequency:      kubeCfg.NodeStatusUpdateFrequency.Duration,
                nodeStatusReportFrequency:      kubeCfg.NodeStatusReportFrequency.Duration,
                os:                             kubeDeps.OSInterface,
                oomWatcher:                     oomWatcher,
                cgroupsPerQOS:                  kubeCfg.CgroupsPerQOS,
                cgroupRoot:                     kubeCfg.CgroupRoot,
                mounter:                        kubeDeps.Mounter,
                hostutil:                       kubeDeps.HostUtil,
                subpather:                      kubeDeps.Subpather,
                maxPods:                        int(kubeCfg.MaxPods),
                podsPerCore:                    int(kubeCfg.PodsPerCore),
                syncLoopMonitor:                atomic.Value{},
                daemonEndpoints:                daemonEndpoints,
                containerManager:               kubeDeps.ContainerManager,
                nodeIPs:                        nodeIPs,
                nodeIPValidator:                validateNodeIP,
                clock:                          clock.RealClock{},
                enableControllerAttachDetach:   kubeCfg.EnableControllerAttachDetach,
                makeIPTablesUtilChains:         kubeCfg.MakeIPTablesUtilChains,
                keepTerminatedPodVolumes:       keepTerminatedPodVolumes,
                nodeStatusMaxImages:            nodeStatusMaxImages,
                tracer:                         tracer,
                nodeStartupLatencyTracker:      kubeDeps.NodeStartupLatencyTracker,
        }

        if klet.cloud != nil </span><span class="cov0" title="0">{
                klet.cloudResourceSyncManager = cloudresource.NewSyncManager(klet.cloud, nodeName, klet.nodeStatusUpdateFrequency)
        }</span>

        <span class="cov8" title="1">var secretManager secret.Manager
        var configMapManager configmap.Manager
        if klet.kubeClient != nil </span><span class="cov0" title="0">{
                switch kubeCfg.ConfigMapAndSecretChangeDetectionStrategy </span>{
                case kubeletconfiginternal.WatchChangeDetectionStrategy:<span class="cov0" title="0">
                        secretManager = secret.NewWatchingSecretManager(klet.kubeClient, klet.resyncInterval)
                        configMapManager = configmap.NewWatchingConfigMapManager(klet.kubeClient, klet.resyncInterval)</span>
                case kubeletconfiginternal.TTLCacheChangeDetectionStrategy:<span class="cov0" title="0">
                        secretManager = secret.NewCachingSecretManager(
                                klet.kubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode))
                        configMapManager = configmap.NewCachingConfigMapManager(
                                klet.kubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode))</span>
                case kubeletconfiginternal.GetChangeDetectionStrategy:<span class="cov0" title="0">
                        secretManager = secret.NewSimpleSecretManager(klet.kubeClient)
                        configMapManager = configmap.NewSimpleConfigMapManager(klet.kubeClient)</span>
                default:<span class="cov0" title="0">
                        return nil, fmt.Errorf("unknown configmap and secret manager mode: %v", kubeCfg.ConfigMapAndSecretChangeDetectionStrategy)</span>
                }

                <span class="cov0" title="0">klet.secretManager = secretManager
                klet.configMapManager = configMapManager</span>
        }

        <span class="cov8" title="1">machineInfo, err := klet.cadvisor.MachineInfo()
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        // Avoid collector collects it as a timestamped metric
        // See PR #95210 and #97006 for more details.
        <span class="cov8" title="1">machineInfo.Timestamp = time.Time{}
        klet.setCachedMachineInfo(machineInfo)

        imageBackOff := flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff)

        klet.livenessManager = proberesults.NewManager()
        klet.readinessManager = proberesults.NewManager()
        klet.startupManager = proberesults.NewManager()
        klet.podCache = kubecontainer.NewCache()

        klet.mirrorPodClient = kubepod.NewBasicMirrorClient(klet.kubeClient, string(nodeName), nodeLister)
        klet.podManager = kubepod.NewBasicPodManager()

        klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet, kubeDeps.PodStartupLatencyTracker, klet.getRootDir())

        klet.resourceAnalyzer = serverstats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration, kubeDeps.Recorder)

        klet.runtimeService = kubeDeps.RemoteRuntimeService

        if kubeDeps.KubeClient != nil </span><span class="cov0" title="0">{
                klet.runtimeClassManager = runtimeclass.NewManager(kubeDeps.KubeClient)
        }</span>

        // setup containerLogManager for CRI container runtime
        <span class="cov8" title="1">containerLogManager, err := logs.NewContainerLogManager(
                klet.runtimeService,
                kubeDeps.OSInterface,
                kubeCfg.ContainerLogMaxSize,
                int(kubeCfg.ContainerLogMaxFiles),
                int(kubeCfg.ContainerLogMaxWorkers),
                kubeCfg.ContainerLogMonitorInterval,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to initialize container log manager: %v", err)
        }</span>
        <span class="cov8" title="1">klet.containerLogManager = containerLogManager

        klet.reasonCache = NewReasonCache()
        klet.workQueue = queue.NewBasicWorkQueue(klet.clock)
        klet.podWorkers = newPodWorkers(
                klet,
                kubeDeps.Recorder,
                klet.workQueue,
                klet.resyncInterval,
                backOffPeriod,
                klet.podCache,
        )

        runtime, err := kuberuntime.NewKubeGenericRuntimeManager(
                kubecontainer.FilterEventRecorder(kubeDeps.Recorder),
                klet.livenessManager,
                klet.readinessManager,
                klet.startupManager,
                rootDirectory,
                machineInfo,
                klet.podWorkers,
                kubeDeps.OSInterface,
                klet,
                insecureContainerLifecycleHTTPClient,
                imageBackOff,
                kubeCfg.SerializeImagePulls,
                kubeCfg.MaxParallelImagePulls,
                float32(kubeCfg.RegistryPullQPS),
                int(kubeCfg.RegistryBurst),
                imageCredentialProviderConfigFile,
                imageCredentialProviderBinDir,
                kubeCfg.CPUCFSQuota,
                kubeCfg.CPUCFSQuotaPeriod,
                kubeDeps.RemoteRuntimeService,
                kubeDeps.RemoteImageService,
                kubeDeps.ContainerManager,
                klet.containerLogManager,
                klet.runtimeClassManager,
                seccompDefault,
                kubeCfg.MemorySwap.SwapBehavior,
                kubeDeps.ContainerManager.GetNodeAllocatableAbsolute,
                *kubeCfg.MemoryThrottlingFactor,
                kubeDeps.PodStartupLatencyTracker,
                kubeDeps.TracerProvider,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">klet.containerRuntime = runtime
        klet.streamingRuntime = runtime
        klet.runner = runtime

        runtimeCache, err := kubecontainer.NewRuntimeCache(klet.containerRuntime, runtimeCacheRefreshPeriod)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">klet.runtimeCache = runtimeCache

        // common provider to get host file system usage associated with a pod managed by kubelet
        hostStatsProvider := stats.NewHostStatsProvider(kubecontainer.RealOS{}, func(podUID types.UID) string </span><span class="cov0" title="0">{
                return getEtcHostsPath(klet.getPodDir(podUID))
        }</span>)
        <span class="cov8" title="1">if kubeDeps.useLegacyCadvisorStats </span><span class="cov0" title="0">{
                klet.StatsProvider = stats.NewCadvisorStatsProvider(
                        klet.cadvisor,
                        klet.resourceAnalyzer,
                        klet.podManager,
                        klet.runtimeCache,
                        klet.containerRuntime,
                        klet.statusManager,
                        hostStatsProvider)
        }</span> else<span class="cov8" title="1"> {
                klet.StatsProvider = stats.NewCRIStatsProvider(
                        klet.cadvisor,
                        klet.resourceAnalyzer,
                        klet.podManager,
                        klet.runtimeCache,
                        kubeDeps.RemoteRuntimeService,
                        kubeDeps.RemoteImageService,
                        hostStatsProvider,
                        utilfeature.DefaultFeatureGate.Enabled(features.PodAndContainerStatsFromCRI))
        }</span>

        <span class="cov8" title="1">eventChannel := make(chan *pleg.PodLifecycleEvent, plegChannelCapacity)

        if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) </span><span class="cov0" title="0">{
                // adjust Generic PLEG relisting period and threshold to higher value when Evented PLEG is turned on
                genericRelistDuration := &amp;pleg.RelistDuration{
                        RelistPeriod:    eventedPlegRelistPeriod,
                        RelistThreshold: eventedPlegRelistThreshold,
                }
                klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, clock.RealClock{})
                // In case Evented PLEG has to fall back on Generic PLEG due to an error,
                // Evented PLEG should be able to reset the Generic PLEG relisting duration
                // to the default value.
                eventedRelistDuration := &amp;pleg.RelistDuration{
                        RelistPeriod:    genericPlegRelistPeriod,
                        RelistThreshold: genericPlegRelistThreshold,
                }
                klet.eventedPleg, err = pleg.NewEventedPLEG(klet.containerRuntime, klet.runtimeService, eventChannel,
                        klet.podCache, klet.pleg, eventedPlegMaxStreamRetries, eventedRelistDuration, clock.RealClock{})
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
        } else<span class="cov8" title="1"> {
                genericRelistDuration := &amp;pleg.RelistDuration{
                        RelistPeriod:    genericPlegRelistPeriod,
                        RelistThreshold: genericPlegRelistThreshold,
                }
                klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, clock.RealClock{})
        }</span>

        <span class="cov8" title="1">klet.runtimeState = newRuntimeState(maxWaitForContainerRuntime)
        klet.runtimeState.addHealthCheck("PLEG", klet.pleg.Healthy)
        if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) </span><span class="cov0" title="0">{
                klet.runtimeState.addHealthCheck("EventedPLEG", klet.eventedPleg.Healthy)
        }</span>
        <span class="cov8" title="1">if _, err := klet.updatePodCIDR(ctx, kubeCfg.PodCIDR); err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Pod CIDR update failed")
        }</span>

        // setup containerGC
        <span class="cov8" title="1">containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">klet.containerGC = containerGC
        klet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, max(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod))

        // setup imageManager
        imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, kubeDeps.TracerProvider)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to initialize image manager: %v", err)
        }</span>
        <span class="cov8" title="1">klet.imageManager = imageManager

        if kubeCfg.ServerTLSBootstrap &amp;&amp; kubeDeps.TLSOptions != nil &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.RotateKubeletServerCertificate) </span><span class="cov0" title="0">{
                klet.serverCertificateManager, err = kubeletcertificate.NewKubeletServerCertificateManager(klet.kubeClient, kubeCfg, klet.nodeName, klet.getLastObservedNodeAddresses, certDirectory)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to initialize certificate manager: %v", err)
                }</span>
                <span class="cov0" title="0">kubeDeps.TLSOptions.Config.GetCertificate = func(*tls.ClientHelloInfo) (*tls.Certificate, error) </span><span class="cov0" title="0">{
                        cert := klet.serverCertificateManager.Current()
                        if cert == nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("no serving certificate available for the kubelet")
                        }</span>
                        <span class="cov0" title="0">return cert, nil</span>
                }
        }

        <span class="cov8" title="1">if kubeDeps.ProbeManager != nil </span><span class="cov0" title="0">{
                klet.probeManager = kubeDeps.ProbeManager
        }</span> else<span class="cov8" title="1"> {
                klet.probeManager = prober.NewManager(
                        klet.statusManager,
                        klet.livenessManager,
                        klet.readinessManager,
                        klet.startupManager,
                        klet.runner,
                        kubeDeps.Recorder)
        }</span>

        <span class="cov8" title="1">tokenManager := token.NewManager(kubeDeps.KubeClient)

        var clusterTrustBundleManager clustertrustbundle.Manager
        if kubeDeps.KubeClient != nil &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.ClusterTrustBundleProjection) </span><span class="cov0" title="0">{
                kubeInformers := informers.NewSharedInformerFactoryWithOptions(kubeDeps.KubeClient, 0)
                clusterTrustBundleManager, err = clustertrustbundle.NewInformerManager(kubeInformers.Certificates().V1alpha1().ClusterTrustBundles(), 2*int(kubeCfg.MaxPods), 5*time.Minute)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("while starting informer-based ClusterTrustBundle manager: %w", err)
                }</span>
                <span class="cov0" title="0">kubeInformers.Start(wait.NeverStop)
                klog.InfoS("Started ClusterTrustBundle informer")</span>
        } else<span class="cov8" title="1"> {
                // In static kubelet mode, use a no-op manager.
                clusterTrustBundleManager = &amp;clustertrustbundle.NoopManager{}
                klog.InfoS("Not starting ClusterTrustBundle informer because we are in static kubelet mode")
        }</span>

        // NewInitializedVolumePluginMgr initializes some storageErrors on the Kubelet runtimeState (in csi_plugin.go init)
        // which affects node ready status. This function must be called before Kubelet is initialized so that the Node
        // ReadyState is accurate with the storage state.
        <span class="cov8" title="1">klet.volumePluginMgr, err =
                NewInitializedVolumePluginMgr(
                        klet,
                        secretManager,
                        configMapManager,
                        tokenManager,
                        clusterTrustBundleManager,
                        kubeDeps.VolumePlugins,
                        kubeDeps.DynamicPluginProber)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">klet.pluginManager = pluginmanager.NewPluginManager(
                klet.getPluginsRegistrationDir(), /* sockDir */
                kubeDeps.Recorder,
        )

        // If the experimentalMounterPathFlag is set, we do not want to
        // check node capabilities since the mount path is not the default
        if len(experimentalMounterPath) != 0 </span><span class="cov0" title="0">{
                // Replace the nameserver in containerized-mounter's rootfs/etc/resolv.conf with kubelet.ClusterDNS
                // so that service name could be resolved
                klet.dnsConfigurer.SetupDNSinContainerizedMounter(experimentalMounterPath)
        }</span>

        // setup volumeManager
        <span class="cov8" title="1">klet.volumeManager = volumemanager.NewVolumeManager(
                kubeCfg.EnableControllerAttachDetach,
                nodeName,
                klet.podManager,
                klet.podWorkers,
                klet.kubeClient,
                klet.volumePluginMgr,
                klet.containerRuntime,
                kubeDeps.Mounter,
                kubeDeps.HostUtil,
                klet.getPodsDir(),
                kubeDeps.Recorder,
                keepTerminatedPodVolumes,
                volumepathhandler.NewBlockVolumePathHandler())

        klet.backOff = flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff)

        // setup eviction manager
        evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig,
                killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock, kubeCfg.LocalStorageCapacityIsolation)

        klet.evictionManager = evictionManager
        klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler)

        // Safe, allowed sysctls can always be used as unsafe sysctls in the spec.
        // Hence, we concatenate those two lists.
        safeAndUnsafeSysctls := append(sysctl.SafeSysctlAllowlist(), allowedUnsafeSysctls...)
        sysctlsAllowlist, err := sysctl.NewAllowlist(safeAndUnsafeSysctls)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">klet.admitHandlers.AddPodAdmitHandler(sysctlsAllowlist)

        // enable active deadline handler
        activeDeadlineHandler, err := newActiveDeadlineHandler(klet.statusManager, kubeDeps.Recorder, klet.clock)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">klet.AddPodSyncLoopHandler(activeDeadlineHandler)
        klet.AddPodSyncHandler(activeDeadlineHandler)

        klet.admitHandlers.AddPodAdmitHandler(klet.containerManager.GetAllocateResourcesPodAdmitHandler())

        criticalPodAdmissionHandler := preemption.NewCriticalPodAdmissionHandler(klet.GetActivePods, killPodNow(klet.podWorkers, kubeDeps.Recorder), kubeDeps.Recorder)
        klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewPredicateAdmitHandler(klet.getNodeAnyWay, criticalPodAdmissionHandler, klet.containerManager.UpdatePluginResources))
        // apply functional Option's
        for _, opt := range kubeDeps.Options </span><span class="cov0" title="0">{
                opt(klet)
        }</span>

        <span class="cov8" title="1">if sysruntime.GOOS == "linux" </span><span class="cov8" title="1">{
                // AppArmor is a Linux kernel security module and it does not support other operating systems.
                klet.appArmorValidator = apparmor.NewValidator()
                klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewAppArmorAdmitHandler(klet.appArmorValidator))
        }</span>

        <span class="cov8" title="1">leaseDuration := time.Duration(kubeCfg.NodeLeaseDurationSeconds) * time.Second
        renewInterval := time.Duration(float64(leaseDuration) * nodeLeaseRenewIntervalFraction)
        klet.nodeLeaseController = lease.NewController(
                klet.clock,
                klet.heartbeatClient,
                string(klet.nodeName),
                kubeCfg.NodeLeaseDurationSeconds,
                klet.onRepeatedHeartbeatFailure,
                renewInterval,
                string(klet.nodeName),
                v1.NamespaceNodeLease,
                util.SetNodeOwnerFunc(klet.heartbeatClient, string(klet.nodeName)))

        // setup node shutdown manager
        shutdownManager, shutdownAdmitHandler := nodeshutdown.NewManager(&amp;nodeshutdown.Config{
                Logger:                           logger,
                ProbeManager:                     klet.probeManager,
                Recorder:                         kubeDeps.Recorder,
                NodeRef:                          nodeRef,
                GetPodsFunc:                      klet.GetActivePods,
                KillPodFunc:                      killPodNow(klet.podWorkers, kubeDeps.Recorder),
                SyncNodeStatusFunc:               klet.syncNodeStatus,
                ShutdownGracePeriodRequested:     kubeCfg.ShutdownGracePeriod.Duration,
                ShutdownGracePeriodCriticalPods:  kubeCfg.ShutdownGracePeriodCriticalPods.Duration,
                ShutdownGracePeriodByPodPriority: kubeCfg.ShutdownGracePeriodByPodPriority,
                StateDirectory:                   rootDirectory,
        })
        klet.shutdownManager = shutdownManager
        klet.usernsManager, err = userns.MakeUserNsManager(klet)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">klet.admitHandlers.AddPodAdmitHandler(shutdownAdmitHandler)

        // Finally, put the most recent version of the config on the Kubelet, so
        // people can see how it was configured.
        klet.kubeletConfiguration = *kubeCfg

        // Generating the status funcs should be the last thing we do,
        // since this relies on the rest of the Kubelet having been constructed.
        klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs()

        return klet, nil</span>
}

type serviceLister interface {
        List(labels.Selector) ([]*v1.Service, error)
}

// Get InstrumenationScope returns the instrumentation scope for the Kubelet

// Kubelet is the main kubelet implementation.
type Kubelet struct {
        kubeletConfiguration kubeletconfiginternal.KubeletConfiguration

        // hostname is the hostname the kubelet detected or was given via flag/config
        hostname string
        // hostnameOverridden indicates the hostname was overridden via flag/config
        hostnameOverridden bool

        nodeName        types.NodeName
        runtimeCache    kubecontainer.RuntimeCache
        kubeClient      clientset.Interface
        heartbeatClient clientset.Interface
        // mirrorPodClient is used to create and delete mirror pods in the API for static
        // pods.
        mirrorPodClient kubepod.MirrorClient

        rootDirectory string

        lastObservedNodeAddressesMux sync.RWMutex
        lastObservedNodeAddresses    []v1.NodeAddress

        // onRepeatedHeartbeatFailure is called when a heartbeat operation fails more than once. optional.
        onRepeatedHeartbeatFailure func()

        // podManager stores the desired set of admitted pods and mirror pods that the kubelet should be
        // running. The actual set of running pods is stored on the podWorkers. The manager is populated
        // by the kubelet config loops which abstracts receiving configuration from many different sources
        // (api for regular pods, local filesystem or http for static pods). The manager may be consulted
        // by other components that need to see the set of desired pods. Note that not all desired pods are
        // running, and not all running pods are in the podManager - for instance, force deleting a pod
        // from the apiserver will remove it from the podManager, but the pod may still be terminating and
        // tracked by the podWorkers. Components that need to know the actual consumed resources of the
        // node or are driven by podWorkers and the sync*Pod methods (status, volume, stats) should also
        // consult the podWorkers when reconciling.
        //
        // TODO: review all kubelet components that need the actual set of pods (vs the desired set)
        // and update them to use podWorkers instead of podManager. This may introduce latency in some
        // methods, but avoids race conditions and correctly accounts for terminating pods that have
        // been force deleted or static pods that have been updated.
        // https://github.com/kubernetes/kubernetes/issues/116970
        podManager kubepod.Manager

        // podWorkers is responsible for driving the lifecycle state machine of each pod. The worker is
        // notified of config changes, updates, periodic reconciliation, container runtime updates, and
        // evictions of all desired pods and will invoke reconciliation methods per pod in separate
        // goroutines. The podWorkers are authoritative in the kubelet for what pods are actually being
        // run and their current state:
        //
        // * syncing: pod should be running (syncPod)
        // * terminating: pod should be stopped (syncTerminatingPod)
        // * terminated: pod should have all resources cleaned up (syncTerminatedPod)
        //
        // and invoke the handler methods that correspond to each state. Components within the
        // kubelet that need to know the phase of the pod in order to correctly set up or tear down
        // resources must consult the podWorkers.
        //
        // Once a pod has been accepted by the pod workers, no other pod with that same UID (and
        // name+namespace, for static pods) will be started until the first pod has fully terminated
        // and been cleaned up by SyncKnownPods. This means a pod may be desired (in API), admitted
        // (in pod manager), and requested (by invoking UpdatePod) but not start for an arbitrarily
        // long interval because a prior pod is still terminating.
        //
        // As an event-driven (by UpdatePod) controller, the podWorkers must periodically be resynced
        // by the kubelet invoking SyncKnownPods with the desired state (admitted pods in podManager).
        // Since the podManager may be unaware of some running pods due to force deletion, the
        // podWorkers are responsible for triggering a sync of pods that are no longer desired but
        // must still run to completion.
        podWorkers PodWorkers

        // evictionManager observes the state of the node for situations that could impact node stability
        // and evicts pods (sets to phase Failed with reason Evicted) to reduce resource pressure. The
        // eviction manager acts on the actual state of the node and considers the podWorker to be
        // authoritative.
        evictionManager eviction.Manager

        // probeManager tracks the set of running pods and ensures any user-defined periodic checks are
        // run to introspect the state of each pod.  The probe manager acts on the actual state of the node
        // and is notified of pods by the podWorker. The probe manager is the authoritative source of the
        // most recent probe status and is responsible for notifying the status manager, which
        // synthesizes them into the overall pod status.
        probeManager prober.Manager

        // secretManager caches the set of secrets used by running pods on this node. The podWorkers
        // notify the secretManager when pods are started and terminated, and the secretManager must
        // then keep the needed secrets up-to-date as they change.
        secretManager secret.Manager

        // configMapManager caches the set of config maps used by running pods on this node. The
        // podWorkers notify the configMapManager when pods are started and terminated, and the
        // configMapManager must then keep the needed config maps up-to-date as they change.
        configMapManager configmap.Manager

        // volumeManager observes the set of running pods and is responsible for attaching, mounting,
        // unmounting, and detaching as those pods move through their lifecycle. It periodically
        // synchronizes the set of known volumes to the set of actually desired volumes and cleans up
        // any orphaned volumes. The volume manager considers the podWorker to be authoritative for
        // which pods are running.
        volumeManager volumemanager.VolumeManager

        // statusManager receives updated pod status updates from the podWorker and updates the API
        // status of those pods to match. The statusManager is authoritative for the synthesized
        // status of the pod from the kubelet's perspective (other components own the individual
        // elements of status) and should be consulted by components in preference to assembling
        // that status themselves. Note that the status manager is downstream of the pod worker
        // and components that need to check whether a pod is still running should instead directly
        // consult the pod worker.
        statusManager status.Manager

        // resyncInterval is the interval between periodic full reconciliations of
        // pods on this node.
        resyncInterval time.Duration

        // sourcesReady records the sources seen by the kubelet, it is thread-safe.
        sourcesReady config.SourcesReady

        // Optional, defaults to /logs/ from /var/log
        logServer http.Handler
        // Optional, defaults to simple Docker implementation
        runner kubecontainer.CommandRunner

        // cAdvisor used for container information.
        cadvisor cadvisor.Interface

        // Set to true to have the node register itself with the apiserver.
        registerNode bool
        // List of taints to add to a node object when the kubelet registers itself.
        registerWithTaints []v1.Taint
        // Set to true to have the node register itself as schedulable.
        registerSchedulable bool
        // for internal book keeping; access only from within registerWithApiserver
        registrationCompleted bool

        // dnsConfigurer is used for setting up DNS resolver configuration when launching pods.
        dnsConfigurer *dns.Configurer

        // serviceLister knows how to list services
        serviceLister serviceLister
        // serviceHasSynced indicates whether services have been sync'd at least once.
        // Check this before trusting a response from the lister.
        serviceHasSynced cache.InformerSynced
        // nodeLister knows how to list nodes
        nodeLister corelisters.NodeLister
        // nodeHasSynced indicates whether nodes have been sync'd at least once.
        // Check this before trusting a response from the node lister.
        nodeHasSynced cache.InformerSynced
        // a list of node labels to register
        nodeLabels map[string]string

        // Last timestamp when runtime responded on ping.
        // Mutex is used to protect this value.
        runtimeState *runtimeState

        // Volume plugins.
        volumePluginMgr *volume.VolumePluginMgr

        // Manages container health check results.
        livenessManager  proberesults.Manager
        readinessManager proberesults.Manager
        startupManager   proberesults.Manager

        // How long to keep idle streaming command execution/port forwarding
        // connections open before terminating them
        streamingConnectionIdleTimeout time.Duration

        // The EventRecorder to use
        recorder record.EventRecorder

        // Policy for handling garbage collection of dead containers.
        containerGC kubecontainer.GC

        // Manager for image garbage collection.
        imageManager images.ImageGCManager

        // Manager for container logs.
        containerLogManager logs.ContainerLogManager

        // Cached MachineInfo returned by cadvisor.
        machineInfoLock sync.RWMutex
        machineInfo     *cadvisorapi.MachineInfo

        // Handles certificate rotations.
        serverCertificateManager certificate.Manager

        // Cloud provider interface.
        cloud cloudprovider.Interface
        // Handles requests to cloud provider with timeout
        cloudResourceSyncManager cloudresource.SyncManager

        // Indicates that the node initialization happens in an external cloud controller
        externalCloudProvider bool
        // Reference to this node.
        nodeRef *v1.ObjectReference

        // Container runtime.
        containerRuntime kubecontainer.Runtime

        // Streaming runtime handles container streaming.
        streamingRuntime kubecontainer.StreamingRuntime

        // Container runtime service (needed by container runtime Start()).
        runtimeService internalapi.RuntimeService

        // reasonCache caches the failure reason of the last creation of all containers, which is
        // used for generating ContainerStatus.
        reasonCache *ReasonCache

        // containerRuntimeReadyExpected indicates whether container runtime being ready is expected
        // so errors are logged without verbosity guard, to avoid excessive error logs at node startup.
        // It's false during the node initialization period of nodeReadyGracePeriod, and after that
        // it's set to true by fastStatusUpdateOnce when it exits.
        containerRuntimeReadyExpected bool

        // nodeStatusUpdateFrequency specifies how often kubelet computes node status. If node lease
        // feature is not enabled, it is also the frequency that kubelet posts node status to master.
        // In that case, be cautious when changing the constant, it must work with nodeMonitorGracePeriod
        // in nodecontroller. There are several constraints:
        // 1. nodeMonitorGracePeriod must be N times more than nodeStatusUpdateFrequency, where
        //    N means number of retries allowed for kubelet to post node status. It is pointless
        //    to make nodeMonitorGracePeriod be less than nodeStatusUpdateFrequency, since there
        //    will only be fresh values from Kubelet at an interval of nodeStatusUpdateFrequency.
        //    The constant must be less than podEvictionTimeout.
        // 2. nodeStatusUpdateFrequency needs to be large enough for kubelet to generate node
        //    status. Kubelet may fail to update node status reliably if the value is too small,
        //    as it takes time to gather all necessary node information.
        nodeStatusUpdateFrequency time.Duration

        // nodeStatusReportFrequency is the frequency that kubelet posts node
        // status to master. It is only used when node lease feature is enabled.
        nodeStatusReportFrequency time.Duration

        // lastStatusReportTime is the time when node status was last reported.
        lastStatusReportTime time.Time

        // syncNodeStatusMux is a lock on updating the node status, because this path is not thread-safe.
        // This lock is used by Kubelet.syncNodeStatus and Kubelet.fastNodeStatusUpdate functions and shouldn't be used anywhere else.
        syncNodeStatusMux sync.Mutex

        // updatePodCIDRMux is a lock on updating pod CIDR, because this path is not thread-safe.
        // This lock is used by Kubelet.updatePodCIDR function and shouldn't be used anywhere else.
        updatePodCIDRMux sync.Mutex

        // updateRuntimeMux is a lock on updating runtime, because this path is not thread-safe.
        // This lock is used by Kubelet.updateRuntimeUp and Kubelet.fastNodeStatusUpdate functions and shouldn't be used anywhere else.
        updateRuntimeMux sync.Mutex

        // nodeLeaseController claims and renews the node lease for this Kubelet
        nodeLeaseController lease.Controller

        // pleg observes the state of the container runtime and notifies the kubelet of changes to containers, which
        // notifies the podWorkers to reconcile the state of the pod (for instance, if a container dies and needs to
        // be restarted).
        pleg pleg.PodLifecycleEventGenerator

        // eventedPleg supplements the pleg to deliver edge-driven container changes with low-latency.
        eventedPleg pleg.PodLifecycleEventGenerator

        // Store kubecontainer.PodStatus for all pods.
        podCache kubecontainer.Cache

        // os is a facade for various syscalls that need to be mocked during testing.
        os kubecontainer.OSInterface

        // Watcher of out of memory events.
        oomWatcher oomwatcher.Watcher

        // Monitor resource usage
        resourceAnalyzer serverstats.ResourceAnalyzer

        // Whether or not we should have the QOS cgroup hierarchy for resource management
        cgroupsPerQOS bool

        // If non-empty, pass this to the container runtime as the root cgroup.
        cgroupRoot string

        // Mounter to use for volumes.
        mounter mount.Interface

        // hostutil to interact with filesystems
        hostutil hostutil.HostUtils

        // subpather to execute subpath actions
        subpather subpath.Interface

        // Manager of non-Runtime containers.
        containerManager cm.ContainerManager

        // Maximum Number of Pods which can be run by this Kubelet
        maxPods int

        // Monitor Kubelet's sync loop
        syncLoopMonitor atomic.Value

        // Container restart Backoff
        backOff *flowcontrol.Backoff

        // Information about the ports which are opened by daemons on Node running this Kubelet server.
        daemonEndpoints *v1.NodeDaemonEndpoints

        // A queue used to trigger pod workers.
        workQueue queue.WorkQueue

        // oneTimeInitializer is used to initialize modules that are dependent on the runtime to be up.
        oneTimeInitializer sync.Once

        // If set, use this IP address or addresses for the node
        nodeIPs []net.IP

        // use this function to validate the kubelet nodeIP
        nodeIPValidator func(net.IP) error

        // If non-nil, this is a unique identifier for the node in an external database, eg. cloudprovider
        providerID string

        // clock is an interface that provides time related functionality in a way that makes it
        // easy to test the code.
        clock clock.WithTicker

        // handlers called during the tryUpdateNodeStatus cycle
        setNodeStatusFuncs []func(context.Context, *v1.Node) error

        lastNodeUnschedulableLock sync.Mutex
        // maintains Node.Spec.Unschedulable value from previous run of tryUpdateNodeStatus()
        lastNodeUnschedulable bool

        // the list of handlers to call during pod admission.
        admitHandlers lifecycle.PodAdmitHandlers

        // softAdmithandlers are applied to the pod after it is admitted by the Kubelet, but before it is
        // run. A pod rejected by a softAdmitHandler will be left in a Pending state indefinitely. If a
        // rejected pod should not be recreated, or the scheduler is not aware of the rejection rule, the
        // admission rule should be applied by a softAdmitHandler.
        softAdmitHandlers lifecycle.PodAdmitHandlers

        // the list of handlers to call during pod sync loop.
        lifecycle.PodSyncLoopHandlers

        // the list of handlers to call during pod sync.
        lifecycle.PodSyncHandlers

        // the number of allowed pods per core
        podsPerCore int

        // enableControllerAttachDetach indicates the Attach/Detach controller
        // should manage attachment/detachment of volumes scheduled to this node,
        // and disable kubelet from executing any attach/detach operations
        enableControllerAttachDetach bool

        // trigger deleting containers in a pod
        containerDeletor *podContainerDeletor

        // config iptables util rules
        makeIPTablesUtilChains bool

        // The AppArmor validator for checking whether AppArmor is supported.
        appArmorValidator apparmor.Validator

        // StatsProvider provides the node and the container stats.
        StatsProvider *stats.Provider

        // This flag, if set, instructs the kubelet to keep volumes from terminated pods mounted to the node.
        // This can be useful for debugging volume related issues.
        keepTerminatedPodVolumes bool // DEPRECATED

        // pluginmanager runs a set of asynchronous loops that figure out which
        // plugins need to be registered/unregistered based on this node and makes it so.
        pluginManager pluginmanager.PluginManager

        // This flag sets a maximum number of images to report in the node status.
        nodeStatusMaxImages int32

        // Handles RuntimeClass objects for the Kubelet.
        runtimeClassManager *runtimeclass.Manager

        // Handles node shutdown events for the Node.
        shutdownManager nodeshutdown.Manager

        // Manage user namespaces
        usernsManager *userns.UsernsManager

        // Mutex to serialize new pod admission and existing pod resizing
        podResizeMutex sync.Mutex

        // OpenTelemetry Tracer
        tracer trace.Tracer

        // Track node startup latencies
        nodeStartupLatencyTracker util.NodeStartupLatencyTracker
}

// ListPodStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) ListPodStats(ctx context.Context) ([]statsapi.PodStats, error) <span class="cov0" title="0">{
        return kl.StatsProvider.ListPodStats(ctx)
}</span>

// ListPodCPUAndMemoryStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) ListPodCPUAndMemoryStats(ctx context.Context) ([]statsapi.PodStats, error) <span class="cov0" title="0">{
        return kl.StatsProvider.ListPodCPUAndMemoryStats(ctx)
}</span>

// ListPodStatsAndUpdateCPUNanoCoreUsage is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) ListPodStatsAndUpdateCPUNanoCoreUsage(ctx context.Context) ([]statsapi.PodStats, error) <span class="cov0" title="0">{
        return kl.StatsProvider.ListPodStatsAndUpdateCPUNanoCoreUsage(ctx)
}</span>

// ImageFsStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) ImageFsStats(ctx context.Context) (*statsapi.FsStats, *statsapi.FsStats, error) <span class="cov0" title="0">{
        return kl.StatsProvider.ImageFsStats(ctx)
}</span>

// GetCgroupStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) GetCgroupStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, *statsapi.NetworkStats, error) <span class="cov0" title="0">{
        return kl.StatsProvider.GetCgroupStats(cgroupName, updateStats)
}</span>

// GetCgroupCPUAndMemoryStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) GetCgroupCPUAndMemoryStats(cgroupName string, updateStats bool) (*statsapi.ContainerStats, error) <span class="cov0" title="0">{
        return kl.StatsProvider.GetCgroupCPUAndMemoryStats(cgroupName, updateStats)
}</span>

// RootFsStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) RootFsStats() (*statsapi.FsStats, error) <span class="cov0" title="0">{
        return kl.StatsProvider.RootFsStats()
}</span>

// RlimitStats is delegated to StatsProvider, which implements stats.Provider interface
func (kl *Kubelet) RlimitStats() (*statsapi.RlimitStats, error) <span class="cov0" title="0">{
        return kl.StatsProvider.RlimitStats()
}</span>

// GetRecorder returns a pointer to the recorder.
func (kl *Kubelet) GetRecorder() *record.EventRecorder <span class="cov0" title="0">{
        return &amp;kl.recorder
}</span>

// GetKubeClient returns a pointer to the kube client.
func (kl *Kubelet) GetKubeClient() *clientset.Interface <span class="cov0" title="0">{
        return &amp;kl.kubeClient
}</span>

// GetHeartbeatClient returns a pointer to the heartbeat client.
func (kl *Kubelet) GetHeartbeatClient() *clientset.Interface <span class="cov0" title="0">{
        return &amp;kl.heartbeatClient
}</span>

// GetHostName returns a pointer to the hostname .
func (kl *Kubelet) GetHostName() *string <span class="cov0" title="0">{
        return &amp;kl.hostname
}</span>

// GetRootDir returns a pointer to the rootdir .
func (kl *Kubelet) GetRootDir() *string <span class="cov0" title="0">{
        return &amp;kl.rootDirectory
}</span>

// GetNodeName returns a pointer to the nodename .
func (kl *Kubelet) GetNodeName() *types.NodeName <span class="cov0" title="0">{
        return &amp;kl.nodeName
}</span>

// GetOS returns a pointer to the pod manager.
func (kl *Kubelet) GetOS() *kubecontainer.OSInterface <span class="cov0" title="0">{
        return &amp;kl.os
}</span>

// GetPodManager returns a pointer to the pod manager.
func (kl *Kubelet) GetPodManager() *kubepod.Manager <span class="cov0" title="0">{
        return &amp;kl.podManager
}</span>

// GetEvictionManager returns a pointer to the eviction manager.
func (kl *Kubelet) GetEvictionManager() *eviction.Manager <span class="cov0" title="0">{
        return &amp;kl.evictionManager
}</span>

// GetProbeManager returns a pointer to the probe manager.
func (kl *Kubelet) GetProbeManager() *prober.Manager <span class="cov0" title="0">{
        return &amp;kl.probeManager
}</span>

// GetSecretManager returns a pointer to the secret manager.
func (kl *Kubelet) GetSecretManager() *secret.Manager <span class="cov0" title="0">{
        return &amp;kl.secretManager
}</span>

// GetConfigMapManager returns a pointer to the config map manager.
func (kl *Kubelet) GetConfigMapManager() *configmap.Manager <span class="cov0" title="0">{
        return &amp;kl.configMapManager
}</span>

// GetVolumeManager returns a pointer to the volume manager.
func (kl *Kubelet) GetVolumeManager() *volumemanager.VolumeManager <span class="cov0" title="0">{
        return &amp;kl.volumeManager
}</span>

// GetStatusManager returns a pointer to the status manager.
func (kl *Kubelet) GetStatusManager() *status.Manager <span class="cov0" title="0">{
        return &amp;kl.statusManager
}</span>

// GetCadvisor returns a pointer to the cAdvisor interface.
func (kl *Kubelet) GetCadvisor() *cadvisor.Interface <span class="cov0" title="0">{
        return &amp;kl.cadvisor
}</span>

// GetCloud returns a pointer to the cloud provider interface.
func (kl *Kubelet) GetCloud() *cloudprovider.Interface <span class="cov0" title="0">{
        return &amp;kl.cloud
}</span>

// GetContainerRuntime returns a pointer to the container runtime.
func (kl *Kubelet) GetContainerRuntime() *kubecontainer.Runtime <span class="cov0" title="0">{
        return &amp;kl.containerRuntime
}</span>

// GetStreamingRuntime returns a pointer to the streaming runtime.
func (kl *Kubelet) GetStreamingRuntime() *kubecontainer.StreamingRuntime <span class="cov0" title="0">{
        return &amp;kl.streamingRuntime
}</span>

// GetRuntimeService returns a pointer to the internal runtime service.
func (kl *Kubelet) GetRuntimeService() *internalapi.RuntimeService <span class="cov0" title="0">{
        return &amp;kl.runtimeService
}</span>

// GetImageManager returns a pointer to the image manager.
func (kl *Kubelet) GetImageManager() *images.ImageGCManager <span class="cov0" title="0">{
        return &amp;kl.imageManager
}</span>

// GetNodeLister returns a pointer to the node lister.
func (kl *Kubelet) GetNodeLister() *corelisters.NodeLister <span class="cov0" title="0">{
        return &amp;kl.nodeLister
}</span>

// GetServiceLister returns a pointer to the service lister.
func (kl *Kubelet) GetServiceLister() *serviceLister <span class="cov0" title="0">{
        return &amp;kl.serviceLister
}</span>

// GetMounter returns a pointer to the mount.
func (kl *Kubelet) GetMounter() *mount.Interface <span class="cov0" title="0">{
        return &amp;kl.mounter
}</span>

// GetRuntimeState returns a pointer to runtimeState
func (kl *Kubelet) GetRuntimeState() *runtimeState <span class="cov0" title="0">{
        // Check if runtimeState is nil, initialize it if needed
        if kl.runtimeState == nil </span><span class="cov0" title="0">{
                kl.runtimeState = newRuntimeState(MaxWaitForContainerRuntime)
        }</span>
        <span class="cov0" title="0">return kl.runtimeState</span>
}

// GetMachineInfo returns a pointer to cadvisorapi.MachineInfo
func (kl *Kubelet) GetMachineInfo() *cadvisorapi.MachineInfo <span class="cov0" title="0">{

        return kl.machineInfo
}</span>

// GetRuntimeClassManager returns a pointer to runtimeclass.Manager
func (kl *Kubelet) GetRuntimeClassManager() *runtimeclass.Manager <span class="cov0" title="0">{
        return kl.runtimeClassManager
}</span>

// GetHostUtil returns a pointer to hostutil
func (kl *Kubelet) GetHostUtil() *hostutil.HostUtils <span class="cov0" title="0">{
        return &amp;kl.hostutil
}</span>

// GetSubPather returns a pointer to hostutil
func (kl *Kubelet) GetSubPather() *subpath.Interface <span class="cov0" title="0">{
        return &amp;kl.subpather
}</span>

// GetSourcesReady returns a pointer to hostutil
func (kl *Kubelet) GetSourcesReady() *config.SourcesReady <span class="cov0" title="0">{
        return &amp;kl.sourcesReady
}</span>

// GetServiceHasSynced returns a pointer to cache.InformerSynced
func (kl *Kubelet) GetServiceHasSynced() *cache.InformerSynced <span class="cov0" title="0">{
        return &amp;kl.serviceHasSynced
}</span>

// GetNodeHasSynced returns a pointer to cache.InformerSynced
func (kl *Kubelet) GetNodeHasSynced() *cache.InformerSynced <span class="cov0" title="0">{
        return &amp;kl.nodeHasSynced
}</span>

// GetDaemonEndpoints returns a pointer to v1.NodeDaemonEndpoints
func (kl *Kubelet) GetDaemonEndpoints() *v1.NodeDaemonEndpoints <span class="cov0" title="0">{

        if kl.daemonEndpoints == nil </span><span class="cov0" title="0">{
                kl.daemonEndpoints = &amp;v1.NodeDaemonEndpoints{}
        }</span>

        <span class="cov0" title="0">return kl.daemonEndpoints</span>
}

// GetCAdvisor returns a pointer to cadvisor.Interface
func (kl *Kubelet) GetCAdvisor() *cadvisor.Interface <span class="cov0" title="0">{
        return &amp;kl.cadvisor
}</span>

// GetTracer returns a pointer to trace.Tracer
func (kl *Kubelet) GetTracer() *trace.Tracer <span class="cov0" title="0">{
        return &amp;kl.tracer
}</span>

// GetMirrorPodManager returns a pointer to kubepod.MirrorClient
func (kl *Kubelet) GetMirrorPodManager() *kubepod.MirrorClient <span class="cov0" title="0">{
        return &amp;kl.mirrorPodClient
}</span>

// GetNodeStartupLatencyTracker returns a pointer to util.NodeStartupLatencyTracker
func (kl *Kubelet) GetNodeStartupLatencyTracker() *util.NodeStartupLatencyTracker <span class="cov0" title="0">{
        return &amp;kl.nodeStartupLatencyTracker
}</span>

// GetRuntimeCache returns a pointer to kubecontainer.RuntimeCache
func (kl *Kubelet) GetRuntimeCache() *kubecontainer.RuntimeCache <span class="cov0" title="0">{
        return &amp;kl.runtimeCache
}</span>

// GetReasonCache returns a pointer to ReasonCache
func (kl *Kubelet) GetReasonCache() *ReasonCache <span class="cov0" title="0">{
        fmt.Printf("Current reason cache in GetReasonCache: %+v\n", kl.reasonCache)

        if kl.reasonCache == nil </span><span class="cov0" title="0">{
                kl.reasonCache = &amp;ReasonCache{}
        }</span>

        <span class="cov0" title="0">return kl.reasonCache</span>
}

// GetPodCache returns a pointer to kubecontainer.Cache
func (kl *Kubelet) GetPodCache() *kubecontainer.Cache <span class="cov0" title="0">{
        return &amp;kl.podCache
}</span>

// GetPodWorkers returns a pointer to PodWorkers
func (kl *Kubelet) GetPodWorkers() *PodWorkers <span class="cov0" title="0">{
        return &amp;kl.podWorkers
}</span>

// GetLiveinessManager returns a pointer to proberesults.Manager
func (kl *Kubelet) GetLiveinessManager() *proberesults.Manager <span class="cov0" title="0">{
        return &amp;kl.livenessManager
}</span>

// GetReadinessManager returns a pointer to proberesults.Manager
func (kl *Kubelet) GetReadinessManager() *proberesults.Manager <span class="cov0" title="0">{
        return &amp;kl.readinessManager
}</span>

// GetStartupManager returns a pointer to proberesults.Manager
func (kl *Kubelet) GetStartupManager() *proberesults.Manager <span class="cov0" title="0">{
        return &amp;kl.startupManager
}</span>

// GetContainerManager returns a pointer to cm.ContainerManager
func (kl *Kubelet) GetContainerManager() *cm.ContainerManager <span class="cov0" title="0">{
        return &amp;kl.containerManager
}</span>

// GetResourceAnalyzer returns a pointer to serverstats.ResourceAnalyzer
func (kl *Kubelet) GetResourceAnalyzer() *serverstats.ResourceAnalyzer <span class="cov0" title="0">{
        return &amp;kl.resourceAnalyzer
}</span>

// GetStatsProvider returns a pointer to stats.Provider
func (kl *Kubelet) GetStatsProvider() *stats.Provider <span class="cov0" title="0">{
        fmt.Printf("Current StatsProvider in GetStatsProvider: %+v\n", kl.StatsProvider)
        if kl.StatsProvider == nil </span><span class="cov0" title="0">{
                kl.StatsProvider = stats.NewCadvisorStatsProvider(
                        nil,
                        nil,
                        nil,
                        nil,
                        nil,
                        nil,
                        nil,
                )
        }</span>
        <span class="cov0" title="0">return kl.StatsProvider</span>
}

// GetContainerGC returns a pointer to kubecontainer.GC
func (kl *Kubelet) GetContainerGC() *kubecontainer.GC <span class="cov0" title="0">{
        return &amp;kl.containerGC
}</span>

// GetBackOff returns a pointer to flowcontrol.Backoff
func (kl *Kubelet) GetBackOff() *flowcontrol.Backoff <span class="cov0" title="0">{

        // Check if backOff is nil, initialize it if needed
        if kl.backOff == nil </span><span class="cov0" title="0">{
                kl.backOff = &amp;flowcontrol.Backoff{}
        }</span>

        <span class="cov0" title="0">return kl.backOff</span>
}

// GetContainerLogManager returns a pointer to logs.ContainerLogManager
func (kl *Kubelet) GetContainerLogManager() *logs.ContainerLogManager <span class="cov0" title="0">{
        return &amp;kl.containerLogManager
}</span>

// GetResyncInterval returns a pointer to time.Duration
func (kl *Kubelet) GetResyncInterval() *time.Duration <span class="cov0" title="0">{
        return &amp;kl.resyncInterval
}</span>

// GetWorkQueue returns a pointer to queue.WorkQueue
func (kl *Kubelet) GetWorkQueue() *queue.WorkQueue <span class="cov0" title="0">{
        return &amp;kl.workQueue
}</span>

// GetPleg returns a pointer to pleg.PodLifecycleEventGenerator
func (kl *Kubelet) GetPleg() *pleg.PodLifecycleEventGenerator <span class="cov0" title="0">{
        return &amp;kl.pleg
}</span>

// GetClock returns a pointer to clock.WithTicker
func (kl *Kubelet) GetClock() *clock.WithTicker <span class="cov0" title="0">{
        return &amp;kl.clock
}</span>

// AdmitHandlers returns a pointer to lifecycle.PodAdmitHandlers
func (kl *Kubelet) AdmitHandlers() *lifecycle.PodAdmitHandlers <span class="cov0" title="0">{
        return &amp;kl.admitHandlers
}</span>

// GetShutdownManager returns a pointer to nodeshutdown.Manager
func (kl *Kubelet) GetShutdownManager() *nodeshutdown.Manager <span class="cov0" title="0">{
        return &amp;kl.shutdownManager
}</span>

// GetVolumePluginMgr returns a pointer to volume.VolumePluginMgr
func (kl *Kubelet) GetVolumePluginMgr() *volume.VolumePluginMgr <span class="cov0" title="0">{

        // Check if volumePluginMgr is nil, initialize it if needed
        if kl.volumePluginMgr == nil </span><span class="cov0" title="0">{
                volumePluginMgr, err := NewInitializedVolumePluginMgr(
                        kl,
                        nil,
                        nil,
                        nil,
                        nil,
                        nil,
                        nil,
                ) // Replace this with your actual initialization logic
                if err != nil </span><span class="cov0" title="0">{
                        fmt.Println("Error initializing VolumePluginMgr in GetVolumePluginMgr")
                }</span>
                <span class="cov0" title="0">kl.volumePluginMgr = volumePluginMgr</span>
        }

        <span class="cov0" title="0">return kl.volumePluginMgr</span>
}

// GetPluginManager returns a pointer to pluginmanager.PluginManager
func (kl *Kubelet) GetPluginManager() *pluginmanager.PluginManager <span class="cov0" title="0">{
        return &amp;kl.pluginManager
}</span>

// SetNodeStatusFuncs returns a pointer to setNodeStatusFuncs
func (kl *Kubelet) SetNodeStatusFuncs() *[]func(context.Context, *v1.Node) error <span class="cov0" title="0">{
        return &amp;kl.setNodeStatusFuncs
}</span>

// GetKubeletConfiguration returns a pointer to kubeletconfiginternal.KubeletConfiguration
func (kl *Kubelet) GetKubeletConfiguration() *kubeletconfiginternal.KubeletConfiguration <span class="cov0" title="0">{
        return &amp;kl.kubeletConfiguration
}</span>

// GetCgroupsPerQOS returns a pointer to bool
func (kl *Kubelet) GetCgroupsPerQOS() *bool <span class="cov0" title="0">{
        return &amp;kl.cgroupsPerQOS
}</span>

// GetDNSConfigurer returns a pointer to dns.Configurer
func (kl *Kubelet) GetDNSConfigurer() *dns.Configurer <span class="cov0" title="0">{
        return kl.dnsConfigurer
}</span>

// setupDataDirs creates:
// 1.  the root directory
// 2.  the pods directory
// 3.  the plugins directory
// 4.  the pod-resources directory
// 5.  the checkpoint directory
func (kl *Kubelet) setupDataDirs() error <span class="cov8" title="1">{
        if cleanedRoot := filepath.Clean(kl.rootDirectory); cleanedRoot != kl.rootDirectory </span><span class="cov0" title="0">{
                return fmt.Errorf("rootDirectory not in canonical form: expected %s, was %s", cleanedRoot, kl.rootDirectory)
        }</span>
        <span class="cov8" title="1">pluginRegistrationDir := kl.getPluginsRegistrationDir()
        pluginsDir := kl.getPluginsDir()
        if err := os.MkdirAll(kl.getRootDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating root directory: %v", err)
        }</span>
        <span class="cov8" title="1">if err := kl.hostutil.MakeRShared(kl.getRootDir()); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error configuring root directory: %v", err)
        }</span>
        <span class="cov8" title="1">if err := os.MkdirAll(kl.getPodsDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating pods directory: %v", err)
        }</span>
        <span class="cov8" title="1">if err := os.MkdirAll(kl.getPluginsDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating plugins directory: %v", err)
        }</span>
        <span class="cov8" title="1">if err := os.MkdirAll(kl.getPluginsRegistrationDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating plugins registry directory: %v", err)
        }</span>
        <span class="cov8" title="1">if err := os.MkdirAll(kl.getPodResourcesDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating podresources directory: %v", err)
        }</span>
        <span class="cov8" title="1">if utilfeature.DefaultFeatureGate.Enabled(features.ContainerCheckpoint) </span><span class="cov0" title="0">{
                if err := os.MkdirAll(kl.getCheckpointsDir(), 0700); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("error creating checkpoint directory: %v", err)
                }</span>
        }
        <span class="cov8" title="1">if selinux.GetEnabled() </span><span class="cov0" title="0">{
                err := selinux.SetFileLabel(pluginRegistrationDir, config.KubeletPluginsDirSELinuxLabel)
                if err != nil </span><span class="cov0" title="0">{
                        klog.InfoS("Unprivileged containerized plugins might not work, could not set selinux context on plugin registration dir", "path", pluginRegistrationDir, "err", err)
                }</span>
                <span class="cov0" title="0">err = selinux.SetFileLabel(pluginsDir, config.KubeletPluginsDirSELinuxLabel)
                if err != nil </span><span class="cov0" title="0">{
                        klog.InfoS("Unprivileged containerized plugins might not work, could not set selinux context on plugins dir", "path", pluginsDir, "err", err)
                }</span>
        }
        <span class="cov8" title="1">return nil</span>
}

// SetupDataDirs creates the export for setupDataDirs
func (kl *Kubelet) SetupDataDirs() error <span class="cov0" title="0">{
        if cleanedRoot := filepath.Clean(kl.rootDirectory); cleanedRoot != kl.rootDirectory </span><span class="cov0" title="0">{
                return fmt.Errorf("rootDirectory not in canonical form: expected %s, was %s", cleanedRoot, kl.rootDirectory)
        }</span>
        <span class="cov0" title="0">pluginRegistrationDir := kl.getPluginsRegistrationDir()
        pluginsDir := kl.getPluginsDir()
        if err := os.MkdirAll(kl.getRootDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating root directory: %v", err)
        }</span>
        <span class="cov0" title="0">if err := kl.hostutil.MakeRShared(kl.getRootDir()); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error configuring root directory: %v", err)
        }</span>
        <span class="cov0" title="0">if err := os.MkdirAll(kl.getPodsDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating pods directory: %v", err)
        }</span>
        <span class="cov0" title="0">if err := os.MkdirAll(kl.getPluginsDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating plugins directory: %v", err)
        }</span>
        <span class="cov0" title="0">if err := os.MkdirAll(kl.getPluginsRegistrationDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating plugins registry directory: %v", err)
        }</span>
        <span class="cov0" title="0">if err := os.MkdirAll(kl.getPodResourcesDir(), 0750); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error creating podresources directory: %v", err)
        }</span>
        <span class="cov0" title="0">if utilfeature.DefaultFeatureGate.Enabled(features.ContainerCheckpoint) </span><span class="cov0" title="0">{
                if err := os.MkdirAll(kl.getCheckpointsDir(), 0700); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("error creating checkpoint directory: %v", err)
                }</span>
        }
        <span class="cov0" title="0">if selinux.GetEnabled() </span><span class="cov0" title="0">{
                err := selinux.SetFileLabel(pluginRegistrationDir, config.KubeletPluginsDirSELinuxLabel)
                if err != nil </span><span class="cov0" title="0">{
                        klog.InfoS("Unprivileged containerized plugins might not work, could not set selinux context on plugin registration dir", "path", pluginRegistrationDir, "err", err)
                }</span>
                <span class="cov0" title="0">err = selinux.SetFileLabel(pluginsDir, config.KubeletPluginsDirSELinuxLabel)
                if err != nil </span><span class="cov0" title="0">{
                        klog.InfoS("Unprivileged containerized plugins might not work, could not set selinux context on plugins dir", "path", pluginsDir, "err", err)
                }</span>
        }
        <span class="cov0" title="0">return nil</span>
}

// StartGarbageCollection starts garbage collection threads.
func (kl *Kubelet) StartGarbageCollection() <span class="cov8" title="1">{
        loggedContainerGCFailure := false
        go wait.Until(func() </span><span class="cov8" title="1">{
                ctx := context.Background()
                if err := kl.containerGC.GarbageCollect(ctx); err != nil </span><span class="cov8" title="1">{
                        klog.ErrorS(err, "Container garbage collection failed")
                        kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ContainerGCFailed, err.Error())
                        loggedContainerGCFailure = true
                }</span> else<span class="cov0" title="0"> {
                        var vLevel klog.Level = 4
                        if loggedContainerGCFailure </span><span class="cov0" title="0">{
                                vLevel = 1
                                loggedContainerGCFailure = false
                        }</span>

                        <span class="cov0" title="0">klog.V(vLevel).InfoS("Container garbage collection succeeded")</span>
                }
        }, ContainerGCPeriod, wait.NeverStop)

        // when the high threshold is set to 100, and the max age is 0 (or the max age feature is disabled)
        // stub the image GC manager
        <span class="cov8" title="1">if kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 &amp;&amp;
                (!utilfeature.DefaultFeatureGate.Enabled(features.ImageMaximumGCAge) || kl.kubeletConfiguration.ImageMaximumGCAge.Duration == 0) </span><span class="cov0" title="0">{
                klog.V(2).InfoS("ImageGCHighThresholdPercent is set 100 and ImageMaximumGCAge is 0, Disable image GC")
                return
        }</span>

        <span class="cov8" title="1">prevImageGCFailed := false
        beganGC := time.Now()
        go wait.Until(func() </span><span class="cov8" title="1">{
                ctx := context.Background()
                if err := kl.imageManager.GarbageCollect(ctx, beganGC); err != nil </span><span class="cov8" title="1">{
                        if prevImageGCFailed </span><span class="cov0" title="0">{
                                klog.ErrorS(err, "Image garbage collection failed multiple times in a row")
                                // Only create an event for repeated failures
                                kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error())
                        }</span> else<span class="cov8" title="1"> {
                                klog.ErrorS(err, "Image garbage collection failed once. Stats initialization may not have completed yet")
                        }</span>
                        <span class="cov8" title="1">prevImageGCFailed = true</span>
                } else<span class="cov0" title="0"> {
                        var vLevel klog.Level = 4
                        if prevImageGCFailed </span><span class="cov0" title="0">{
                                vLevel = 1
                                prevImageGCFailed = false
                        }</span>

                        <span class="cov0" title="0">klog.V(vLevel).InfoS("Image garbage collection succeeded")</span>
                }
        }, ImageGCPeriod, wait.NeverStop)
}

// initializeModules will initialize internal modules that do not require the container runtime to be up.
// Note that the modules here must not depend on modules that are not initialized here.
func (kl *Kubelet) initializeModules() error <span class="cov0" title="0">{
        // Prometheus metrics.
        metrics.Register(
                collectors.NewVolumeStatsCollector(kl),
                collectors.NewLogMetricsCollector(kl.StatsProvider.ListPodStats),
        )
        metrics.SetNodeName(kl.nodeName)
        servermetrics.Register()

        // Setup filesystem directories.
        if err := kl.setupDataDirs(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // If the container logs directory does not exist, create it.
        <span class="cov0" title="0">if _, err := os.Stat(ContainerLogsDir); err != nil </span><span class="cov0" title="0">{
                if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to create directory %q: %v", ContainerLogsDir, err)
                }</span>
        }

        // Start the image manager.
        <span class="cov0" title="0">kl.imageManager.Start()

        // Start the certificate manager if it was enabled.
        if kl.serverCertificateManager != nil </span><span class="cov0" title="0">{
                kl.serverCertificateManager.Start()
        }</span>

        // Start out of memory watcher.
        <span class="cov0" title="0">if kl.oomWatcher != nil </span><span class="cov0" title="0">{
                if err := kl.oomWatcher.Start(kl.nodeRef); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to start OOM watcher: %w", err)
                }</span>
        }

        // Start resource analyzer
        <span class="cov0" title="0">kl.resourceAnalyzer.Start()

        return nil</span>
}

// initializeRuntimeDependentModules will initialize internal modules that require the container runtime to be up.
func (kl *Kubelet) initializeRuntimeDependentModules() <span class="cov0" title="0">{
        if err := kl.cadvisor.Start(); err != nil </span><span class="cov0" title="0">{
                // Fail kubelet and rely on the babysitter to retry starting kubelet.
                klog.ErrorS(err, "Failed to start cAdvisor")
                os.Exit(1)
        }</span>

        // trigger on-demand stats collection once so that we have capacity information for ephemeral storage.
        // ignore any errors, since if stats collection is not successful, the container manager will fail to start below.
        <span class="cov0" title="0">kl.StatsProvider.GetCgroupStats("/", true)
        // Start container manager.
        node, err := kl.getNodeAnyWay()
        if err != nil </span><span class="cov0" title="0">{
                // Fail kubelet and rely on the babysitter to retry starting kubelet.
                klog.ErrorS(err, "Kubelet failed to get node info")
                os.Exit(1)
        }</span>
        // containerManager must start after cAdvisor because it needs filesystem capacity information
        <span class="cov0" title="0">if err := kl.containerManager.Start(node, kl.GetActivePods, kl.sourcesReady, kl.statusManager, kl.runtimeService, kl.supportLocalStorageCapacityIsolation()); err != nil </span><span class="cov0" title="0">{
                // Fail kubelet and rely on the babysitter to retry starting kubelet.
                klog.ErrorS(err, "Failed to start ContainerManager")
                os.Exit(1)
        }</span>
        // eviction manager must start after cadvisor because it needs to know if the container runtime has a dedicated imagefs
        <span class="cov0" title="0">kl.evictionManager.Start(kl.StatsProvider, kl.GetActivePods, kl.PodIsFinished, evictionMonitoringPeriod)

        // container log manager must start after container runtime is up to retrieve information from container runtime
        // and inform container to reopen log file after log rotation.
        kl.containerLogManager.Start()
        // Adding Registration Callback function for CSI Driver
        kl.pluginManager.AddHandler(pluginwatcherapi.CSIPlugin, plugincache.PluginHandler(csi.PluginHandler))
        // Adding Registration Callback function for DRA Plugin
        if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) </span><span class="cov0" title="0">{
                kl.pluginManager.AddHandler(pluginwatcherapi.DRAPlugin, plugincache.PluginHandler(draplugin.NewRegistrationHandler()))
        }</span>
        // Adding Registration Callback function for Device Manager
        <span class="cov0" title="0">kl.pluginManager.AddHandler(pluginwatcherapi.DevicePlugin, kl.containerManager.GetPluginRegistrationHandler())

        // Start the plugin manager
        klog.V(4).InfoS("Starting plugin manager")
        go kl.pluginManager.Run(kl.sourcesReady, wait.NeverStop)

        err = kl.shutdownManager.Start()
        if err != nil </span><span class="cov0" title="0">{
                // The shutdown manager is not critical for kubelet, so log failure, but don't block Kubelet startup if there was a failure starting it.
                klog.ErrorS(err, "Failed to start node shutdown manager")
        }</span>
}

// Run starts the kubelet reacting to config updates
func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) <span class="cov0" title="0">{
        ctx := context.Background()
        if kl.logServer == nil </span><span class="cov0" title="0">{
                file := http.FileServer(http.Dir(nodeLogDir))
                if utilfeature.DefaultFeatureGate.Enabled(features.NodeLogQuery) &amp;&amp; kl.kubeletConfiguration.EnableSystemLogQuery </span><span class="cov0" title="0">{
                        kl.logServer = http.StripPrefix("/logs/", http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) </span><span class="cov0" title="0">{
                                if nlq, errs := newNodeLogQuery(req.URL.Query()); len(errs) &gt; 0 </span><span class="cov0" title="0">{
                                        http.Error(w, errs.ToAggregate().Error(), http.StatusBadRequest)
                                        return
                                }</span> else<span class="cov0" title="0"> if nlq != nil </span><span class="cov0" title="0">{
                                        if req.URL.Path != "/" &amp;&amp; req.URL.Path != "" </span><span class="cov0" title="0">{
                                                http.Error(w, "path not allowed in query mode", http.StatusNotAcceptable)
                                                return
                                        }</span>
                                        <span class="cov0" title="0">if errs := nlq.validate(); len(errs) &gt; 0 </span><span class="cov0" title="0">{
                                                http.Error(w, errs.ToAggregate().Error(), http.StatusNotAcceptable)
                                                return
                                        }</span>
                                        // Validation ensures that the request does not query services and files at the same time
                                        <span class="cov0" title="0">if len(nlq.Services) &gt; 0 </span><span class="cov0" title="0">{
                                                journal.ServeHTTP(w, req)
                                                return
                                        }</span>
                                        // Validation ensures that the request does not explicitly query multiple files at the same time
                                        <span class="cov0" title="0">if len(nlq.Files) == 1 </span><span class="cov0" title="0">{
                                                // Account for the \ being used on Windows clients
                                                req.URL.Path = filepath.ToSlash(nlq.Files[0])
                                        }</span>
                                }
                                // Fall back in case the caller is directly trying to query a file
                                // Example: kubectl get --raw /api/v1/nodes/$name/proxy/logs/foo.log
                                <span class="cov0" title="0">file.ServeHTTP(w, req)</span>
                        }))
                } else<span class="cov0" title="0"> {
                        kl.logServer = http.StripPrefix("/logs/", file)
                }</span>
        }
        <span class="cov0" title="0">if kl.kubeClient == nil </span><span class="cov0" title="0">{
                klog.InfoS("No API server defined - no node status update will be sent")
        }</span>

        // Start the cloud provider sync manager
        <span class="cov0" title="0">if kl.cloudResourceSyncManager != nil </span><span class="cov0" title="0">{
                go kl.cloudResourceSyncManager.Run(wait.NeverStop)
        }</span>

        <span class="cov0" title="0">if err := kl.initializeModules(); err != nil </span><span class="cov0" title="0">{
                kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error())
                klog.ErrorS(err, "Failed to initialize internal modules")
                os.Exit(1)
        }</span>

        // Start volume manager
        <span class="cov0" title="0">go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)

        if kl.kubeClient != nil </span><span class="cov0" title="0">{
                // Start two go-routines to update the status.
                //
                // The first will report to the apiserver every nodeStatusUpdateFrequency and is aimed to provide regular status intervals,
                // while the second is used to provide a more timely status update during initialization and runs an one-shot update to the apiserver
                // once the node becomes ready, then exits afterwards.
                //
                // Introduce some small jittering to ensure that over time the requests won't start
                // accumulating at approximately the same time from the set of nodes due to priority and
                // fairness effect.
                go wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop)
                go kl.fastStatusUpdateOnce()

                // start syncing lease
                go kl.nodeLeaseController.Run(context.Background())
        }</span>
        <span class="cov0" title="0">go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop)

        // Set up iptables util rules
        if kl.makeIPTablesUtilChains </span><span class="cov0" title="0">{
                kl.initNetworkUtil()
        }</span>

        // Start component sync loops.
        <span class="cov0" title="0">kl.statusManager.Start()

        // Start syncing RuntimeClasses if enabled.
        if kl.runtimeClassManager != nil </span><span class="cov0" title="0">{
                kl.runtimeClassManager.Start(wait.NeverStop)
        }</span>

        // Start the pod lifecycle event generator.
        <span class="cov0" title="0">kl.pleg.Start()

        // Start eventedPLEG only if EventedPLEG feature gate is enabled.
        if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) </span><span class="cov0" title="0">{
                kl.eventedPleg.Start()
        }</span>

        <span class="cov0" title="0">kl.syncLoop(ctx, updates, kl)</span>
}

// SyncPod is the transaction script for the sync of a single pod (setting up)
// a pod. This method is reentrant and expected to converge a pod towards the
// desired state of the spec. The reverse (teardown) is handled in
// SyncTerminatingPod and SyncTerminatedPod. If SyncPod exits without error,
// then the pod runtime state is in sync with the desired configuration state
// (pod is running). If SyncPod exits with a transient error, the next
// invocation of SyncPod is expected to make progress towards reaching the
// desired state. SyncPod exits with isTerminal when the pod was detected to
// have reached a terminal lifecycle phase due to container exits (for
// RestartNever or RestartOnFailure) and the next method invoked will be
// SyncTerminatingPod. If the pod terminates for any other reason, SyncPod
// will receive a context cancellation and should exit as soon as possible.
//
// Arguments:
//
// updateType - whether this is a create (first time) or an update, should
// only be used for metrics since this method must be reentrant
//
// pod - the pod that is being set up
//
// mirrorPod - the mirror pod known to the kubelet for this pod, if any
//
// podStatus - the most recent pod status observed for this pod which can
// be used to determine the set of actions that should be taken during
// this loop of SyncPod
//
// The workflow is:
//   - If the pod is being created, record pod worker start latency
//   - Call generateAPIPodStatus to prepare an v1.PodStatus for the pod
//   - If the pod is being seen as running for the first time, record pod
//     start latency
//   - Update the status of the pod in the status manager
//   - Stop the pod's containers if it should not be running due to soft
//     admission
//   - Ensure any background tracking for a runnable pod is started
//   - Create a mirror pod if the pod is a static pod, and does not
//     already have a mirror pod
//   - Create the data directories for the pod if they do not exist
//   - Wait for volumes to attach/mount
//   - Fetch the pull secrets for the pod
//   - Call the container runtime's SyncPod callback
//   - Update the traffic shaping for the pod's ingress and egress limits
//
// If any step of this workflow errors, the error is returned, and is repeated
// on the next SyncPod call.
//
// This operation writes all events that are dispatched in order to provide
// the most accurate information possible about an error situation to aid debugging.
// Callers should not write an event if this operation returns an error.
func (kl *Kubelet) SyncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (isTerminal bool, err error) <span class="cov8" title="1">{
        ctx, otelSpan := kl.tracer.Start(ctx, "syncPod", trace.WithAttributes(
                semconv.K8SPodUIDKey.String(string(pod.UID)),
                attribute.String("k8s.pod", klog.KObj(pod).String()),
                semconv.K8SPodNameKey.String(pod.Name),
                attribute.String("k8s.pod.update_type", updateType.String()),
                semconv.K8SNamespaceNameKey.String(pod.Namespace),
        ))
        klog.V(4).InfoS("SyncPod enter", "pod", klog.KObj(pod), "podUID", pod.UID)
        defer func() </span><span class="cov8" title="1">{
                klog.V(4).InfoS("SyncPod exit", "pod", klog.KObj(pod), "podUID", pod.UID, "isTerminal", isTerminal)
                otelSpan.End()
        }</span>()

        // Latency measurements for the main workflow are relative to the
        // first time the pod was seen by kubelet.
        <span class="cov8" title="1">var firstSeenTime time.Time
        if firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok </span><span class="cov0" title="0">{
                firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get()
        }</span>

        // Record pod worker start latency if being created
        // TODO: make pod workers record their own latencies
        <span class="cov8" title="1">if updateType == kubetypes.SyncPodCreate </span><span class="cov8" title="1">{
                if !firstSeenTime.IsZero() </span><span class="cov0" title="0">{
                        // This is the first time we are syncing the pod. Record the latency
                        // since kubelet first saw the pod if firstSeenTime is set.
                        metrics.PodWorkerStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime))
                }</span> else<span class="cov8" title="1"> {
                        klog.V(3).InfoS("First seen time not recorded for pod",
                                "podUID", pod.UID,
                                "pod", klog.KObj(pod))
                }</span>
        }

        // Generate final API pod status with pod and status manager status
        <span class="cov8" title="1">apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, false)
        // The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576)
        // TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and
        // set pod IP to hostIP directly in runtime.GetPodStatus
        podStatus.IPs = make([]string, 0, len(apiPodStatus.PodIPs))
        for _, ipInfo := range apiPodStatus.PodIPs </span><span class="cov8" title="1">{
                podStatus.IPs = append(podStatus.IPs, ipInfo.IP)
        }</span>
        <span class="cov8" title="1">if len(podStatus.IPs) == 0 &amp;&amp; len(apiPodStatus.PodIP) &gt; 0 </span><span class="cov0" title="0">{
                podStatus.IPs = []string{apiPodStatus.PodIP}
        }</span>

        // If the pod is terminal, we don't need to continue to setup the pod
        <span class="cov8" title="1">if apiPodStatus.Phase == v1.PodSucceeded || apiPodStatus.Phase == v1.PodFailed </span><span class="cov8" title="1">{
                kl.statusManager.SetPodStatus(pod, apiPodStatus)
                isTerminal = true
                return isTerminal, nil
        }</span>

        // If the pod should not be running, we request the pod's containers be stopped. This is not the same
        // as termination (we want to stop the pod, but potentially restart it later if soft admission allows
        // it later). Set the status and phase appropriately
        <span class="cov8" title="1">runnable := kl.canRunPod(pod)
        if !runnable.Admit </span><span class="cov0" title="0">{
                // Pod is not runnable; and update the Pod and Container statuses to why.
                if apiPodStatus.Phase != v1.PodFailed &amp;&amp; apiPodStatus.Phase != v1.PodSucceeded </span><span class="cov0" title="0">{
                        apiPodStatus.Phase = v1.PodPending
                }</span>
                <span class="cov0" title="0">apiPodStatus.Reason = runnable.Reason
                apiPodStatus.Message = runnable.Message
                // Waiting containers are not creating.
                const waitingReason = "Blocked"
                for _, cs := range apiPodStatus.InitContainerStatuses </span><span class="cov0" title="0">{
                        if cs.State.Waiting != nil </span><span class="cov0" title="0">{
                                cs.State.Waiting.Reason = waitingReason
                        }</span>
                }
                <span class="cov0" title="0">for _, cs := range apiPodStatus.ContainerStatuses </span><span class="cov0" title="0">{
                        if cs.State.Waiting != nil </span><span class="cov0" title="0">{
                                cs.State.Waiting.Reason = waitingReason
                        }</span>
                }
        }

        // Record the time it takes for the pod to become running
        // since kubelet first saw the pod if firstSeenTime is set.
        <span class="cov8" title="1">existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID)
        if !ok || existingStatus.Phase == v1.PodPending &amp;&amp; apiPodStatus.Phase == v1.PodRunning &amp;&amp;
                !firstSeenTime.IsZero() </span><span class="cov8" title="1">{
                metrics.PodStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime))
        }</span>

        <span class="cov8" title="1">kl.statusManager.SetPodStatus(pod, apiPodStatus)

        // Pods that are not runnable must be stopped - return a typed error to the pod worker
        if !runnable.Admit </span><span class="cov0" title="0">{
                klog.V(2).InfoS("Pod is not runnable and must have running containers stopped", "pod", klog.KObj(pod), "podUID", pod.UID, "message", runnable.Message)
                var syncErr error
                p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)
                if err := kl.killPod(ctx, pod, p, nil); err != nil </span><span class="cov0" title="0">{
                        if !wait.Interrupted(err) </span><span class="cov0" title="0">{
                                kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, "error killing pod: %v", err)
                                syncErr = fmt.Errorf("error killing pod: %w", err)
                                utilruntime.HandleError(syncErr)
                        }</span>
                } else<span class="cov0" title="0"> {
                        // There was no error killing the pod, but the pod cannot be run.
                        // Return an error to signal that the sync loop should back off.
                        syncErr = fmt.Errorf("pod cannot be run: %v", runnable.Message)
                }</span>
                <span class="cov0" title="0">return false, syncErr</span>
        }

        // If the network plugin is not ready, only start the pod if it uses the host network
        <span class="cov8" title="1">if err := kl.runtimeState.networkErrors(); err != nil &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) </span><span class="cov8" title="1">{
                kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, "%s: %v", NetworkNotReadyErrorMsg, err)
                return false, fmt.Errorf("%s: %v", NetworkNotReadyErrorMsg, err)
        }</span>

        // ensure the kubelet knows about referenced secrets or configmaps used by the pod
        <span class="cov8" title="1">if !kl.podWorkers.IsPodTerminationRequested(pod.UID) </span><span class="cov8" title="1">{
                if kl.secretManager != nil </span><span class="cov8" title="1">{
                        kl.secretManager.RegisterPod(pod)
                }</span>
                <span class="cov8" title="1">if kl.configMapManager != nil </span><span class="cov8" title="1">{
                        kl.configMapManager.RegisterPod(pod)
                }</span>
        }

        // Create Cgroups for the pod and apply resource parameters
        // to them if cgroups-per-qos flag is enabled.
        <span class="cov8" title="1">pcm := kl.containerManager.NewPodContainerManager()
        // If pod has already been terminated then we need not create
        // or update the pod's cgroup
        // TODO: once context cancellation is added this check can be removed
        if !kl.podWorkers.IsPodTerminationRequested(pod.UID) </span><span class="cov8" title="1">{
                // When the kubelet is restarted with the cgroups-per-qos
                // flag enabled, all the pod's running containers
                // should be killed intermittently and brought back up
                // under the qos cgroup hierarchy.
                // Check if this is the pod's first sync
                firstSync := true
                for _, containerStatus := range apiPodStatus.ContainerStatuses </span><span class="cov8" title="1">{
                        if containerStatus.State.Running != nil </span><span class="cov0" title="0">{
                                firstSync = false
                                break</span>
                        }
                }
                // Don't kill containers in pod if pod's cgroups already
                // exists or the pod is running for the first time
                <span class="cov8" title="1">podKilled := false
                if !pcm.Exists(pod) &amp;&amp; !firstSync </span><span class="cov0" title="0">{
                        p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)
                        if err := kl.killPod(ctx, pod, p, nil); err == nil </span><span class="cov0" title="0">{
                                if wait.Interrupted(err) </span><span class="cov0" title="0">{
                                        return false, err
                                }</span>
                                <span class="cov0" title="0">podKilled = true</span>
                        } else<span class="cov0" title="0"> {
                                klog.ErrorS(err, "KillPod failed", "pod", klog.KObj(pod), "podStatus", podStatus)
                        }</span>
                }
                // Create and Update pod's Cgroups
                // Don't create cgroups for run once pod if it was killed above
                // The current policy is not to restart the run once pods when
                // the kubelet is restarted with the new flag as run once pods are
                // expected to run only once and if the kubelet is restarted then
                // they are not expected to run again.
                // We don't create and apply updates to cgroup if its a run once pod and was killed above
                <span class="cov8" title="1">if !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) </span><span class="cov8" title="1">{
                        if !pcm.Exists(pod) </span><span class="cov0" title="0">{
                                if err := kl.containerManager.UpdateQOSCgroups(); err != nil </span><span class="cov0" title="0">{
                                        klog.V(2).InfoS("Failed to update QoS cgroups while syncing pod", "pod", klog.KObj(pod), "err", err)
                                }</span>
                                <span class="cov0" title="0">if err := pcm.EnsureExists(pod); err != nil </span><span class="cov0" title="0">{
                                        kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, "unable to ensure pod container exists: %v", err)
                                        return false, fmt.Errorf("failed to ensure that the pod: %v cgroups exist and are correctly applied: %v", pod.UID, err)
                                }</span>
                        }
                }
        }

        // Create Mirror Pod for Static Pod if it doesn't already exist
        <span class="cov8" title="1">if kubetypes.IsStaticPod(pod) </span><span class="cov8" title="1">{
                deleted := false
                if mirrorPod != nil </span><span class="cov8" title="1">{
                        if mirrorPod.DeletionTimestamp != nil || !kubepod.IsMirrorPodOf(mirrorPod, pod) </span><span class="cov8" title="1">{
                                // The mirror pod is semantically different from the static pod. Remove
                                // it. The mirror pod will get recreated later.
                                klog.InfoS("Trying to delete pod", "pod", klog.KObj(pod), "podUID", mirrorPod.ObjectMeta.UID)
                                podFullName := kubecontainer.GetPodFullName(pod)
                                var err error
                                deleted, err = kl.mirrorPodClient.DeleteMirrorPod(podFullName, &amp;mirrorPod.ObjectMeta.UID)
                                if deleted </span><span class="cov8" title="1">{
                                        klog.InfoS("Deleted mirror pod because it is outdated", "pod", klog.KObj(mirrorPod))
                                }</span> else<span class="cov0" title="0"> if err != nil </span><span class="cov0" title="0">{
                                        klog.ErrorS(err, "Failed deleting mirror pod", "pod", klog.KObj(mirrorPod))
                                }</span>
                        }
                }
                <span class="cov8" title="1">if mirrorPod == nil || deleted </span><span class="cov8" title="1">{
                        node, err := kl.GetNode()
                        if err != nil </span><span class="cov0" title="0">{
                                klog.V(4).ErrorS(err, "No need to create a mirror pod, since failed to get node info from the cluster", "node", klog.KRef("", string(kl.nodeName)))
                        }</span> else<span class="cov8" title="1"> if node.DeletionTimestamp != nil </span><span class="cov0" title="0">{
                                klog.V(4).InfoS("No need to create a mirror pod, since node has been removed from the cluster", "node", klog.KRef("", string(kl.nodeName)))
                        }</span> else<span class="cov8" title="1"> {
                                klog.V(4).InfoS("Creating a mirror pod for static pod", "pod", klog.KObj(pod))
                                if err := kl.mirrorPodClient.CreateMirrorPod(pod); err != nil </span><span class="cov0" title="0">{
                                        klog.ErrorS(err, "Failed creating a mirror pod for", "pod", klog.KObj(pod))
                                }</span>
                        }
                }
        }

        // Make data directories for the pod
        <span class="cov8" title="1">if err := kl.makePodDataDirs(pod); err != nil </span><span class="cov0" title="0">{
                kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, "error making pod data directories: %v", err)
                klog.ErrorS(err, "Unable to make pod data directories for pod", "pod", klog.KObj(pod))
                return false, err
        }</span>

        // Wait for volumes to attach/mount
        <span class="cov8" title="1">if err := kl.volumeManager.WaitForAttachAndMount(ctx, pod); err != nil </span><span class="cov0" title="0">{
                if !wait.Interrupted(err) </span><span class="cov0" title="0">{
                        kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, "Unable to attach or mount volumes: %v", err)
                        klog.ErrorS(err, "Unable to attach or mount volumes for pod; skipping pod", "pod", klog.KObj(pod))
                }</span>
                <span class="cov0" title="0">return false, err</span>
        }

        // Fetch the pull secrets for the pod
        <span class="cov8" title="1">pullSecrets := kl.getPullSecretsForPod(pod)

        // Ensure the pod is being probed
        kl.probeManager.AddPod(pod)

        if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) </span><span class="cov8" title="1">{
                // Handle pod resize here instead of doing it in HandlePodUpdates because
                // this conveniently retries any Deferred resize requests
                // TODO(vinaykul,InPlacePodVerticalScaling): Investigate doing this in HandlePodUpdates + periodic SyncLoop scan
                //     See: https://github.com/kubernetes/kubernetes/pull/102884#discussion_r663160060
                if kl.podWorkers.CouldHaveRunningContainers(pod.UID) &amp;&amp; !kubetypes.IsStaticPod(pod) </span><span class="cov0" title="0">{
                        pod = kl.handlePodResourcesResize(pod)
                }</span>
        }

        // TODO(#113606): use cancellation from the incoming context parameter, which comes from the pod worker.
        // Currently, using cancellation from that context causes test failures. To remove this WithoutCancel,
        // any wait.Interrupted errors need to be filtered from result and bypass the reasonCache - cancelling
        // the context for SyncPod is a known and deliberate error, not a generic error.
        // Use WithoutCancel instead of a new context.TODO() to propagate trace context
        // Call the container runtime's SyncPod callback
        <span class="cov8" title="1">sctx := context.WithoutCancel(ctx)
        result := kl.containerRuntime.SyncPod(sctx, pod, podStatus, pullSecrets, kl.backOff)
        kl.reasonCache.Update(pod.UID, result)
        if err := result.Error(); err != nil </span><span class="cov0" title="0">{
                // Do not return error if the only failures were pods in backoff
                for _, r := range result.SyncResults </span><span class="cov0" title="0">{
                        if r.Error != kubecontainer.ErrCrashLoopBackOff &amp;&amp; r.Error != images.ErrImagePullBackOff </span><span class="cov0" title="0">{
                                // Do not record an event here, as we keep all event logging for sync pod failures
                                // local to container runtime, so we get better errors.
                                return false, err
                        }</span>
                }

                <span class="cov0" title="0">return false, nil</span>
        }

        <span class="cov8" title="1">if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) &amp;&amp; isPodResizeInProgress(pod, &amp;apiPodStatus) </span><span class="cov0" title="0">{
                // While resize is in progress, periodically call PLEG to update pod cache
                runningPod := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)
                if err, _ := kl.pleg.UpdateCache(&amp;runningPod, pod.UID); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Failed to update pod cache", "pod", klog.KObj(pod))
                        return false, err
                }</span>
        }

        <span class="cov8" title="1">return false, nil</span>
}

// SyncTerminatingPod is expected to terminate all running containers in a pod. Once this method
// returns without error, the pod is considered to be terminated and it will be safe to clean up any
// pod state that is tied to the lifetime of running containers. The next method invoked will be
// SyncTerminatedPod. This method is expected to return with the grace period provided and the
// provided context may be cancelled if the duration is exceeded. The method may also be interrupted
// with a context cancellation if the grace period is shortened by the user or the kubelet (such as
// during eviction). This method is not guaranteed to be called if a pod is force deleted from the
// configuration and the kubelet is restarted - SyncTerminatingRuntimePod handles those orphaned
// pods.
func (kl *Kubelet) SyncTerminatingPod(_ context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus, gracePeriod *int64, podStatusFn func(*v1.PodStatus)) error <span class="cov8" title="1">{
        // TODO(#113606): connect this with the incoming context parameter, which comes from the pod worker.
        // Currently, using that context causes test failures.
        ctx, otelSpan := kl.tracer.Start(context.Background(), "syncTerminatingPod", trace.WithAttributes(
                semconv.K8SPodUIDKey.String(string(pod.UID)),
                attribute.String("k8s.pod", klog.KObj(pod).String()),
                semconv.K8SPodNameKey.String(pod.Name),
                semconv.K8SNamespaceNameKey.String(pod.Namespace),
        ))
        defer otelSpan.End()
        klog.V(4).InfoS("SyncTerminatingPod enter", "pod", klog.KObj(pod), "podUID", pod.UID)
        defer klog.V(4).InfoS("SyncTerminatingPod exit", "pod", klog.KObj(pod), "podUID", pod.UID)

        apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, false)
        if podStatusFn != nil </span><span class="cov8" title="1">{
                podStatusFn(&amp;apiPodStatus)
        }</span>
        <span class="cov8" title="1">kl.statusManager.SetPodStatus(pod, apiPodStatus)

        if gracePeriod != nil </span><span class="cov8" title="1">{
                klog.V(4).InfoS("Pod terminating with grace period", "pod", klog.KObj(pod), "podUID", pod.UID, "gracePeriod", *gracePeriod)
        }</span> else<span class="cov0" title="0"> {
                klog.V(4).InfoS("Pod terminating with grace period", "pod", klog.KObj(pod), "podUID", pod.UID, "gracePeriod", nil)
        }</span>

        <span class="cov8" title="1">kl.probeManager.StopLivenessAndStartup(pod)

        p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)
        if err := kl.killPod(ctx, pod, p, gracePeriod); err != nil </span><span class="cov0" title="0">{
                kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, "error killing pod: %v", err)
                // there was an error killing the pod, so we return that error directly
                utilruntime.HandleError(err)
                return err
        }</span>

        // Once the containers are stopped, we can stop probing for liveness and readiness.
        // TODO: once a pod is terminal, certain probes (liveness exec) could be stopped immediately after
        //   the detection of a container shutdown or (for readiness) after the first failure. Tracked as
        //   https://github.com/kubernetes/kubernetes/issues/107894 although may not be worth optimizing.
        <span class="cov8" title="1">kl.probeManager.RemovePod(pod)

        // Guard against consistency issues in KillPod implementations by checking that there are no
        // running containers. This method is invoked infrequently so this is effectively free and can
        // catch race conditions introduced by callers updating pod status out of order.
        // TODO: have KillPod return the terminal status of stopped containers and write that into the
        //  cache immediately
        podStatus, err := kl.containerRuntime.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Unable to read pod status prior to final pod termination", "pod", klog.KObj(pod), "podUID", pod.UID)
                return err
        }</span>
        <span class="cov8" title="1">var runningContainers []string
        type container struct {
                Name       string
                State      string
                ExitCode   int
                FinishedAt string
        }
        var containers []container
        klogV := klog.V(4)
        klogVEnabled := klogV.Enabled()
        for _, s := range podStatus.ContainerStatuses </span><span class="cov0" title="0">{
                if s.State == kubecontainer.ContainerStateRunning </span><span class="cov0" title="0">{
                        runningContainers = append(runningContainers, s.ID.String())
                }</span>
                <span class="cov0" title="0">if klogVEnabled </span><span class="cov0" title="0">{
                        containers = append(containers, container{Name: s.Name, State: string(s.State), ExitCode: s.ExitCode, FinishedAt: s.FinishedAt.UTC().Format(time.RFC3339Nano)})
                }</span>
        }
        <span class="cov8" title="1">if klogVEnabled </span><span class="cov0" title="0">{
                sort.Slice(containers, func(i, j int) bool </span><span class="cov0" title="0">{ return containers[i].Name &lt; containers[j].Name }</span>)
                <span class="cov0" title="0">klog.V(4).InfoS("Post-termination container state", "pod", klog.KObj(pod), "podUID", pod.UID, "containers", containers)</span>
        }
        <span class="cov8" title="1">if len(runningContainers) &gt; 0 </span><span class="cov0" title="0">{
                return fmt.Errorf("detected running containers after a successful KillPod, CRI violation: %v", runningContainers)
        }</span>

        // NOTE: resources must be unprepared AFTER all containers have stopped
        // and BEFORE the pod status is changed on the API server
        // to avoid race conditions with the resource deallocation code in kubernetes core.
        <span class="cov8" title="1">if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) </span><span class="cov0" title="0">{
                if err := kl.UnprepareDynamicResources(pod); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // Compute and update the status in cache once the pods are no longer running.
        // The computation is done here to ensure the pod status used for it contains
        // information about the container end states (including exit codes) - when
        // SyncTerminatedPod is called the containers may already be removed.
        <span class="cov8" title="1">apiPodStatus = kl.generateAPIPodStatus(pod, podStatus, true)
        kl.statusManager.SetPodStatus(pod, apiPodStatus)

        // we have successfully stopped all containers, the pod is terminating, our status is "done"
        klog.V(4).InfoS("Pod termination stopped all running containers", "pod", klog.KObj(pod), "podUID", pod.UID)

        return nil</span>
}

// SyncTerminatingRuntimePod is expected to terminate running containers in a pod that we have no
// configuration for. Once this method returns without error, any remaining local state can be safely
// cleaned up by background processes in each subsystem. Unlike syncTerminatingPod, we lack
// knowledge of the full pod spec and so cannot perform lifecycle related operations, only ensure
// that the remnant of the running pod is terminated and allow garbage collection to proceed. We do
// not update the status of the pod because with the source of configuration removed, we have no
// place to send that status.
func (kl *Kubelet) SyncTerminatingRuntimePod(_ context.Context, runningPod *kubecontainer.Pod) error <span class="cov0" title="0">{
        // TODO(#113606): connect this with the incoming context parameter, which comes from the pod worker.
        // Currently, using that context causes test failures.
        ctx := context.Background()
        pod := runningPod.ToAPIPod()
        klog.V(4).InfoS("SyncTerminatingRuntimePod enter", "pod", klog.KObj(pod), "podUID", pod.UID)
        defer klog.V(4).InfoS("SyncTerminatingRuntimePod exit", "pod", klog.KObj(pod), "podUID", pod.UID)

        // we kill the pod directly since we have lost all other information about the pod.
        klog.V(4).InfoS("Orphaned running pod terminating without grace period", "pod", klog.KObj(pod), "podUID", pod.UID)
        // TODO: this should probably be zero, to bypass any waiting (needs fixes in container runtime)
        gracePeriod := int64(1)
        if err := kl.killPod(ctx, pod, *runningPod, &amp;gracePeriod); err != nil </span><span class="cov0" title="0">{
                kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToKillPod, "error killing pod: %v", err)
                // there was an error killing the pod, so we return that error directly
                utilruntime.HandleError(err)
                return err
        }</span>
        <span class="cov0" title="0">klog.V(4).InfoS("Pod termination stopped all running orphaned containers", "pod", klog.KObj(pod), "podUID", pod.UID)
        return nil</span>
}

// SyncTerminatedPod cleans up a pod that has terminated (has no running containers).
// The invocations in this call are expected to tear down all pod resources.
// When this method exits the pod is expected to be ready for cleanup. This method
// reduces the latency of pod cleanup but is not guaranteed to get called in all scenarios.
//
// Because the kubelet has no local store of information, all actions in this method that modify
// on-disk state must be reentrant and be garbage collected by HandlePodCleanups or a separate loop.
// This typically occurs when a pod is force deleted from configuration (local disk or API) and the
// kubelet restarts in the middle of the action.
func (kl *Kubelet) SyncTerminatedPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus) error <span class="cov0" title="0">{
        ctx, otelSpan := kl.tracer.Start(ctx, "syncTerminatedPod", trace.WithAttributes(
                semconv.K8SPodUIDKey.String(string(pod.UID)),
                attribute.String("k8s.pod", klog.KObj(pod).String()),
                semconv.K8SPodNameKey.String(pod.Name),
                semconv.K8SNamespaceNameKey.String(pod.Namespace),
        ))
        defer otelSpan.End()
        klog.V(4).InfoS("SyncTerminatedPod enter", "pod", klog.KObj(pod), "podUID", pod.UID)
        defer klog.V(4).InfoS("SyncTerminatedPod exit", "pod", klog.KObj(pod), "podUID", pod.UID)

        // generate the final status of the pod
        // TODO: should we simply fold this into TerminatePod? that would give a single pod update
        apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, true)

        kl.statusManager.SetPodStatus(pod, apiPodStatus)

        // volumes are unmounted after the pod worker reports ShouldPodRuntimeBeRemoved (which is satisfied
        // before syncTerminatedPod is invoked)
        if err := kl.volumeManager.WaitForUnmount(ctx, pod); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">klog.V(4).InfoS("Pod termination unmounted volumes", "pod", klog.KObj(pod), "podUID", pod.UID)

        if !kl.keepTerminatedPodVolumes </span><span class="cov0" title="0">{
                // This waiting loop relies on the background cleanup which starts after pod workers respond
                // true for ShouldPodRuntimeBeRemoved, which happens after `SyncTerminatingPod` is completed.
                if err := wait.PollUntilContextCancel(ctx, 100*time.Millisecond, true, func(ctx context.Context) (bool, error) </span><span class="cov0" title="0">{
                        volumesExist := kl.podVolumesExist(pod.UID)
                        if volumesExist </span><span class="cov0" title="0">{
                                klog.V(3).InfoS("Pod is terminated, but some volumes have not been cleaned up", "pod", klog.KObj(pod), "podUID", pod.UID)
                        }</span>
                        <span class="cov0" title="0">return !volumesExist, nil</span>
                }); err != nil <span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov0" title="0">klog.V(3).InfoS("Pod termination cleaned up volume paths", "pod", klog.KObj(pod), "podUID", pod.UID)</span>
        }

        // After volume unmount is complete, let the secret and configmap managers know we're done with this pod
        <span class="cov0" title="0">if kl.secretManager != nil </span><span class="cov0" title="0">{
                kl.secretManager.UnregisterPod(pod)
        }</span>
        <span class="cov0" title="0">if kl.configMapManager != nil </span><span class="cov0" title="0">{
                kl.configMapManager.UnregisterPod(pod)
        }</span>

        // Note: we leave pod containers to be reclaimed in the background since dockershim requires the
        // container for retrieving logs and we want to make sure logs are available until the pod is
        // physically deleted.

        // remove any cgroups in the hierarchy for pods that are no longer running.
        <span class="cov0" title="0">if kl.cgroupsPerQOS </span><span class="cov0" title="0">{
                pcm := kl.containerManager.NewPodContainerManager()
                name, _ := pcm.GetPodContainerName(pod)
                if err := pcm.Destroy(name); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
                <span class="cov0" title="0">klog.V(4).InfoS("Pod termination removed cgroups", "pod", klog.KObj(pod), "podUID", pod.UID)</span>
        }

        <span class="cov0" title="0">kl.usernsManager.Release(pod.UID)

        // mark the final pod status
        kl.statusManager.TerminatePod(pod)
        klog.V(4).InfoS("Pod is terminated and will need no more status updates", "pod", klog.KObj(pod), "podUID", pod.UID)

        return nil</span>
}

// Get pods which should be resynchronized. Currently, the following pod should be resynchronized:
//   - pod whose work is ready.
//   - internal modules that request sync of a pod.
//
// This method does not return orphaned pods (those known only to the pod worker that may have
// been deleted from configuration). Those pods are synced by HandlePodCleanups as a consequence
// of driving the state machine to completion.
//
// TODO: Consider synchronizing all pods which have not recently been acted on to be resilient
// to bugs that might prevent updates from being delivered (such as the previous bug with
// orphaned pods). Instead of asking the work queue for pending work, consider asking the
// PodWorker which pods should be synced.
func (kl *Kubelet) getPodsToSync() []*v1.Pod <span class="cov8" title="1">{
        allPods := kl.podManager.GetPods()
        podUIDs := kl.workQueue.GetWork()
        podUIDSet := sets.NewString()
        for _, podUID := range podUIDs </span><span class="cov8" title="1">{
                podUIDSet.Insert(string(podUID))
        }</span>
        <span class="cov8" title="1">var podsToSync []*v1.Pod
        for _, pod := range allPods </span><span class="cov8" title="1">{
                if podUIDSet.Has(string(pod.UID)) </span><span class="cov8" title="1">{
                        // The work of the pod is ready
                        podsToSync = append(podsToSync, pod)
                        continue</span>
                }
                <span class="cov8" title="1">for _, podSyncLoopHandler := range kl.PodSyncLoopHandlers </span><span class="cov8" title="1">{
                        if podSyncLoopHandler.ShouldSync(pod) </span><span class="cov8" title="1">{
                                podsToSync = append(podsToSync, pod)
                                break</span>
                        }
                }
        }
        <span class="cov8" title="1">return podsToSync</span>
}

func (kl *Kubelet) GetPodsToSync() []*v1.Pod <span class="cov0" title="0">{
        allPods := kl.podManager.GetPods()
        podUIDs := kl.workQueue.GetWork()
        podUIDSet := sets.NewString()
        for _, podUID := range podUIDs </span><span class="cov0" title="0">{
                podUIDSet.Insert(string(podUID))
        }</span>
        <span class="cov0" title="0">var podsToSync []*v1.Pod
        for _, pod := range allPods </span><span class="cov0" title="0">{
                if podUIDSet.Has(string(pod.UID)) </span><span class="cov0" title="0">{
                        // The work of the pod is ready
                        podsToSync = append(podsToSync, pod)
                        continue</span>
                }
                <span class="cov0" title="0">for _, podSyncLoopHandler := range kl.PodSyncLoopHandlers </span><span class="cov0" title="0">{
                        if podSyncLoopHandler.ShouldSync(pod) </span><span class="cov0" title="0">{
                                podsToSync = append(podsToSync, pod)
                                break</span>
                        }
                }
        }
        <span class="cov0" title="0">return podsToSync</span>
}

// deletePod deletes the pod from the internal state of the kubelet by:
// 1.  stopping the associated pod worker asynchronously
// 2.  signaling to kill the pod by sending on the podKillingCh channel
//
// deletePod returns an error if not all sources are ready or the pod is not
// found in the runtime cache.
func (kl *Kubelet) deletePod(pod *v1.Pod) error <span class="cov8" title="1">{
        if pod == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("deletePod does not allow nil pod")
        }</span>
        <span class="cov8" title="1">if !kl.sourcesReady.AllReady() </span><span class="cov8" title="1">{
                // If the sources aren't ready, skip deletion, as we may accidentally delete pods
                // for sources that haven't reported yet.
                return fmt.Errorf("skipping delete because sources aren't ready yet")
        }</span>
        <span class="cov8" title="1">klog.V(3).InfoS("Pod has been deleted and must be killed", "pod", klog.KObj(pod), "podUID", pod.UID)
        kl.podWorkers.UpdatePod(UpdatePodOptions{
                Pod:        pod,
                UpdateType: kubetypes.SyncPodKill,
        })
        // We leave the volume/directory cleanup to the periodic cleanup routine.
        return nil</span>
}

// rejectPod records an event about the pod with the given reason and message,
// and updates the pod to the failed phase in the status manager.
func (kl *Kubelet) rejectPod(pod *v1.Pod, reason, message string) <span class="cov8" title="1">{
        kl.recorder.Eventf(pod, v1.EventTypeWarning, reason, message)
        kl.statusManager.SetPodStatus(pod, v1.PodStatus{
                Phase:   v1.PodFailed,
                Reason:  reason,
                Message: "Pod was rejected: " + message})
}</span>

// canAdmitPod determines if a pod can be admitted, and gives a reason if it
// cannot. "pod" is new pod, while "pods" are all admitted pods
// The function returns a boolean value indicating whether the pod
// can be admitted, a brief single-word reason and a message explaining why
// the pod cannot be admitted.
func (kl *Kubelet) canAdmitPod(pods []*v1.Pod, pod *v1.Pod) (bool, string, string) <span class="cov8" title="1">{
        // the kubelet will invoke each pod admit handler in sequence
        // if any handler rejects, the pod is rejected.
        // TODO: move out of disk check into a pod admitter
        // TODO: out of resource eviction should have a pod admitter call-out
        attrs := &amp;lifecycle.PodAdmitAttributes{Pod: pod, OtherPods: pods}
        if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) </span><span class="cov8" title="1">{
                // Use allocated resources values from checkpoint store (source of truth) to determine fit
                otherPods := make([]*v1.Pod, 0, len(pods))
                for _, p := range pods </span><span class="cov8" title="1">{
                        op := p.DeepCopy()
                        kl.updateContainerResourceAllocation(op)

                        otherPods = append(otherPods, op)
                }</span>
                <span class="cov8" title="1">attrs.OtherPods = otherPods</span>
        }
        <span class="cov8" title="1">for _, podAdmitHandler := range kl.admitHandlers </span><span class="cov8" title="1">{
                if result := podAdmitHandler.Admit(attrs); !result.Admit </span><span class="cov8" title="1">{
                        return false, result.Reason, result.Message
                }</span>
        }

        <span class="cov8" title="1">return true, "", ""</span>
}

func (kl *Kubelet) canRunPod(pod *v1.Pod) lifecycle.PodAdmitResult <span class="cov8" title="1">{
        attrs := &amp;lifecycle.PodAdmitAttributes{Pod: pod}
        // Get "OtherPods". Rejected pods are failed, so only include admitted pods that are alive.
        attrs.OtherPods = kl.GetActivePods()

        for _, handler := range kl.softAdmitHandlers </span><span class="cov0" title="0">{
                if result := handler.Admit(attrs); !result.Admit </span><span class="cov0" title="0">{
                        return result
                }</span>
        }

        <span class="cov8" title="1">return lifecycle.PodAdmitResult{Admit: true}</span>
}

// syncLoop is the main loop for processing changes. It watches for changes from
// three channels (file, apiserver, and http) and creates a union of them. For
// any new change seen, will run a sync against desired state and running state. If
// no changes are seen to the configuration, will synchronize the last known desired
// state every sync-frequency seconds. Never returns.
func (kl *Kubelet) syncLoop(ctx context.Context, updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) <span class="cov8" title="1">{
        klog.InfoS("Starting kubelet main sync loop")
        // The syncTicker wakes up kubelet to checks if there are any pod workers
        // that need to be sync'd. A one-second period is sufficient because the
        // sync interval is defaulted to 10s.
        syncTicker := time.NewTicker(time.Second)
        defer syncTicker.Stop()
        housekeepingTicker := time.NewTicker(housekeepingPeriod)
        defer housekeepingTicker.Stop()
        plegCh := kl.pleg.Watch()
        const (
                base   = 100 * time.Millisecond
                max    = 5 * time.Second
                factor = 2
        )
        duration := base
        // Responsible for checking limits in resolv.conf
        // The limits do not have anything to do with individual pods
        // Since this is called in syncLoop, we don't need to call it anywhere else
        if kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != "" </span><span class="cov0" title="0">{
                kl.dnsConfigurer.CheckLimitsForResolvConf()
        }</span>

        <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                if err := kl.runtimeState.runtimeErrors(); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Skipping pod synchronization")
                        // exponential backoff
                        time.Sleep(duration)
                        duration = time.Duration(math.Min(float64(max), factor*float64(duration)))
                        continue</span>
                }
                // reset backoff if we have a success
                <span class="cov8" title="1">duration = base

                kl.syncLoopMonitor.Store(kl.clock.Now())
                if !kl.syncLoopIteration(ctx, updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) </span><span class="cov8" title="1">{
                        break</span>
                }
                <span class="cov0" title="0">kl.syncLoopMonitor.Store(kl.clock.Now())</span>
        }
}

// SyncLoop is responsible for synchronizing changes and dispatching pods to the given handler.
func (kl *Kubelet) SyncLoop(ctx context.Context, updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) <span class="cov0" title="0">{
        kl.syncLoop(ctx, updates, handler)
}</span>

// syncLoopIteration reads from various channels and dispatches pods to the
// given handler.
//
// Arguments:
// 1.  configCh:       a channel to read config events from
// 2.  handler:        the SyncHandler to dispatch pods to
// 3.  syncCh:         a channel to read periodic sync events from
// 4.  housekeepingCh: a channel to read housekeeping events from
// 5.  plegCh:         a channel to read PLEG updates from
//
// Events are also read from the kubelet liveness manager's update channel.
//
// The workflow is to read from one of the channels, handle that event, and
// update the timestamp in the sync loop monitor.
//
// Here is an appropriate place to note that despite the syntactical
// similarity to the switch statement, the case statements in a select are
// evaluated in a pseudorandom order if there are multiple channels ready to
// read from when the select is evaluated.  In other words, case statements
// are evaluated in random order, and you can not assume that the case
// statements evaluate in order if multiple channels have events.
//
// With that in mind, in truly no particular order, the different channels
// are handled as follows:
//
//   - configCh: dispatch the pods for the config change to the appropriate
//     handler callback for the event type
//   - plegCh: update the runtime cache; sync pod
//   - syncCh: sync all pods waiting for sync
//   - housekeepingCh: trigger cleanup of pods
//   - health manager: sync pods that have failed or in which one or more
//     containers have failed health checks
//
// SyncLoopIteration reads from various channels and dispatches pods to the given handler.
// It returns false if the update channel is closed, otherwise it returns true.

func (kl *Kubelet) syncLoopIteration(ctx context.Context, configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler,
        syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool <span class="cov8" title="1">{
        select </span>{
        case u, open := &lt;-configCh:<span class="cov8" title="1">
                // Update from a config source; dispatch it to the right handler
                // callback.
                if !open </span><span class="cov8" title="1">{
                        klog.ErrorS(nil, "Update channel is closed, exiting the sync loop")
                        return false
                }</span>

                <span class="cov0" title="0">switch u.Op </span>{
                case kubetypes.ADD:<span class="cov0" title="0">
                        klog.V(2).InfoS("SyncLoop ADD", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
                        // After restarting, kubelet will get all existing pods through
                        // ADD as if they are new pods. These pods will then go through the
                        // admission process and *may* be rejected. This can be resolved
                        // once we have checkpointing.
                        handler.HandlePodAdditions(u.Pods)</span>
                case kubetypes.UPDATE:<span class="cov0" title="0">
                        klog.V(2).InfoS("SyncLoop UPDATE", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
                        handler.HandlePodUpdates(u.Pods)</span>
                case kubetypes.REMOVE:<span class="cov0" title="0">
                        klog.V(2).InfoS("SyncLoop REMOVE", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
                        handler.HandlePodRemoves(u.Pods)</span>
                case kubetypes.RECONCILE:<span class="cov0" title="0">
                        klog.V(4).InfoS("SyncLoop RECONCILE", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
                        handler.HandlePodReconcile(u.Pods)</span>
                case kubetypes.DELETE:<span class="cov0" title="0">
                        klog.V(2).InfoS("SyncLoop DELETE", "source", u.Source, "pods", klog.KObjSlice(u.Pods))
                        // DELETE is treated as a UPDATE because of graceful deletion.
                        handler.HandlePodUpdates(u.Pods)</span>
                case kubetypes.SET:<span class="cov0" title="0">
                        // TODO: Do we want to support this?
                        klog.ErrorS(nil, "Kubelet does not support snapshot update")</span>
                default:<span class="cov0" title="0">
                        klog.ErrorS(nil, "Invalid operation type received", "operation", u.Op)</span>
                }

                <span class="cov0" title="0">kl.sourcesReady.AddSource(u.Source)</span>

        case e := &lt;-plegCh:<span class="cov0" title="0">
                if isSyncPodWorthy(e) </span><span class="cov0" title="0">{
                        // PLEG event for a pod; sync it.
                        if pod, ok := kl.podManager.GetPodByUID(e.ID); ok </span><span class="cov0" title="0">{
                                klog.V(2).InfoS("SyncLoop (PLEG): event for pod", "pod", klog.KObj(pod), "event", e)
                                handler.HandlePodSyncs([]*v1.Pod{pod})
                        }</span> else<span class="cov0" title="0"> {
                                // If the pod no longer exists, ignore the event.
                                klog.V(4).InfoS("SyncLoop (PLEG): pod does not exist, ignore irrelevant event", "event", e)
                        }</span>
                }

                <span class="cov0" title="0">if e.Type == pleg.ContainerDied </span><span class="cov0" title="0">{
                        if containerID, ok := e.Data.(string); ok </span><span class="cov0" title="0">{
                                kl.cleanUpContainersInPod(e.ID, containerID)
                        }</span>
                }
        case &lt;-syncCh:<span class="cov0" title="0">
                // Sync pods waiting for sync
                podsToSync := kl.getPodsToSync()
                if len(podsToSync) == 0 </span><span class="cov0" title="0">{
                        break</span>
                }
                <span class="cov0" title="0">klog.V(4).InfoS("SyncLoop (SYNC) pods", "total", len(podsToSync), "pods", klog.KObjSlice(podsToSync))
                handler.HandlePodSyncs(podsToSync)</span>
        case update := &lt;-kl.livenessManager.Updates():<span class="cov0" title="0">
                if update.Result == proberesults.Failure </span><span class="cov0" title="0">{
                        handleProbeSync(kl, update, handler, "liveness", "unhealthy")
                }</span>
        case update := &lt;-kl.readinessManager.Updates():<span class="cov0" title="0">
                ready := update.Result == proberesults.Success
                kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready)

                status := ""
                if ready </span><span class="cov0" title="0">{
                        status = "ready"
                }</span>
                <span class="cov0" title="0">handleProbeSync(kl, update, handler, "readiness", status)</span>
        case update := &lt;-kl.startupManager.Updates():<span class="cov0" title="0">
                started := update.Result == proberesults.Success
                kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started)

                status := "unhealthy"
                if started </span><span class="cov0" title="0">{
                        status = "started"
                }</span>
                <span class="cov0" title="0">handleProbeSync(kl, update, handler, "startup", status)</span>
        case &lt;-housekeepingCh:<span class="cov0" title="0">
                if !kl.sourcesReady.AllReady() </span><span class="cov0" title="0">{
                        // If the sources aren't ready or volume manager has not yet synced the states,
                        // skip housekeeping, as we may accidentally delete pods from unready sources.
                        klog.V(4).InfoS("SyncLoop (housekeeping, skipped): sources aren't ready yet")
                }</span> else<span class="cov0" title="0"> {
                        start := time.Now()
                        klog.V(4).InfoS("SyncLoop (housekeeping)")
                        if err := handler.HandlePodCleanups(ctx); err != nil </span><span class="cov0" title="0">{
                                klog.ErrorS(err, "Failed cleaning pods")
                        }</span>
                        <span class="cov0" title="0">duration := time.Since(start)
                        if duration &gt; housekeepingWarningDuration </span><span class="cov0" title="0">{
                                klog.ErrorS(fmt.Errorf("housekeeping took too long"), "Housekeeping took longer than expected", "expected", housekeepingWarningDuration, "actual", duration.Round(time.Millisecond))
                        }</span>
                        <span class="cov0" title="0">klog.V(4).InfoS("SyncLoop (housekeeping) end", "duration", duration.Round(time.Millisecond))</span>
                }
        }
        <span class="cov0" title="0">return true</span>
}

// SyncLoopIteration performs a single iteration of the sync loop.
// It takes a context, a channel for pod updates, a sync handler, and various other channels.
// It returns a boolean indicating whether the iteration was successful.
func (kl *Kubelet) SyncLoopIteration(ctx context.Context, configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler,
        syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool <span class="cov0" title="0">{
        return kl.syncLoopIteration(ctx, configCh, handler,
                syncCh, housekeepingCh, plegCh)
}</span>

func handleProbeSync(kl *Kubelet, update proberesults.Update, handler SyncHandler, probe, status string) <span class="cov0" title="0">{
        // We should not use the pod from manager, because it is never updated after initialization.
        pod, ok := kl.podManager.GetPodByUID(update.PodUID)
        if !ok </span><span class="cov0" title="0">{
                // If the pod no longer exists, ignore the update.
                klog.V(4).InfoS("SyncLoop (probe): ignore irrelevant update", "probe", probe, "status", status, "update", update)
                return
        }</span>
        <span class="cov0" title="0">klog.V(1).InfoS("SyncLoop (probe)", "probe", probe, "status", status, "pod", klog.KObj(pod))
        handler.HandlePodSyncs([]*v1.Pod{pod})</span>
}

// HandlePodAdditions is the callback in SyncHandler for pods being added from
// a config source.
func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) <span class="cov8" title="1">{
        start := kl.clock.Now()
        sort.Sort(sliceutils.PodsByCreationTime(pods))
        if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) </span><span class="cov8" title="1">{
                kl.podResizeMutex.Lock()
                defer kl.podResizeMutex.Unlock()
        }</span>
        <span class="cov8" title="1">for _, pod := range pods </span><span class="cov8" title="1">{
                existingPods := kl.podManager.GetPods()
                // Always add the pod to the pod manager. Kubelet relies on the pod
                // manager as the source of truth for the desired state. If a pod does
                // not exist in the pod manager, it means that it has been deleted in
                // the apiserver and no action (other than cleanup) is required.
                kl.podManager.AddPod(pod)

                pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
                if wasMirror </span><span class="cov0" title="0">{
                        if pod == nil </span><span class="cov0" title="0">{
                                klog.V(2).InfoS("Unable to find pod for mirror pod, skipping", "mirrorPod", klog.KObj(mirrorPod), "mirrorPodUID", mirrorPod.UID)
                                continue</span>
                        }
                        <span class="cov0" title="0">kl.podWorkers.UpdatePod(UpdatePodOptions{
                                Pod:        pod,
                                MirrorPod:  mirrorPod,
                                UpdateType: kubetypes.SyncPodUpdate,
                                StartTime:  start,
                        })
                        continue</span>
                }

                // Only go through the admission process if the pod is not requested
                // for termination by another part of the kubelet. If the pod is already
                // using resources (previously admitted), the pod worker is going to be
                // shutting it down. If the pod hasn't started yet, we know that when
                // the pod worker is invoked it will also avoid setting up the pod, so
                // we simply avoid doing any work.
                <span class="cov8" title="1">if !kl.podWorkers.IsPodTerminationRequested(pod.UID) </span><span class="cov8" title="1">{
                        // We failed pods that we rejected, so activePods include all admitted
                        // pods that are alive.
                        activePods := kl.filterOutInactivePods(existingPods)

                        if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) </span><span class="cov8" title="1">{
                                // To handle kubelet restarts, test pod admissibility using AllocatedResources values
                                // (for cpu &amp; memory) from checkpoint store. If found, that is the source of truth.
                                podCopy := pod.DeepCopy()
                                kl.updateContainerResourceAllocation(podCopy)

                                // Check if we can admit the pod; if not, reject it.
                                if ok, reason, message := kl.canAdmitPod(activePods, podCopy); !ok </span><span class="cov0" title="0">{
                                        kl.rejectPod(pod, reason, message)
                                        continue</span>
                                }
                                // For new pod, checkpoint the resource values at which the Pod has been admitted
                                <span class="cov8" title="1">if err := kl.statusManager.SetPodAllocation(podCopy); err != nil </span><span class="cov0" title="0">{
                                        //TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate
                                        klog.ErrorS(err, "SetPodAllocation failed", "pod", klog.KObj(pod))
                                }</span>
                        } else<span class="cov8" title="1"> {
                                // Check if we can admit the pod; if not, reject it.
                                if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok </span><span class="cov8" title="1">{
                                        kl.rejectPod(pod, reason, message)
                                        continue</span>
                                }
                        }
                }
                <span class="cov8" title="1">kl.podWorkers.UpdatePod(UpdatePodOptions{
                        Pod:        pod,
                        MirrorPod:  mirrorPod,
                        UpdateType: kubetypes.SyncPodCreate,
                        StartTime:  start,
                })</span>
        }
}

// updateContainerResourceAllocation updates AllocatedResources values
// (for cpu &amp; memory) from checkpoint store
func (kl *Kubelet) updateContainerResourceAllocation(pod *v1.Pod) <span class="cov8" title="1">{
        for _, c := range pod.Spec.Containers </span><span class="cov8" title="1">{
                allocatedResources, found := kl.statusManager.GetContainerResourceAllocation(string(pod.UID), c.Name)
                if c.Resources.Requests != nil &amp;&amp; found </span><span class="cov8" title="1">{
                        if _, ok := allocatedResources[v1.ResourceCPU]; ok </span><span class="cov8" title="1">{
                                c.Resources.Requests[v1.ResourceCPU] = allocatedResources[v1.ResourceCPU]
                        }</span>
                        <span class="cov8" title="1">if _, ok := allocatedResources[v1.ResourceMemory]; ok </span><span class="cov8" title="1">{
                                c.Resources.Requests[v1.ResourceMemory] = allocatedResources[v1.ResourceMemory]
                        }</span>
                }
        }
}

// HandlePodUpdates is the callback in the SyncHandler interface for pods
// being updated from a config source.
func (kl *Kubelet) HandlePodUpdates(pods []*v1.Pod) <span class="cov8" title="1">{
        start := kl.clock.Now()
        for _, pod := range pods </span><span class="cov8" title="1">{
                kl.podManager.UpdatePod(pod)

                pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
                if wasMirror </span><span class="cov0" title="0">{
                        if pod == nil </span><span class="cov0" title="0">{
                                klog.V(2).InfoS("Unable to find pod for mirror pod, skipping", "mirrorPod", klog.KObj(mirrorPod), "mirrorPodUID", mirrorPod.UID)
                                continue</span>
                        }
                }

                <span class="cov8" title="1">kl.podWorkers.UpdatePod(UpdatePodOptions{
                        Pod:        pod,
                        MirrorPod:  mirrorPod,
                        UpdateType: kubetypes.SyncPodUpdate,
                        StartTime:  start,
                })</span>
        }
}

// HandlePodRemoves is the callback in the SyncHandler interface for pods
// being removed from a config source.
func (kl *Kubelet) HandlePodRemoves(pods []*v1.Pod) <span class="cov8" title="1">{
        start := kl.clock.Now()
        for _, pod := range pods </span><span class="cov8" title="1">{
                kl.podManager.RemovePod(pod)

                pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
                if wasMirror </span><span class="cov0" title="0">{
                        if pod == nil </span><span class="cov0" title="0">{
                                klog.V(2).InfoS("Unable to find pod for mirror pod, skipping", "mirrorPod", klog.KObj(mirrorPod), "mirrorPodUID", mirrorPod.UID)
                                continue</span>
                        }
                        <span class="cov0" title="0">kl.podWorkers.UpdatePod(UpdatePodOptions{
                                Pod:        pod,
                                MirrorPod:  mirrorPod,
                                UpdateType: kubetypes.SyncPodUpdate,
                                StartTime:  start,
                        })
                        continue</span>
                }

                // Deletion is allowed to fail because the periodic cleanup routine
                // will trigger deletion again.
                <span class="cov8" title="1">if err := kl.deletePod(pod); err != nil </span><span class="cov8" title="1">{
                        klog.V(2).InfoS("Failed to delete pod", "pod", klog.KObj(pod), "err", err)
                }</span>
        }
}

// HandlePodReconcile is the callback in the SyncHandler interface for pods
// that should be reconciled. Pods are reconciled when only the status of the
// pod is updated in the API.
func (kl *Kubelet) HandlePodReconcile(pods []*v1.Pod) <span class="cov0" title="0">{
        start := kl.clock.Now()
        for _, pod := range pods </span><span class="cov0" title="0">{
                // Update the pod in pod manager, status manager will do periodically reconcile according
                // to the pod manager.
                kl.podManager.UpdatePod(pod)

                pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
                if wasMirror </span><span class="cov0" title="0">{
                        if pod == nil </span><span class="cov0" title="0">{
                                klog.V(2).InfoS("Unable to find pod for mirror pod, skipping", "mirrorPod", klog.KObj(mirrorPod), "mirrorPodUID", mirrorPod.UID)
                                continue</span>
                        }
                        // Static pods should be reconciled the same way as regular pods
                }

                // TODO: reconcile being calculated in the config manager is questionable, and avoiding
                // extra syncs may no longer be necessary. Reevaluate whether Reconcile and Sync can be
                // merged (after resolving the next two TODOs).

                // Reconcile Pod "Ready" condition if necessary. Trigger sync pod for reconciliation.
                // TODO: this should be unnecessary today - determine what is the cause for this to
                // be different than Sync, or if there is a better place for it. For instance, we have
                // needsReconcile in kubelet/config, here, and in status_manager.
                <span class="cov0" title="0">if status.NeedToReconcilePodReadiness(pod) </span><span class="cov0" title="0">{
                        kl.podWorkers.UpdatePod(UpdatePodOptions{
                                Pod:        pod,
                                MirrorPod:  mirrorPod,
                                UpdateType: kubetypes.SyncPodSync,
                                StartTime:  start,
                        })
                }</span>

                // After an evicted pod is synced, all dead containers in the pod can be removed.
                // TODO: this is questionable - status read is async and during eviction we already
                // expect to not have some container info. The pod worker knows whether a pod has
                // been evicted, so if this is about minimizing the time to react to an eviction we
                // can do better. If it's about preserving pod status info we can also do better.
                <span class="cov0" title="0">if eviction.PodIsEvicted(pod.Status) </span><span class="cov0" title="0">{
                        if podStatus, err := kl.podCache.Get(pod.UID); err == nil </span><span class="cov0" title="0">{
                                kl.containerDeletor.deleteContainersInPod("", podStatus, true)
                        }</span>
                }
        }
}

// HandlePodSyncs is the callback in the syncHandler interface for pods
// that should be dispatched to pod workers for sync.
func (kl *Kubelet) HandlePodSyncs(pods []*v1.Pod) <span class="cov8" title="1">{
        start := kl.clock.Now()
        for _, pod := range pods </span><span class="cov8" title="1">{
                pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
                if wasMirror </span><span class="cov0" title="0">{
                        if pod == nil </span><span class="cov0" title="0">{
                                klog.V(2).InfoS("Unable to find pod for mirror pod, skipping", "mirrorPod", klog.KObj(mirrorPod), "mirrorPodUID", mirrorPod.UID)
                                continue</span>
                        }
                        // Syncing a mirror pod is a programmer error since the intent of sync is to
                        // batch notify all pending work. We should make it impossible to double sync,
                        // but for now log a programmer error to prevent accidental introduction.
                        <span class="cov0" title="0">klog.V(3).InfoS("Programmer error, HandlePodSyncs does not expect to receive mirror pods", "podUID", pod.UID, "mirrorPodUID", mirrorPod.UID)
                        continue</span>
                }
                <span class="cov8" title="1">kl.podWorkers.UpdatePod(UpdatePodOptions{
                        Pod:        pod,
                        MirrorPod:  mirrorPod,
                        UpdateType: kubetypes.SyncPodSync,
                        StartTime:  start,
                })</span>
        }
}

func isPodResizeInProgress(pod *v1.Pod, podStatus *v1.PodStatus) bool <span class="cov8" title="1">{
        for _, c := range pod.Spec.Containers </span><span class="cov8" title="1">{
                if cs, ok := podutil.GetContainerStatus(podStatus.ContainerStatuses, c.Name); ok </span><span class="cov8" title="1">{
                        if cs.Resources == nil </span><span class="cov8" title="1">{
                                continue</span>
                        }
                        <span class="cov0" title="0">if !cmp.Equal(c.Resources.Limits, cs.Resources.Limits) || !cmp.Equal(cs.AllocatedResources, cs.Resources.Requests) </span><span class="cov0" title="0">{
                                return true
                        }</span>
                }
        }
        <span class="cov8" title="1">return false</span>
}

func (kl *Kubelet) canResizePod(pod *v1.Pod) (bool, *v1.Pod, v1.PodResizeStatus) <span class="cov8" title="1">{
        var otherActivePods []*v1.Pod

        node, err := kl.getNodeAnyWay()
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "getNodeAnyway function failed")
                return false, nil, ""
        }</span>
        <span class="cov8" title="1">podCopy := pod.DeepCopy()
        cpuAvailable := node.Status.Allocatable.Cpu().MilliValue()
        memAvailable := node.Status.Allocatable.Memory().Value()
        cpuRequests := resource.GetResourceRequest(podCopy, v1.ResourceCPU)
        memRequests := resource.GetResourceRequest(podCopy, v1.ResourceMemory)
        if cpuRequests &gt; cpuAvailable || memRequests &gt; memAvailable </span><span class="cov8" title="1">{
                klog.V(3).InfoS("Resize is not feasible as request exceeds allocatable node resources", "pod", podCopy.Name)
                return false, podCopy, v1.PodResizeStatusInfeasible
        }</span>

        // Treat the existing pod needing resize as a new pod with desired resources seeking admit.
        // If desired resources don't fit, pod continues to run with currently allocated resources.
        <span class="cov8" title="1">activePods := kl.GetActivePods()
        for _, p := range activePods </span><span class="cov8" title="1">{
                if p.UID != pod.UID </span><span class="cov8" title="1">{
                        otherActivePods = append(otherActivePods, p)
                }</span>
        }

        <span class="cov8" title="1">if ok, failReason, failMessage := kl.canAdmitPod(otherActivePods, podCopy); !ok </span><span class="cov8" title="1">{
                // Log reason and return. Let the next sync iteration retry the resize
                klog.V(3).InfoS("Resize cannot be accommodated", "pod", podCopy.Name, "reason", failReason, "message", failMessage)
                return false, podCopy, v1.PodResizeStatusDeferred
        }</span>

        <span class="cov8" title="1">for _, container := range podCopy.Spec.Containers </span><span class="cov8" title="1">{
                idx, found := podutil.GetIndexOfContainerStatus(podCopy.Status.ContainerStatuses, container.Name)
                if found </span><span class="cov8" title="1">{
                        for rName, rQuantity := range container.Resources.Requests </span><span class="cov8" title="1">{
                                podCopy.Status.ContainerStatuses[idx].AllocatedResources[rName] = rQuantity
                        }</span>
                }
        }
        <span class="cov8" title="1">return true, podCopy, v1.PodResizeStatusInProgress</span>
}

func (kl *Kubelet) handlePodResourcesResize(pod *v1.Pod) *v1.Pod <span class="cov8" title="1">{
        if pod.Status.Phase != v1.PodRunning </span><span class="cov0" title="0">{
                return pod
        }</span>
        <span class="cov8" title="1">podResized := false
        for _, container := range pod.Spec.Containers </span><span class="cov8" title="1">{
                if len(container.Resources.Requests) == 0 </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov8" title="1">containerStatus, found := podutil.GetContainerStatus(pod.Status.ContainerStatuses, container.Name)
                if !found </span><span class="cov0" title="0">{
                        klog.V(5).InfoS("ContainerStatus not found", "pod", pod.Name, "container", container.Name)
                        break</span>
                }
                <span class="cov8" title="1">if len(containerStatus.AllocatedResources) != len(container.Resources.Requests) </span><span class="cov0" title="0">{
                        klog.V(5).InfoS("ContainerStatus.AllocatedResources length mismatch", "pod", pod.Name, "container", container.Name)
                        break</span>
                }
                <span class="cov8" title="1">if !cmp.Equal(container.Resources.Requests, containerStatus.AllocatedResources) </span><span class="cov8" title="1">{
                        podResized = true
                        break</span>
                }
        }
        <span class="cov8" title="1">if !podResized </span><span class="cov0" title="0">{
                return pod
        }</span>

        <span class="cov8" title="1">kl.podResizeMutex.Lock()
        defer kl.podResizeMutex.Unlock()
        fit, updatedPod, resizeStatus := kl.canResizePod(pod)
        if updatedPod == nil </span><span class="cov0" title="0">{
                return pod
        }</span>
        <span class="cov8" title="1">if fit </span><span class="cov8" title="1">{
                // Update pod resource allocation checkpoint
                if err := kl.statusManager.SetPodAllocation(updatedPod); err != nil </span><span class="cov0" title="0">{
                        //TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate
                        klog.ErrorS(err, "SetPodAllocation failed", "pod", klog.KObj(updatedPod))
                        return pod
                }</span>
        }
        <span class="cov8" title="1">if resizeStatus != "" </span><span class="cov8" title="1">{
                // Save resize decision to checkpoint
                if err := kl.statusManager.SetPodResizeStatus(updatedPod.UID, resizeStatus); err != nil </span><span class="cov0" title="0">{
                        //TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate
                        klog.ErrorS(err, "SetPodResizeStatus failed", "pod", klog.KObj(updatedPod))
                        return pod
                }</span>
                <span class="cov8" title="1">updatedPod.Status.Resize = resizeStatus</span>
        }
        <span class="cov8" title="1">kl.podManager.UpdatePod(updatedPod)
        kl.statusManager.SetPodStatus(updatedPod, updatedPod.Status)
        return updatedPod</span>
}

// HandlePodResourcesResize handles the resizing of pod resources.
func (kl *Kubelet) HandlePodResourcesResize(pod *v1.Pod) *v1.Pod <span class="cov0" title="0">{

        return kl.handlePodResourcesResize(pod)
}</span>

// LatestLoopEntryTime returns the last time in the sync loop monitor.
func (kl *Kubelet) LatestLoopEntryTime() time.Time <span class="cov0" title="0">{
        val := kl.syncLoopMonitor.Load()
        if val == nil </span><span class="cov0" title="0">{
                return time.Time{}
        }</span>
        <span class="cov0" title="0">return val.(time.Time)</span>
}

// updateRuntimeUp calls the container runtime status callback, initializing
// the runtime dependent modules when the container runtime first comes up,
// and returns an error if the status check fails.  If the status check is OK,
// update the container runtime uptime in the kubelet runtimeState.
func (kl *Kubelet) updateRuntimeUp() <span class="cov0" title="0">{
        kl.updateRuntimeMux.Lock()
        defer kl.updateRuntimeMux.Unlock()
        ctx := context.Background()

        s, err := kl.containerRuntime.Status(ctx)
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Container runtime sanity check failed")
                return
        }</span>
        <span class="cov0" title="0">if s == nil </span><span class="cov0" title="0">{
                klog.ErrorS(nil, "Container runtime status is nil")
                return
        }</span>
        // Periodically log the whole runtime status for debugging.
        <span class="cov0" title="0">klog.V(4).InfoS("Container runtime status", "status", s)
        klogErrorS := klog.ErrorS
        if !kl.containerRuntimeReadyExpected </span><span class="cov0" title="0">{
                klogErrorS = klog.V(4).ErrorS
        }</span>
        <span class="cov0" title="0">networkReady := s.GetRuntimeCondition(kubecontainer.NetworkReady)
        if networkReady == nil || !networkReady.Status </span><span class="cov0" title="0">{
                klogErrorS(nil, "Container runtime network not ready", "networkReady", networkReady)
                kl.runtimeState.setNetworkState(fmt.Errorf("container runtime network not ready: %v", networkReady))
        }</span> else<span class="cov0" title="0"> {
                // Set nil if the container runtime network is ready.
                kl.runtimeState.setNetworkState(nil)
        }</span>
        // information in RuntimeReady condition will be propagated to NodeReady condition.
        <span class="cov0" title="0">runtimeReady := s.GetRuntimeCondition(kubecontainer.RuntimeReady)
        // If RuntimeReady is not set or is false, report an error.
        if runtimeReady == nil || !runtimeReady.Status </span><span class="cov0" title="0">{
                klogErrorS(nil, "Container runtime not ready", "runtimeReady", runtimeReady)
                kl.runtimeState.setRuntimeState(fmt.Errorf("container runtime not ready: %v", runtimeReady))
                return
        }</span>
        <span class="cov0" title="0">kl.runtimeState.setRuntimeState(nil)
        kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules)
        kl.runtimeState.setRuntimeSync(kl.clock.Now())</span>
}

// GetConfiguration returns the KubeletConfiguration used to configure the kubelet.
func (kl *Kubelet) GetConfiguration() kubeletconfiginternal.KubeletConfiguration <span class="cov8" title="1">{
        return kl.kubeletConfiguration
}</span>

// BirthCry sends an event that the kubelet has started up.
func (kl *Kubelet) BirthCry() <span class="cov8" title="1">{
        // Make an event that kubelet restarted.
        kl.recorder.Eventf(kl.nodeRef, v1.EventTypeNormal, events.StartingKubelet, "Starting kubelet.")
}</span>

// ResyncInterval returns the interval used for periodic syncs.
func (kl *Kubelet) ResyncInterval() time.Duration <span class="cov0" title="0">{
        return kl.resyncInterval
}</span>

// ListenAndServe runs the kubelet HTTP server.
func (kl *Kubelet) ListenAndServe(kubeCfg *kubeletconfiginternal.KubeletConfiguration, tlsOptions *server.TLSOptions,
        auth server.AuthInterface, tp trace.TracerProvider) <span class="cov0" title="0">{
        server.ListenAndServeKubeletServer(kl, kl.resourceAnalyzer, kubeCfg, tlsOptions, auth, tp)
}</span>

// ListenAndServeReadOnly runs the kubelet HTTP server in read-only mode.
func (kl *Kubelet) ListenAndServeReadOnly(address net.IP, port uint) <span class="cov0" title="0">{
        server.ListenAndServeKubeletReadOnlyServer(kl, kl.resourceAnalyzer, address, port)
}</span>

// ListenAndServePodResources runs the kubelet podresources grpc service
func (kl *Kubelet) ListenAndServePodResources() <span class="cov0" title="0">{
        endpoint, err := util.LocalEndpoint(kl.getPodResourcesDir(), podresources.Socket)
        if err != nil </span><span class="cov0" title="0">{
                klog.V(2).InfoS("Failed to get local endpoint for PodResources endpoint", "err", err)
                return
        }</span>

        <span class="cov0" title="0">providers := podresources.PodResourcesProviders{
                Pods:             kl.podManager,
                Devices:          kl.containerManager,
                Cpus:             kl.containerManager,
                Memory:           kl.containerManager,
                DynamicResources: kl.containerManager,
        }

        server.ListenAndServePodResources(endpoint, providers)</span>
}

// Delete the eligible dead container instances in a pod. Depending on the configuration, the latest dead containers may be kept around.
func (kl *Kubelet) cleanUpContainersInPod(podID types.UID, exitedContainerID string) <span class="cov0" title="0">{
        if podStatus, err := kl.podCache.Get(podID); err == nil </span><span class="cov0" title="0">{
                // When an evicted or deleted pod has already synced, all containers can be removed.
                removeAll := kl.podWorkers.ShouldPodContentBeRemoved(podID)
                kl.containerDeletor.deleteContainersInPod(exitedContainerID, podStatus, removeAll)
        }</span>
}

// fastStatusUpdateOnce starts a loop that checks if the current state of kubelet + container runtime
// would be able to turn the node ready, and sync the ready state to the apiserver as soon as possible.
// Function returns after the node status update after such event, or when the node is already ready.
// Function is executed only during Kubelet start which improves latency to ready node by updating
// kubelet state, runtime status and node statuses ASAP.
func (kl *Kubelet) fastStatusUpdateOnce() <span class="cov0" title="0">{
        ctx := context.Background()
        start := kl.clock.Now()
        stopCh := make(chan struct{})

        // Keep trying to make fast node status update until either timeout is reached or an update is successful.
        wait.Until(func() </span><span class="cov0" title="0">{
                // fastNodeStatusUpdate returns true when it succeeds or when the grace period has expired
                // (status was not updated within nodeReadyGracePeriod and the second argument below gets true),
                // then we close the channel and abort the loop.
                if kl.fastNodeStatusUpdate(ctx, kl.clock.Since(start) &gt;= nodeReadyGracePeriod) </span><span class="cov0" title="0">{
                        close(stopCh)
                }</span>
        }, 100*time.Millisecond, stopCh)
}

// CheckpointContainer tries to checkpoint a container. The parameters are used to
// look up the specified container. If the container specified by the given parameters
// cannot be found an error is returned. If the container is found the container
// engine will be asked to checkpoint the given container into the kubelet's default
// checkpoint directory.
func (kl *Kubelet) CheckpointContainer(
        ctx context.Context,
        podUID types.UID,
        podFullName,
        containerName string,
        options *runtimeapi.CheckpointContainerRequest,
) error <span class="cov8" title="1">{
        container, err := kl.findContainer(ctx, podFullName, podUID, containerName)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if container == nil </span><span class="cov8" title="1">{
                return fmt.Errorf("container %v not found", containerName)
        }</span>

        <span class="cov8" title="1">options.Location = filepath.Join(
                kl.getCheckpointsDir(),
                fmt.Sprintf(
                        "checkpoint-%s-%s-%s.tar",
                        podFullName,
                        containerName,
                        time.Now().Format(time.RFC3339),
                ),
        )

        options.ContainerId = string(container.ID.ID)

        if err := kl.containerRuntime.CheckpointContainer(ctx, options); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// ListMetricDescriptors gets the descriptors for the metrics that will be returned in ListPodSandboxMetrics.
func (kl *Kubelet) ListMetricDescriptors(ctx context.Context) ([]*runtimeapi.MetricDescriptor, error) <span class="cov0" title="0">{
        return kl.containerRuntime.ListMetricDescriptors(ctx)
}</span>

// ListPodSandboxMetrics retrieves the metrics for all pod sandboxes.
func (kl *Kubelet) ListPodSandboxMetrics(ctx context.Context) ([]*runtimeapi.PodSandboxMetrics, error) <span class="cov0" title="0">{
        return kl.containerRuntime.ListPodSandboxMetrics(ctx)
}</span>

func (kl *Kubelet) supportLocalStorageCapacityIsolation() bool <span class="cov8" title="1">{
        return kl.GetConfiguration().LocalStorageCapacityIsolation
}</span>

// SupportLocalStorageCapacityIsolation returns true if the kubelet supports local storage capacity isolation.
func (kl *Kubelet) SupportLocalStorageCapacityIsolation() bool <span class="cov0" title="0">{
        return kl.GetConfiguration().LocalStorageCapacityIsolation
}</span>

// isSyncPodWorthy filters out events that are not worthy of pod syncing
func isSyncPodWorthy(event *pleg.PodLifecycleEvent) bool <span class="cov0" title="0">{
        // ContainerRemoved doesn't affect pod state
        return event.Type != pleg.ContainerRemoved
}</span>

// PrepareDynamicResources calls the container Manager PrepareDynamicResources API
// This method implements the RuntimeHelper interface
func (kl *Kubelet) PrepareDynamicResources(pod *v1.Pod) error <span class="cov0" title="0">{
        return kl.containerManager.PrepareDynamicResources(pod)
}</span>

// UnprepareDynamicResources calls the container Manager UnprepareDynamicResources API
// This method implements the RuntimeHelper interface
func (kl *Kubelet) UnprepareDynamicResources(pod *v1.Pod) error <span class="cov0" title="0">{
        return kl.containerManager.UnprepareDynamicResources(pod)
}</span>
</pre>
		
		<pre class="file" id="file2" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "context"
        "fmt"
        "net"
        "os"
        "path/filepath"

        cadvisorapiv1 "github.com/google/cadvisor/info/v1"
        cadvisorv2 "github.com/google/cadvisor/info/v2"
        "k8s.io/klog/v2"
        "k8s.io/mount-utils"
        utilpath "k8s.io/utils/path"
        utilstrings "k8s.io/utils/strings"

        v1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/kubernetes/pkg/kubelet/cm"
        "k8s.io/kubernetes/pkg/kubelet/config"
        kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
        kubelettypes "k8s.io/kubernetes/pkg/kubelet/types"
        utilnode "k8s.io/kubernetes/pkg/util/node"
        "k8s.io/kubernetes/pkg/volume/csi"
)

// getRootDir returns the full path to the directory under which kubelet can
// store data.  These functions are useful to pass interfaces to other modules
// that may need to know where to write data without getting a whole kubelet
// instance.
func (kl *Kubelet) getRootDir() string <span class="cov8" title="1">{
        return kl.rootDirectory
}</span>

// getPodsDir returns the full path to the directory under which pod
// directories are created.
func (kl *Kubelet) getPodsDir() string <span class="cov8" title="1">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletPodsDirName)
}</span>

// GetPodsDirGetters returns the full path to the directory under which pod
// directories are created.
func (kl *Kubelet) GetPodsDirGetters() string <span class="cov0" title="0">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletPodsDirName)
}</span>

// getPluginsDir returns the full path to the directory under which plugin
// directories are created.  Plugins can use these directories for data that
// they need to persist.  Plugins should create subdirectories under this named
// after their own names.
func (kl *Kubelet) getPluginsDir() string <span class="cov8" title="1">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletPluginsDirName)
}</span>

// getPluginsRegistrationDir returns the full path to the directory under which
// plugins socket should be placed to be registered.
// More information is available about plugin registration in the pluginwatcher
// module
func (kl *Kubelet) getPluginsRegistrationDir() string <span class="cov8" title="1">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletPluginsRegistrationDirName)
}</span>

// GetPluginsRegistrationDir returns the full path to the directory under which
// plugins socket should be placed to be registered.
// More information is available about plugin registration in the pluginwatcher
// module
func (kl *Kubelet) GetPluginsRegistrationDir() string <span class="cov0" title="0">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletPluginsRegistrationDirName)
}</span>

// getPluginDir returns a data directory name for a given plugin name.
// Plugins can use these directories to store data that they need to persist.
// For per-pod plugin data, see getPodPluginDir.
func (kl *Kubelet) getPluginDir(pluginName string) string <span class="cov0" title="0">{
        return filepath.Join(kl.getPluginsDir(), pluginName)
}</span>

// getCheckpointsDir returns a data directory name for checkpoints.
// Checkpoints can be stored in this directory for further use.
func (kl *Kubelet) getCheckpointsDir() string <span class="cov8" title="1">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletCheckpointsDirName)
}</span>

func (kl *Kubelet) GetCheckpointsDir() string <span class="cov0" title="0">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletCheckpointsDirName)
}</span>

// getVolumeDevicePluginsDir returns the full path to the directory under which plugin
// directories are created.  Plugins can use these directories for data that
// they need to persist.  Plugins should create subdirectories under this named
// after their own names.
func (kl *Kubelet) getVolumeDevicePluginsDir() string <span class="cov0" title="0">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletPluginsDirName)
}</span>

// getVolumeDevicePluginDir returns a data directory name for a given plugin name.
// Plugins can use these directories to store data that they need to persist.
// For per-pod plugin data, see getVolumeDevicePluginsDir.
func (kl *Kubelet) getVolumeDevicePluginDir(pluginName string) string <span class="cov0" title="0">{
        return filepath.Join(kl.getVolumeDevicePluginsDir(), pluginName, config.DefaultKubeletVolumeDevicesDirName)
}</span>

// GetPodDir returns the full path to the per-pod data directory for the
// specified pod. This directory may not exist if the pod does not exist.
func (kl *Kubelet) GetPodDir(podUID types.UID) string <span class="cov0" title="0">{
        return kl.getPodDir(podUID)
}</span>

// ListPodsFromDisk gets a list of pods that have data directories.
func (kl *Kubelet) ListPodsFromDisk() ([]types.UID, error) <span class="cov0" title="0">{
        return kl.listPodsFromDisk()
}</span>

// getPodDir returns the full path to the per-pod directory for the pod with
// the given UID.
func (kl *Kubelet) getPodDir(podUID types.UID) string <span class="cov8" title="1">{
        return filepath.Join(kl.getPodsDir(), string(podUID))
}</span>

// getPodVolumesSubpathsDir returns the full path to the per-pod subpaths directory under
// which subpath volumes are created for the specified pod.  This directory may not
// exist if the pod does not exist or subpaths are not specified.
func (kl *Kubelet) getPodVolumeSubpathsDir(podUID types.UID) string <span class="cov8" title="1">{
        return filepath.Join(kl.getPodDir(podUID), config.DefaultKubeletVolumeSubpathsDirName)
}</span>

// getPodVolumesDir returns the full path to the per-pod data directory under
// which volumes are created for the specified pod.  This directory may not
// exist if the pod does not exist.
func (kl *Kubelet) getPodVolumesDir(podUID types.UID) string <span class="cov8" title="1">{
        return filepath.Join(kl.getPodDir(podUID), config.DefaultKubeletVolumesDirName)
}</span>

// getPodVolumeDir returns the full path to the directory which represents the
// named volume under the named plugin for specified pod.  This directory may not
// exist if the pod does not exist.
func (kl *Kubelet) getPodVolumeDir(podUID types.UID, pluginName string, volumeName string) string <span class="cov0" title="0">{
        return filepath.Join(kl.getPodVolumesDir(podUID), pluginName, volumeName)
}</span>

// getPodVolumeDevicesDir returns the full path to the per-pod data directory under
// which volumes are created for the specified pod. This directory may not
// exist if the pod does not exist.
func (kl *Kubelet) getPodVolumeDevicesDir(podUID types.UID) string <span class="cov0" title="0">{
        return filepath.Join(kl.getPodDir(podUID), config.DefaultKubeletVolumeDevicesDirName)
}</span>

// getPodVolumeDeviceDir returns the full path to the directory which represents the
// named plugin for specified pod. This directory may not exist if the pod does not exist.
func (kl *Kubelet) getPodVolumeDeviceDir(podUID types.UID, pluginName string) string <span class="cov0" title="0">{
        return filepath.Join(kl.getPodVolumeDevicesDir(podUID), pluginName)
}</span>

// getPodPluginsDir returns the full path to the per-pod data directory under
// which plugins may store data for the specified pod.  This directory may not
// exist if the pod does not exist.
func (kl *Kubelet) getPodPluginsDir(podUID types.UID) string <span class="cov8" title="1">{
        return filepath.Join(kl.getPodDir(podUID), config.DefaultKubeletPluginsDirName)
}</span>

// getPodPluginDir returns a data directory name for a given plugin name for a
// given pod UID.  Plugins can use these directories to store data that they
// need to persist.  For non-per-pod plugin data, see getPluginDir.
func (kl *Kubelet) getPodPluginDir(podUID types.UID, pluginName string) string <span class="cov0" title="0">{
        return filepath.Join(kl.getPodPluginsDir(podUID), pluginName)
}</span>

// getPodContainerDir returns the full path to the per-pod data directory under
// which container data is held for the specified pod.  This directory may not
// exist if the pod or container does not exist.
func (kl *Kubelet) getPodContainerDir(podUID types.UID, ctrName string) string <span class="cov0" title="0">{
        return filepath.Join(kl.getPodDir(podUID), config.DefaultKubeletContainersDirName, ctrName)
}</span>

// getPodResourcesSocket returns the full path to the directory containing the pod resources socket
func (kl *Kubelet) getPodResourcesDir() string <span class="cov8" title="1">{
        return filepath.Join(kl.getRootDir(), config.DefaultKubeletPodResourcesDirName)
}</span>

// GetPods returns all pods bound to the kubelet and their spec, and the mirror
// pods.
func (kl *Kubelet) GetPods() []*v1.Pod <span class="cov0" title="0">{
        pods := kl.podManager.GetPods()
        // a kubelet running without apiserver requires an additional
        // update of the static pod status. See #57106
        for i, p := range pods </span><span class="cov0" title="0">{
                if kubelettypes.IsStaticPod(p) </span><span class="cov0" title="0">{
                        if status, ok := kl.statusManager.GetPodStatus(p.UID); ok </span><span class="cov0" title="0">{
                                klog.V(2).InfoS("Pod status updated", "pod", klog.KObj(p), "status", status.Phase)
                                // do not mutate the cache
                                p = p.DeepCopy()
                                p.Status = status
                                pods[i] = p
                        }</span>
                }
        }
        <span class="cov0" title="0">return pods</span>
}

// GetRunningPods returns all pods running on kubelet from looking at the
// container runtime cache. This function converts kubecontainer.Pod to
// v1.Pod, so only the fields that exist in both kubecontainer.Pod and
// v1.Pod are considered meaningful.
func (kl *Kubelet) GetRunningPods(ctx context.Context) ([]*v1.Pod, error) <span class="cov0" title="0">{
        pods, err := kl.runtimeCache.GetPods(ctx)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov0" title="0">apiPods := make([]*v1.Pod, 0, len(pods))
        for _, pod := range pods </span><span class="cov0" title="0">{
                apiPods = append(apiPods, pod.ToAPIPod())
        }</span>
        <span class="cov0" title="0">return apiPods, nil</span>
}

// GetPodByFullName gets the pod with the given 'full' name, which
// incorporates the namespace as well as whether the pod was found.
func (kl *Kubelet) GetPodByFullName(podFullName string) (*v1.Pod, bool) <span class="cov0" title="0">{
        return kl.podManager.GetPodByFullName(podFullName)
}</span>

// GetPodByName provides the first pod that matches namespace and name, as well
// as whether the pod was found.
func (kl *Kubelet) GetPodByName(namespace, name string) (*v1.Pod, bool) <span class="cov0" title="0">{
        return kl.podManager.GetPodByName(namespace, name)
}</span>

// GetPodByCgroupfs provides the pod that maps to the specified cgroup, as well
// as whether the pod was found.
func (kl *Kubelet) GetPodByCgroupfs(cgroupfs string) (*v1.Pod, bool) <span class="cov0" title="0">{
        pcm := kl.containerManager.NewPodContainerManager()
        if result, podUID := pcm.IsPodCgroup(cgroupfs); result </span><span class="cov0" title="0">{
                return kl.podManager.GetPodByUID(podUID)
        }</span>
        <span class="cov0" title="0">return nil, false</span>
}

// GetHostname Returns the hostname as the kubelet sees it.
func (kl *Kubelet) GetHostname() string <span class="cov0" title="0">{
        return kl.hostname
}</span>

// getRuntime returns the current Runtime implementation in use by the kubelet.
func (kl *Kubelet) getRuntime() kubecontainer.Runtime <span class="cov8" title="1">{
        return kl.containerRuntime
}</span>

// GetNode returns the node info for the configured node name of this Kubelet.
func (kl *Kubelet) GetNode() (*v1.Node, error) <span class="cov8" title="1">{
        if kl.kubeClient == nil </span><span class="cov0" title="0">{
                return kl.initialNode(context.TODO())
        }</span>
        <span class="cov8" title="1">return kl.nodeLister.Get(string(kl.nodeName))</span>
}

// getNodeAnyWay() must return a *v1.Node which is required by RunGeneralPredicates().
// The *v1.Node is obtained as follows:
// Return kubelet's nodeInfo for this node, except on error or if in standalone mode,
// in which case return a manufactured nodeInfo representing a node with no pods,
// zero capacity, and the default labels.
func (kl *Kubelet) getNodeAnyWay() (*v1.Node, error) <span class="cov8" title="1">{
        if kl.kubeClient != nil </span><span class="cov8" title="1">{
                if n, err := kl.nodeLister.Get(string(kl.nodeName)); err == nil </span><span class="cov8" title="1">{
                        return n, nil
                }</span>
        }
        <span class="cov0" title="0">return kl.initialNode(context.TODO())</span>
}

// GetNodeAnyWay attempts to return the node from kubelet's nodeInfo, or the
func (kl *Kubelet) GetNodeAnyWay() (*v1.Node, error) <span class="cov0" title="0">{
        if kl.kubeClient != nil </span><span class="cov0" title="0">{
                if n, err := kl.nodeLister.Get(string(kl.nodeName)); err == nil </span><span class="cov0" title="0">{
                        return n, nil
                }</span>
        }
        <span class="cov0" title="0">return kl.initialNode(context.TODO())</span>
}

// GetNodeConfig returns the container manager node config.
func (kl *Kubelet) GetNodeConfig() cm.NodeConfig <span class="cov0" title="0">{
        return kl.containerManager.GetNodeConfig()
}</span>

// GetPodCgroupRoot returns the listeral cgroupfs value for the cgroup containing all pods
func (kl *Kubelet) GetPodCgroupRoot() string <span class="cov0" title="0">{
        return kl.containerManager.GetPodCgroupRoot()
}</span>

// GetHostIPs returns host IPs or nil in case of error.
func (kl *Kubelet) GetHostIPs() ([]net.IP, error) <span class="cov0" title="0">{
        node, err := kl.GetNode()
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("cannot get node: %v", err)
        }</span>
        <span class="cov0" title="0">return utilnode.GetNodeHostIPs(node)</span>
}

// getHostIPsAnyWay attempts to return the host IPs from kubelet's nodeInfo, or
// the initialNode.
func (kl *Kubelet) getHostIPsAnyWay() ([]net.IP, error) <span class="cov8" title="1">{
        node, err := kl.getNodeAnyWay()
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">return utilnode.GetNodeHostIPs(node)</span>
}

// GetExtraSupplementalGroupsForPod returns a list of the extra
// supplemental groups for the Pod. These extra supplemental groups come
// from annotations on persistent volumes that the pod depends on.
func (kl *Kubelet) GetExtraSupplementalGroupsForPod(pod *v1.Pod) []int64 <span class="cov8" title="1">{
        return kl.volumeManager.GetExtraSupplementalGroupsForPod(pod)
}</span>

// getPodVolumePathListFromDisk returns a list of the volume paths by reading the
// volume directories for the given pod from the disk.
func (kl *Kubelet) getPodVolumePathListFromDisk(podUID types.UID) ([]string, error) <span class="cov8" title="1">{
        volumes := []string{}
        podVolDir := kl.getPodVolumesDir(podUID)

        if pathExists, pathErr := mount.PathExists(podVolDir); pathErr != nil </span><span class="cov0" title="0">{
                return volumes, fmt.Errorf("error checking if path %q exists: %v", podVolDir, pathErr)
        }</span> else<span class="cov8" title="1"> if !pathExists </span><span class="cov8" title="1">{
                klog.V(6).InfoS("Path does not exist", "path", podVolDir)
                return volumes, nil
        }</span>

        <span class="cov8" title="1">volumePluginDirs, err := os.ReadDir(podVolDir)
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Could not read directory", "path", podVolDir)
                return volumes, err
        }</span>
        <span class="cov8" title="1">for _, volumePluginDir := range volumePluginDirs </span><span class="cov0" title="0">{
                volumePluginName := volumePluginDir.Name()
                volumePluginPath := filepath.Join(podVolDir, volumePluginName)
                volumeDirs, err := utilpath.ReadDirNoStat(volumePluginPath)
                if err != nil </span><span class="cov0" title="0">{
                        return volumes, fmt.Errorf("could not read directory %s: %v", volumePluginPath, err)
                }</span>
                <span class="cov0" title="0">unescapePluginName := utilstrings.UnescapeQualifiedName(volumePluginName)

                if unescapePluginName != csi.CSIPluginName </span><span class="cov0" title="0">{
                        for _, volumeDir := range volumeDirs </span><span class="cov0" title="0">{
                                volumes = append(volumes, filepath.Join(volumePluginPath, volumeDir))
                        }</span>
                } else<span class="cov0" title="0"> {
                        // For CSI volumes, the mounted volume path has an extra sub path "/mount", so also add it
                        // to the list if the mounted path exists.
                        for _, volumeDir := range volumeDirs </span><span class="cov0" title="0">{
                                path := filepath.Join(volumePluginPath, volumeDir)
                                csimountpath := csi.GetCSIMounterPath(path)
                                if pathExists, _ := mount.PathExists(csimountpath); pathExists </span><span class="cov0" title="0">{
                                        volumes = append(volumes, csimountpath)
                                }</span>
                        }
                }
        }
        <span class="cov8" title="1">return volumes, nil</span>
}

func (kl *Kubelet) getMountedVolumePathListFromDisk(podUID types.UID) ([]string, error) <span class="cov8" title="1">{
        mountedVolumes := []string{}
        volumePaths, err := kl.getPodVolumePathListFromDisk(podUID)
        if err != nil </span><span class="cov0" title="0">{
                return mountedVolumes, err
        }</span>
        // Only use IsLikelyNotMountPoint to check might not cover all cases. For CSI volumes that
        // either: 1) don't mount or 2) bind mount in the rootfs, the mount check will not work as expected.
        // We plan to remove this mountpoint check as a condition before deleting pods since it is
        // not reliable and the condition might be different for different types of volumes. But it requires
        // a reliable way to clean up unused volume dir to avoid problems during pod deletion. See discussion in issue #74650
        <span class="cov8" title="1">for _, volumePath := range volumePaths </span><span class="cov0" title="0">{
                isNotMount, err := kl.mounter.IsLikelyNotMountPoint(volumePath)
                if err != nil </span><span class="cov0" title="0">{
                        return mountedVolumes, fmt.Errorf("fail to check mount point %q: %v", volumePath, err)
                }</span>
                <span class="cov0" title="0">if !isNotMount </span><span class="cov0" title="0">{
                        mountedVolumes = append(mountedVolumes, volumePath)
                }</span>
        }
        <span class="cov8" title="1">return mountedVolumes, nil</span>
}

// getPodVolumeSubpathListFromDisk returns a list of the volume-subpath paths by reading the
// subpath directories for the given pod from the disk.
func (kl *Kubelet) getPodVolumeSubpathListFromDisk(podUID types.UID) ([]string, error) <span class="cov8" title="1">{
        volumes := []string{}
        podSubpathsDir := kl.getPodVolumeSubpathsDir(podUID)

        if pathExists, pathErr := mount.PathExists(podSubpathsDir); pathErr != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("error checking if path %q exists: %v", podSubpathsDir, pathErr)
        }</span> else<span class="cov8" title="1"> if !pathExists </span><span class="cov8" title="1">{
                return volumes, nil
        }</span>

        // Explicitly walks /&lt;volume&gt;/&lt;container name&gt;/&lt;subPathIndex&gt;
        <span class="cov0" title="0">volumePluginDirs, err := os.ReadDir(podSubpathsDir)
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Could not read directory", "path", podSubpathsDir)
                return volumes, err
        }</span>
        <span class="cov0" title="0">for _, volumePluginDir := range volumePluginDirs </span><span class="cov0" title="0">{
                volumePluginName := volumePluginDir.Name()
                volumePluginPath := filepath.Join(podSubpathsDir, volumePluginName)
                containerDirs, err := os.ReadDir(volumePluginPath)
                if err != nil </span><span class="cov0" title="0">{
                        return volumes, fmt.Errorf("could not read directory %s: %v", volumePluginPath, err)
                }</span>
                <span class="cov0" title="0">for _, containerDir := range containerDirs </span><span class="cov0" title="0">{
                        containerName := containerDir.Name()
                        containerPath := filepath.Join(volumePluginPath, containerName)
                        // Switch to ReadDirNoStat at the subPathIndex level to prevent issues with stat'ing
                        // mount points that may not be responsive
                        subPaths, err := utilpath.ReadDirNoStat(containerPath)
                        if err != nil </span><span class="cov0" title="0">{
                                return volumes, fmt.Errorf("could not read directory %s: %v", containerPath, err)
                        }</span>
                        <span class="cov0" title="0">for _, subPathDir := range subPaths </span><span class="cov0" title="0">{
                                volumes = append(volumes, filepath.Join(containerPath, subPathDir))
                        }</span>
                }
        }
        <span class="cov0" title="0">return volumes, nil</span>
}

// GetRequestedContainersInfo returns container info.
func (kl *Kubelet) GetRequestedContainersInfo(containerName string, options cadvisorv2.RequestOptions) (map[string]*cadvisorapiv1.ContainerInfo, error) <span class="cov0" title="0">{
        return kl.cadvisor.GetRequestedContainersInfo(containerName, options)
}</span>

// GetVersionInfo returns information about the version of cAdvisor in use.
func (kl *Kubelet) GetVersionInfo() (*cadvisorapiv1.VersionInfo, error) <span class="cov0" title="0">{
        return kl.cadvisor.VersionInfo()
}</span>

// GetCachedMachineInfo assumes that the machine info can't change without a reboot
func (kl *Kubelet) GetCachedMachineInfo() (*cadvisorapiv1.MachineInfo, error) <span class="cov8" title="1">{
        kl.machineInfoLock.RLock()
        defer kl.machineInfoLock.RUnlock()
        return kl.machineInfo, nil
}</span>

func (kl *Kubelet) setCachedMachineInfo(info *cadvisorapiv1.MachineInfo) <span class="cov8" title="1">{
        kl.machineInfoLock.Lock()
        defer kl.machineInfoLock.Unlock()
        kl.machineInfo = info
}</span>

func (kl *Kubelet) SetCachedMachineInfo(info *cadvisorapiv1.MachineInfo) <span class="cov0" title="0">{
        kl.machineInfoLock.Lock()
        defer kl.machineInfoLock.Unlock()
        kl.machineInfo = info
}</span>
</pre>
		
		<pre class="file" id="file3" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "context"
        "fmt"

        v1 "k8s.io/api/core/v1"
        runtimeapi "k8s.io/cri-api/pkg/apis/runtime/v1"
        "k8s.io/klog/v2"
)

// providerRequiresNetworkingConfiguration returns whether the cloud provider
// requires special networking configuration.
func (kl *Kubelet) providerRequiresNetworkingConfiguration() bool <span class="cov0" title="0">{
        // TODO: We should have a mechanism to say whether native cloud provider
        // is used or whether we are using overlay networking. We should return
        // true for cloud providers if they implement Routes() interface and
        // we are not using overlay networking.
        if kl.cloud == nil || kl.cloud.ProviderName() != "gce" </span><span class="cov0" title="0">{
                return false
        }</span>
        <span class="cov0" title="0">_, supported := kl.cloud.Routes()
        return supported</span>
}

// updatePodCIDR updates the pod CIDR in the runtime state if it is different
// from the current CIDR. Return true if pod CIDR is actually changed.
func (kl *Kubelet) updatePodCIDR(ctx context.Context, cidr string) (bool, error) <span class="cov8" title="1">{
        kl.updatePodCIDRMux.Lock()
        defer kl.updatePodCIDRMux.Unlock()

        podCIDR := kl.runtimeState.podCIDR()

        if podCIDR == cidr </span><span class="cov8" title="1">{
                return false, nil
        }</span>

        // kubelet -&gt; generic runtime -&gt; runtime shim -&gt; network plugin
        // docker/non-cri implementations have a passthrough UpdatePodCIDR
        <span class="cov0" title="0">if err := kl.getRuntime().UpdatePodCIDR(ctx, cidr); err != nil </span><span class="cov0" title="0">{
                // If updatePodCIDR would fail, theoretically pod CIDR could not change.
                // But it is better to be on the safe side to still return true here.
                return true, fmt.Errorf("failed to update pod CIDR: %v", err)
        }</span>
        <span class="cov0" title="0">klog.InfoS("Updating Pod CIDR", "originalPodCIDR", podCIDR, "newPodCIDR", cidr)
        kl.runtimeState.setPodCIDR(cidr)
        return true, nil</span>
}

// GetPodDNS returns DNS settings for the pod.
// This function is defined in kubecontainer.RuntimeHelper interface so we
// have to implement it.
func (kl *Kubelet) GetPodDNS(pod *v1.Pod) (*runtimeapi.DNSConfig, error) <span class="cov8" title="1">{
        return kl.dnsConfigurer.GetPodDNS(pod)
}</span>
</pre>
		
		<pre class="file" id="file4" style="display: none">//go:build linux
// +build linux

/*
Copyright 2018 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "time"

        "k8s.io/apimachinery/pkg/util/wait"
        "k8s.io/klog/v2"
        utiliptables "k8s.io/kubernetes/pkg/util/iptables"
        utilexec "k8s.io/utils/exec"
)

const (
        // KubeIPTablesHintChain is the chain whose existence in either iptables-legacy
        // or iptables-nft indicates which version of iptables the system is using
        KubeIPTablesHintChain utiliptables.Chain = "KUBE-IPTABLES-HINT"

        // KubeFirewallChain is kubernetes firewall rules
        KubeFirewallChain utiliptables.Chain = "KUBE-FIREWALL"
)

func (kl *Kubelet) initNetworkUtil() <span class="cov0" title="0">{
        exec := utilexec.New()
        iptClients := []utiliptables.Interface{
                utiliptables.New(exec, utiliptables.ProtocolIPv4),
                utiliptables.New(exec, utiliptables.ProtocolIPv6),
        }

        for i := range iptClients </span><span class="cov0" title="0">{
                iptClient := iptClients[i]
                if kl.syncIPTablesRules(iptClient) </span><span class="cov0" title="0">{
                        klog.InfoS("Initialized iptables rules.", "protocol", iptClient.Protocol())
                        go iptClient.Monitor(
                                utiliptables.Chain("KUBE-KUBELET-CANARY"),
                                []utiliptables.Table{utiliptables.TableMangle, utiliptables.TableNAT, utiliptables.TableFilter},
                                func() </span><span class="cov0" title="0">{ kl.syncIPTablesRules(iptClient) }</span>,
                                1*time.Minute, wait.NeverStop,
                        )
                } else<span class="cov0" title="0"> {
                        klog.InfoS("Failed to initialize iptables rules; some functionality may be missing.", "protocol", iptClient.Protocol())
                }</span>
        }
}

// syncIPTablesRules ensures the KUBE-IPTABLES-HINT chain exists, and the martian packet
// protection rule is installed.
func (kl *Kubelet) syncIPTablesRules(iptClient utiliptables.Interface) bool <span class="cov0" title="0">{
        // Create hint chain so other components can see whether we are using iptables-legacy
        // or iptables-nft.
        if _, err := iptClient.EnsureChain(utiliptables.TableMangle, KubeIPTablesHintChain); err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Failed to ensure that iptables hint chain exists")
                return false
        }</span>

        <span class="cov0" title="0">if !iptClient.IsIPv6() </span><span class="cov0" title="0">{ // ipv6 doesn't have this issue
                // Set up the KUBE-FIREWALL chain and martian packet protection rule.
                // (See below.)

                // NOTE: kube-proxy (in iptables mode) creates an identical copy of this
                // rule. If you want to change this rule in the future, you MUST do so in
                // a way that will interoperate correctly with skewed versions of the rule
                // created by kube-proxy.

                if _, err := iptClient.EnsureChain(utiliptables.TableFilter, KubeFirewallChain); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Failed to ensure that filter table KUBE-FIREWALL chain exists")
                        return false
                }</span>

                <span class="cov0" title="0">if _, err := iptClient.EnsureRule(utiliptables.Prepend, utiliptables.TableFilter, utiliptables.ChainOutput, "-j", string(KubeFirewallChain)); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Failed to ensure that OUTPUT chain jumps to KUBE-FIREWALL")
                        return false
                }</span>
                <span class="cov0" title="0">if _, err := iptClient.EnsureRule(utiliptables.Prepend, utiliptables.TableFilter, utiliptables.ChainInput, "-j", string(KubeFirewallChain)); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Failed to ensure that INPUT chain jumps to KUBE-FIREWALL")
                        return false
                }</span>

                // Kube-proxy's use of `route_localnet` to enable NodePorts on localhost
                // creates a security hole (https://issue.k8s.io/90259) which this
                // iptables rule mitigates. This rule should have been added to
                // kube-proxy, but it mistakenly ended up in kubelet instead, and we are
                // keeping it in kubelet for now in case other third-party components
                // depend on it.
                <span class="cov0" title="0">if _, err := iptClient.EnsureRule(utiliptables.Append, utiliptables.TableFilter, KubeFirewallChain,
                        "-m", "comment", "--comment", "block incoming localnet connections",
                        "--dst", "127.0.0.0/8",
                        "!", "--src", "127.0.0.0/8",
                        "-m", "conntrack",
                        "!", "--ctstate", "RELATED,ESTABLISHED,DNAT",
                        "-j", "DROP"); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Failed to ensure rule to drop invalid localhost packets in filter table KUBE-FIREWALL chain")
                        return false
                }</span>
        }

        <span class="cov0" title="0">return true</span>
}
</pre>
		
		<pre class="file" id="file5" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "context"
        "fmt"
        "net"
        goruntime "runtime"
        "sort"
        "strings"
        "time"

        v1 "k8s.io/api/core/v1"
        apiequality "k8s.io/apimachinery/pkg/api/equality"
        apierrors "k8s.io/apimachinery/pkg/api/errors"
        "k8s.io/apimachinery/pkg/api/resource"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/util/sets"
        cloudprovider "k8s.io/cloud-provider"
        cloudproviderapi "k8s.io/cloud-provider/api"
        nodeutil "k8s.io/component-helpers/node/util"
        "k8s.io/klog/v2"
        kubeletapis "k8s.io/kubelet/pkg/apis"
        v1helper "k8s.io/kubernetes/pkg/apis/core/v1/helper"
        "k8s.io/kubernetes/pkg/kubelet/events"
        "k8s.io/kubernetes/pkg/kubelet/nodestatus"
        "k8s.io/kubernetes/pkg/kubelet/util"
        taintutil "k8s.io/kubernetes/pkg/util/taints"
        volutil "k8s.io/kubernetes/pkg/volume/util"
)

// registerWithAPIServer registers the node with the cluster master. It is safe
// to call multiple times, but not concurrently (kl.registrationCompleted is
// not locked).
func (kl *Kubelet) registerWithAPIServer() <span class="cov0" title="0">{
        if kl.registrationCompleted </span><span class="cov0" title="0">{
                return
        }</span>

        <span class="cov0" title="0">kl.nodeStartupLatencyTracker.RecordAttemptRegisterNode()

        step := 100 * time.Millisecond

        for </span><span class="cov0" title="0">{
                time.Sleep(step)
                step = step * 2
                if step &gt;= 7*time.Second </span><span class="cov0" title="0">{
                        step = 7 * time.Second
                }</span>

                <span class="cov0" title="0">node, err := kl.initialNode(context.TODO())
                if err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Unable to construct v1.Node object for kubelet")
                        continue</span>
                }

                <span class="cov0" title="0">klog.InfoS("Attempting to register node", "node", klog.KObj(node))
                registered := kl.tryRegisterWithAPIServer(node)
                if registered </span><span class="cov0" title="0">{
                        klog.InfoS("Successfully registered node", "node", klog.KObj(node))
                        kl.registrationCompleted = true
                        return
                }</span>
        }
}

// tryRegisterWithAPIServer makes an attempt to register the given node with
// the API server, returning a boolean indicating whether the attempt was
// successful.  If a node with the same name already exists, it reconciles the
// value of the annotation for controller-managed attach-detach of attachable
// persistent volumes for the node.
func (kl *Kubelet) tryRegisterWithAPIServer(node *v1.Node) bool <span class="cov0" title="0">{
        _, err := kl.kubeClient.CoreV1().Nodes().Create(context.TODO(), node, metav1.CreateOptions{})
        if err == nil </span><span class="cov0" title="0">{
                kl.nodeStartupLatencyTracker.RecordRegisteredNewNode()
                return true
        }</span>

        <span class="cov0" title="0">if !apierrors.IsAlreadyExists(err) </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Unable to register node with API server", "node", klog.KObj(node))
                return false
        }</span>

        <span class="cov0" title="0">existingNode, err := kl.kubeClient.CoreV1().Nodes().Get(context.TODO(), string(kl.nodeName), metav1.GetOptions{})
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Unable to register node with API server, error getting existing node", "node", klog.KObj(node))
                return false
        }</span>
        <span class="cov0" title="0">if existingNode == nil </span><span class="cov0" title="0">{
                klog.InfoS("Unable to register node with API server, no node instance returned", "node", klog.KObj(node))
                return false
        }</span>

        <span class="cov0" title="0">originalNode := existingNode.DeepCopy()

        klog.InfoS("Node was previously registered", "node", klog.KObj(node))

        // Edge case: the node was previously registered; reconcile
        // the value of the controller-managed attach-detach
        // annotation.
        requiresUpdate := kl.reconcileCMADAnnotationWithExistingNode(node, existingNode)
        requiresUpdate = kl.updateDefaultLabels(node, existingNode) || requiresUpdate
        requiresUpdate = kl.reconcileExtendedResource(node, existingNode) || requiresUpdate
        requiresUpdate = kl.reconcileHugePageResource(node, existingNode) || requiresUpdate
        if requiresUpdate </span><span class="cov0" title="0">{
                if _, _, err := nodeutil.PatchNodeStatus(kl.kubeClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, existingNode); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Unable to reconcile node with API server,error updating node", "node", klog.KObj(node))
                        return false
                }</span>
        }

        <span class="cov0" title="0">return true</span>
}

// reconcileHugePageResource will update huge page capacity for each page size and remove huge page sizes no longer supported
func (kl *Kubelet) reconcileHugePageResource(initialNode, existingNode *v1.Node) bool <span class="cov0" title="0">{
        requiresUpdate := updateDefaultResources(initialNode, existingNode)
        supportedHugePageResources := sets.String{}

        for resourceName := range initialNode.Status.Capacity </span><span class="cov0" title="0">{
                if !v1helper.IsHugePageResourceName(resourceName) </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">supportedHugePageResources.Insert(string(resourceName))

                initialCapacity := initialNode.Status.Capacity[resourceName]
                initialAllocatable := initialNode.Status.Allocatable[resourceName]

                capacity, resourceIsSupported := existingNode.Status.Capacity[resourceName]
                allocatable := existingNode.Status.Allocatable[resourceName]

                // Add or update capacity if it the size was previously unsupported or has changed
                if !resourceIsSupported || capacity.Cmp(initialCapacity) != 0 </span><span class="cov0" title="0">{
                        existingNode.Status.Capacity[resourceName] = initialCapacity.DeepCopy()
                        requiresUpdate = true
                }</span>

                // Add or update allocatable if it the size was previously unsupported or has changed
                <span class="cov0" title="0">if !resourceIsSupported || allocatable.Cmp(initialAllocatable) != 0 </span><span class="cov0" title="0">{
                        existingNode.Status.Allocatable[resourceName] = initialAllocatable.DeepCopy()
                        requiresUpdate = true
                }</span>

        }

        <span class="cov0" title="0">for resourceName := range existingNode.Status.Capacity </span><span class="cov0" title="0">{
                if !v1helper.IsHugePageResourceName(resourceName) </span><span class="cov0" title="0">{
                        continue</span>
                }

                // If huge page size no longer is supported, we remove it from the node
                <span class="cov0" title="0">if !supportedHugePageResources.Has(string(resourceName)) </span><span class="cov0" title="0">{
                        delete(existingNode.Status.Capacity, resourceName)
                        delete(existingNode.Status.Allocatable, resourceName)
                        klog.InfoS("Removing huge page resource which is no longer supported", "resourceName", resourceName)
                        requiresUpdate = true
                }</span>
        }
        <span class="cov0" title="0">return requiresUpdate</span>
}

// Zeros out extended resource capacity during reconciliation.
func (kl *Kubelet) reconcileExtendedResource(initialNode, node *v1.Node) bool <span class="cov0" title="0">{
        requiresUpdate := updateDefaultResources(initialNode, node)
        // Check with the device manager to see if node has been recreated, in which case extended resources should be zeroed until they are available
        if kl.containerManager.ShouldResetExtendedResourceCapacity() </span><span class="cov0" title="0">{
                for k := range node.Status.Capacity </span><span class="cov0" title="0">{
                        if v1helper.IsExtendedResourceName(k) </span><span class="cov0" title="0">{
                                klog.InfoS("Zero out resource capacity in existing node", "resourceName", k, "node", klog.KObj(node))
                                node.Status.Capacity[k] = *resource.NewQuantity(int64(0), resource.DecimalSI)
                                node.Status.Allocatable[k] = *resource.NewQuantity(int64(0), resource.DecimalSI)
                                requiresUpdate = true
                        }</span>
                }
        }
        <span class="cov0" title="0">return requiresUpdate</span>
}

// updateDefaultResources will set the default resources on the existing node according to the initial node
func updateDefaultResources(initialNode, existingNode *v1.Node) bool <span class="cov0" title="0">{
        requiresUpdate := false
        if existingNode.Status.Capacity == nil </span><span class="cov0" title="0">{
                if initialNode.Status.Capacity != nil </span><span class="cov0" title="0">{
                        existingNode.Status.Capacity = initialNode.Status.Capacity.DeepCopy()
                        requiresUpdate = true
                }</span> else<span class="cov0" title="0"> {
                        existingNode.Status.Capacity = make(map[v1.ResourceName]resource.Quantity)
                }</span>
        }

        <span class="cov0" title="0">if existingNode.Status.Allocatable == nil </span><span class="cov0" title="0">{
                if initialNode.Status.Allocatable != nil </span><span class="cov0" title="0">{
                        existingNode.Status.Allocatable = initialNode.Status.Allocatable.DeepCopy()
                        requiresUpdate = true
                }</span> else<span class="cov0" title="0"> {
                        existingNode.Status.Allocatable = make(map[v1.ResourceName]resource.Quantity)
                }</span>
        }
        <span class="cov0" title="0">return requiresUpdate</span>
}

// updateDefaultLabels will set the default labels on the node
func (kl *Kubelet) updateDefaultLabels(initialNode, existingNode *v1.Node) bool <span class="cov0" title="0">{
        defaultLabels := []string{
                v1.LabelHostname,
                v1.LabelTopologyZone,
                v1.LabelTopologyRegion,
                v1.LabelFailureDomainBetaZone,
                v1.LabelFailureDomainBetaRegion,
                v1.LabelInstanceTypeStable,
                v1.LabelInstanceType,
                v1.LabelOSStable,
                v1.LabelArchStable,
                v1.LabelWindowsBuild,
                kubeletapis.LabelOS,
                kubeletapis.LabelArch,
        }

        needsUpdate := false
        if existingNode.Labels == nil </span><span class="cov0" title="0">{
                existingNode.Labels = make(map[string]string)
        }</span>
        //Set default labels but make sure to not set labels with empty values
        <span class="cov0" title="0">for _, label := range defaultLabels </span><span class="cov0" title="0">{
                if _, hasInitialValue := initialNode.Labels[label]; !hasInitialValue </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov0" title="0">if existingNode.Labels[label] != initialNode.Labels[label] </span><span class="cov0" title="0">{
                        existingNode.Labels[label] = initialNode.Labels[label]
                        needsUpdate = true
                }</span>

                <span class="cov0" title="0">if existingNode.Labels[label] == "" </span><span class="cov0" title="0">{
                        delete(existingNode.Labels, label)
                }</span>
        }

        <span class="cov0" title="0">return needsUpdate</span>
}

// reconcileCMADAnnotationWithExistingNode reconciles the controller-managed
// attach-detach annotation on a new node and the existing node, returning
// whether the existing node must be updated.
func (kl *Kubelet) reconcileCMADAnnotationWithExistingNode(node, existingNode *v1.Node) bool <span class="cov0" title="0">{
        var (
                existingCMAAnnotation    = existingNode.Annotations[volutil.ControllerManagedAttachAnnotation]
                newCMAAnnotation, newSet = node.Annotations[volutil.ControllerManagedAttachAnnotation]
        )

        if newCMAAnnotation == existingCMAAnnotation </span><span class="cov0" title="0">{
                return false
        }</span>

        // If the just-constructed node and the existing node do
        // not have the same value, update the existing node with
        // the correct value of the annotation.
        <span class="cov0" title="0">if !newSet </span><span class="cov0" title="0">{
                klog.InfoS("Controller attach-detach setting changed to false; updating existing Node")
                delete(existingNode.Annotations, volutil.ControllerManagedAttachAnnotation)
        }</span> else<span class="cov0" title="0"> {
                klog.InfoS("Controller attach-detach setting changed to true; updating existing Node")
                if existingNode.Annotations == nil </span><span class="cov0" title="0">{
                        existingNode.Annotations = make(map[string]string)
                }</span>
                <span class="cov0" title="0">existingNode.Annotations[volutil.ControllerManagedAttachAnnotation] = newCMAAnnotation</span>
        }

        <span class="cov0" title="0">return true</span>
}

// initialNode constructs the initial v1.Node for this Kubelet, incorporating node
// labels, information from the cloud provider, and Kubelet configuration.
func (kl *Kubelet) initialNode(ctx context.Context) (*v1.Node, error) <span class="cov0" title="0">{
        node := &amp;v1.Node{
                ObjectMeta: metav1.ObjectMeta{
                        Name: string(kl.nodeName),
                        Labels: map[string]string{
                                v1.LabelHostname:      kl.hostname,
                                v1.LabelOSStable:      goruntime.GOOS,
                                v1.LabelArchStable:    goruntime.GOARCH,
                                kubeletapis.LabelOS:   goruntime.GOOS,
                                kubeletapis.LabelArch: goruntime.GOARCH,
                        },
                },
                Spec: v1.NodeSpec{
                        Unschedulable: !kl.registerSchedulable,
                },
        }
        osLabels, err := getOSSpecificLabels()
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">for label, value := range osLabels </span><span class="cov0" title="0">{
                node.Labels[label] = value
        }</span>

        <span class="cov0" title="0">nodeTaints := make([]v1.Taint, len(kl.registerWithTaints))
        copy(nodeTaints, kl.registerWithTaints)
        unschedulableTaint := v1.Taint{
                Key:    v1.TaintNodeUnschedulable,
                Effect: v1.TaintEffectNoSchedule,
        }

        // Taint node with TaintNodeUnschedulable when initializing
        // node to avoid race condition; refer to #63897 for more detail.
        if node.Spec.Unschedulable &amp;&amp;
                !taintutil.TaintExists(nodeTaints, &amp;unschedulableTaint) </span><span class="cov0" title="0">{
                nodeTaints = append(nodeTaints, unschedulableTaint)
        }</span>

        <span class="cov0" title="0">if kl.externalCloudProvider </span><span class="cov0" title="0">{
                taint := v1.Taint{
                        Key:    cloudproviderapi.TaintExternalCloudProvider,
                        Value:  "true",
                        Effect: v1.TaintEffectNoSchedule,
                }

                nodeTaints = append(nodeTaints, taint)
        }</span>
        <span class="cov0" title="0">if len(nodeTaints) &gt; 0 </span><span class="cov0" title="0">{
                node.Spec.Taints = nodeTaints
        }</span>
        // Initially, set NodeNetworkUnavailable to true.
        <span class="cov0" title="0">if kl.providerRequiresNetworkingConfiguration() </span><span class="cov0" title="0">{
                node.Status.Conditions = append(node.Status.Conditions, v1.NodeCondition{
                        Type:               v1.NodeNetworkUnavailable,
                        Status:             v1.ConditionTrue,
                        Reason:             "NoRouteCreated",
                        Message:            "Node created without a route",
                        LastTransitionTime: metav1.NewTime(kl.clock.Now()),
                })
        }</span>

        <span class="cov0" title="0">if kl.enableControllerAttachDetach </span><span class="cov0" title="0">{
                if node.Annotations == nil </span><span class="cov0" title="0">{
                        node.Annotations = make(map[string]string)
                }</span>

                <span class="cov0" title="0">klog.V(2).InfoS("Setting node annotation to enable volume controller attach/detach")
                node.Annotations[volutil.ControllerManagedAttachAnnotation] = "true"</span>
        } else<span class="cov0" title="0"> {
                klog.V(2).InfoS("Controller attach/detach is disabled for this node; Kubelet will attach and detach volumes")
        }</span>

        <span class="cov0" title="0">if kl.keepTerminatedPodVolumes </span><span class="cov0" title="0">{
                if node.Annotations == nil </span><span class="cov0" title="0">{
                        node.Annotations = make(map[string]string)
                }</span>
                <span class="cov0" title="0">klog.V(2).InfoS("Setting node annotation to keep pod volumes of terminated pods attached to the node")
                node.Annotations[volutil.KeepTerminatedPodVolumesAnnotation] = "true"</span>
        }

        // @question: should this be place after the call to the cloud provider? which also applies labels
        <span class="cov0" title="0">for k, v := range kl.nodeLabels </span><span class="cov0" title="0">{
                if cv, found := node.ObjectMeta.Labels[k]; found </span><span class="cov0" title="0">{
                        klog.InfoS("the node label will overwrite default setting", "labelKey", k, "labelValue", v, "default", cv)
                }</span>
                <span class="cov0" title="0">node.ObjectMeta.Labels[k] = v</span>
        }

        <span class="cov0" title="0">if kl.providerID != "" </span><span class="cov0" title="0">{
                node.Spec.ProviderID = kl.providerID
        }</span>

        <span class="cov0" title="0">if kl.cloud != nil </span><span class="cov0" title="0">{
                instances, ok := kl.cloud.Instances()
                if !ok </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("failed to get instances from cloud provider")
                }</span>

                // TODO: We can't assume that the node has credentials to talk to the
                // cloudprovider from arbitrary nodes. At most, we should talk to a
                // local metadata server here.
                <span class="cov0" title="0">var err error
                if node.Spec.ProviderID == "" </span><span class="cov0" title="0">{
                        node.Spec.ProviderID, err = cloudprovider.GetInstanceProviderID(ctx, kl.cloud, kl.nodeName)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil, err
                        }</span>
                }

                <span class="cov0" title="0">instanceType, err := instances.InstanceType(ctx, kl.nodeName)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
                <span class="cov0" title="0">if instanceType != "" </span><span class="cov0" title="0">{
                        klog.InfoS("Adding label from cloud provider", "labelKey", v1.LabelInstanceType, "labelValue", instanceType)
                        node.ObjectMeta.Labels[v1.LabelInstanceType] = instanceType
                        klog.InfoS("Adding node label from cloud provider", "labelKey", v1.LabelInstanceTypeStable, "labelValue", instanceType)
                        node.ObjectMeta.Labels[v1.LabelInstanceTypeStable] = instanceType
                }</span>
                // If the cloud has zone information, label the node with the zone information
                <span class="cov0" title="0">zones, ok := kl.cloud.Zones()
                if ok </span><span class="cov0" title="0">{
                        zone, err := zones.GetZone(ctx)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil, fmt.Errorf("failed to get zone from cloud provider: %v", err)
                        }</span>
                        <span class="cov0" title="0">if zone.FailureDomain != "" </span><span class="cov0" title="0">{
                                klog.InfoS("Adding node label from cloud provider", "labelKey", v1.LabelFailureDomainBetaZone, "labelValue", zone.FailureDomain)
                                node.ObjectMeta.Labels[v1.LabelFailureDomainBetaZone] = zone.FailureDomain
                                klog.InfoS("Adding node label from cloud provider", "labelKey", v1.LabelTopologyZone, "labelValue", zone.FailureDomain)
                                node.ObjectMeta.Labels[v1.LabelTopologyZone] = zone.FailureDomain
                        }</span>
                        <span class="cov0" title="0">if zone.Region != "" </span><span class="cov0" title="0">{
                                klog.InfoS("Adding node label from cloud provider", "labelKey", v1.LabelFailureDomainBetaRegion, "labelValue", zone.Region)
                                node.ObjectMeta.Labels[v1.LabelFailureDomainBetaRegion] = zone.Region
                                klog.InfoS("Adding node label from cloud provider", "labelKey", v1.LabelTopologyRegion, "labelValue", zone.Region)
                                node.ObjectMeta.Labels[v1.LabelTopologyRegion] = zone.Region
                        }</span>
                }
        }

        <span class="cov0" title="0">kl.setNodeStatus(ctx, node)

        return node, nil</span>
}

// fastNodeStatusUpdate is a "lightweight" version of syncNodeStatus which doesn't hit the
// apiserver except for the final run, to be called by fastStatusUpdateOnce in each loop.
// It holds the same lock as syncNodeStatus and is thread-safe when called concurrently with
// syncNodeStatus. Its return value indicates whether the loop running it should exit
// (final run), and it also sets kl.containerRuntimeReadyExpected.
func (kl *Kubelet) fastNodeStatusUpdate(ctx context.Context, timeout bool) (completed bool) <span class="cov0" title="0">{
        kl.syncNodeStatusMux.Lock()
        defer func() </span><span class="cov0" title="0">{
                kl.syncNodeStatusMux.Unlock()

                if completed </span><span class="cov0" title="0">{
                        // containerRuntimeReadyExpected is read by updateRuntimeUp().
                        // Not going for a more granular mutex as this path runs only once.
                        kl.updateRuntimeMux.Lock()
                        defer kl.updateRuntimeMux.Unlock()
                        kl.containerRuntimeReadyExpected = true
                }</span>
        }()

        <span class="cov0" title="0">if timeout </span><span class="cov0" title="0">{
                klog.ErrorS(nil, "Node not becoming ready in time after startup")
                return true
        }</span>

        <span class="cov0" title="0">originalNode, err := kl.GetNode()
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Error getting the current node from lister")
                return false
        }</span>

        <span class="cov0" title="0">readyIdx, originalNodeReady := nodeutil.GetNodeCondition(&amp;originalNode.Status, v1.NodeReady)
        if readyIdx == -1 </span><span class="cov0" title="0">{
                klog.ErrorS(nil, "Node does not have NodeReady condition", "originalNode", originalNode)
                return false
        }</span>

        <span class="cov0" title="0">if originalNodeReady.Status == v1.ConditionTrue </span><span class="cov0" title="0">{
                return true
        }</span>

        // This is in addition to the regular syncNodeStatus logic so we can get the container runtime status earlier.
        // This function itself has a mutex and it doesn't recursively call fastNodeStatusUpdate or syncNodeStatus.
        <span class="cov0" title="0">kl.updateRuntimeUp()

        node, changed := kl.updateNode(ctx, originalNode)

        if !changed </span><span class="cov0" title="0">{
                // We don't do markVolumesFromNode(node) here and leave it to the regular syncNodeStatus().
                return false
        }</span>

        <span class="cov0" title="0">readyIdx, nodeReady := nodeutil.GetNodeCondition(&amp;node.Status, v1.NodeReady)
        if readyIdx == -1 </span><span class="cov0" title="0">{
                klog.ErrorS(nil, "Node does not have NodeReady condition", "node", node)
                return false
        }</span>

        <span class="cov0" title="0">if nodeReady.Status == v1.ConditionFalse </span><span class="cov0" title="0">{
                return false
        }</span>

        <span class="cov0" title="0">klog.InfoS("Fast updating node status as it just became ready")
        if _, err := kl.patchNodeStatus(originalNode, node); err != nil </span><span class="cov0" title="0">{
                // The originalNode is probably stale, but we know that the current state of kubelet would turn
                // the node to be ready. Retry using syncNodeStatus() which fetches from the apiserver.
                klog.ErrorS(err, "Error updating node status, will retry with syncNodeStatus")

                // The reversed kl.syncNodeStatusMux.Unlock/Lock() below to allow kl.syncNodeStatus() execution.
                kl.syncNodeStatusMux.Unlock()
                kl.syncNodeStatus()
                // This lock action is unnecessary if we add a flag to check in the defer before unlocking it,
                // but having it here makes the logic a bit easier to read.
                kl.syncNodeStatusMux.Lock()
        }</span>

        // We don't do markVolumesFromNode(node) here and leave it to the regular syncNodeStatus().
        <span class="cov0" title="0">return true</span>
}

// syncNodeStatus should be called periodically from a goroutine.
// It synchronizes node status to master if there is any change or enough time
// passed from the last sync, registering the kubelet first if necessary.
func (kl *Kubelet) syncNodeStatus() <span class="cov8" title="1">{
        kl.syncNodeStatusMux.Lock()
        defer kl.syncNodeStatusMux.Unlock()
        ctx := context.Background()

        if kl.kubeClient == nil || kl.heartbeatClient == nil </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov8" title="1">if kl.registerNode </span><span class="cov0" title="0">{
                // This will exit immediately if it doesn't need to do anything.
                kl.registerWithAPIServer()
        }</span>
        <span class="cov8" title="1">if err := kl.updateNodeStatus(ctx); err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Unable to update node status")
        }</span>
}

func (kl *Kubelet) SyncNodeStatus() <span class="cov0" title="0">{
        kl.syncNodeStatusMux.Lock()
        defer kl.syncNodeStatusMux.Unlock()
        ctx := context.Background()

        if kl.kubeClient == nil || kl.heartbeatClient == nil </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov0" title="0">if kl.registerNode </span><span class="cov0" title="0">{
                // This will exit immediately if it doesn't need to do anything.
                kl.registerWithAPIServer()
        }</span>
        <span class="cov0" title="0">if err := kl.updateNodeStatus(ctx); err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Unable to update node status")
        }</span>
}

// updateNodeStatus updates node status to master with retries if there is any
// change or enough time passed from the last sync.
func (kl *Kubelet) updateNodeStatus(ctx context.Context) error <span class="cov8" title="1">{
        klog.V(5).InfoS("Updating node status")
        for i := 0; i &lt; nodeStatusUpdateRetry; i++ </span><span class="cov8" title="1">{
                if err := kl.tryUpdateNodeStatus(ctx, i); err != nil </span><span class="cov0" title="0">{
                        if i &gt; 0 &amp;&amp; kl.onRepeatedHeartbeatFailure != nil </span><span class="cov0" title="0">{
                                kl.onRepeatedHeartbeatFailure()
                        }</span>
                        <span class="cov0" title="0">klog.ErrorS(err, "Error updating node status, will retry")</span>
                } else<span class="cov8" title="1"> {
                        return nil
                }</span>
        }
        <span class="cov0" title="0">return fmt.Errorf("update node status exceeds retry count")</span>
}

// tryUpdateNodeStatus tries to update node status to master if there is any
// change or enough time passed from the last sync.
func (kl *Kubelet) tryUpdateNodeStatus(ctx context.Context, tryNumber int) error <span class="cov8" title="1">{
        // In large clusters, GET and PUT operations on Node objects coming
        // from here are the majority of load on apiserver and etcd.
        // To reduce the load on etcd, we are serving GET operations from
        // apiserver cache (the data might be slightly delayed but it doesn't
        // seem to cause more conflict - the delays are pretty small).
        // If it result in a conflict, all retries are served directly from etcd.
        opts := metav1.GetOptions{}
        if tryNumber == 0 </span><span class="cov8" title="1">{
                util.FromApiserverCache(&amp;opts)
        }</span>
        <span class="cov8" title="1">originalNode, err := kl.heartbeatClient.CoreV1().Nodes().Get(ctx, string(kl.nodeName), opts)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error getting node %q: %v", kl.nodeName, err)
        }</span>
        <span class="cov8" title="1">if originalNode == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("nil %q node object", kl.nodeName)
        }</span>

        <span class="cov8" title="1">node, changed := kl.updateNode(ctx, originalNode)
        shouldPatchNodeStatus := changed || kl.clock.Since(kl.lastStatusReportTime) &gt;= kl.nodeStatusReportFrequency

        if !shouldPatchNodeStatus </span><span class="cov0" title="0">{
                kl.markVolumesFromNode(node)
                return nil
        }</span>

        <span class="cov8" title="1">updatedNode, err := kl.patchNodeStatus(originalNode, node)
        if err == nil </span><span class="cov8" title="1">{
                kl.markVolumesFromNode(updatedNode)
        }</span>
        <span class="cov8" title="1">return err</span>
}

// updateNode creates a copy of originalNode and runs update logic on it.
// It returns the updated node object and a bool indicating if anything has been changed.
func (kl *Kubelet) updateNode(ctx context.Context, originalNode *v1.Node) (*v1.Node, bool) <span class="cov8" title="1">{
        node := originalNode.DeepCopy()

        podCIDRChanged := false
        if len(node.Spec.PodCIDRs) != 0 </span><span class="cov0" title="0">{
                // Pod CIDR could have been updated before, so we cannot rely on
                // node.Spec.PodCIDR being non-empty. We also need to know if pod CIDR is
                // actually changed.
                var err error
                podCIDRs := strings.Join(node.Spec.PodCIDRs, ",")
                if podCIDRChanged, err = kl.updatePodCIDR(ctx, podCIDRs); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Error updating pod CIDR")
                }</span>
        }

        <span class="cov8" title="1">areRequiredLabelsNotPresent := false
        osName, osLabelExists := node.Labels[v1.LabelOSStable]
        if !osLabelExists || osName != goruntime.GOOS </span><span class="cov8" title="1">{
                if len(node.Labels) == 0 </span><span class="cov8" title="1">{
                        node.Labels = make(map[string]string)
                }</span>
                <span class="cov8" title="1">node.Labels[v1.LabelOSStable] = goruntime.GOOS
                areRequiredLabelsNotPresent = true</span>
        }
        // Set the arch if there is a mismatch
        <span class="cov8" title="1">arch, archLabelExists := node.Labels[v1.LabelArchStable]
        if !archLabelExists || arch != goruntime.GOARCH </span><span class="cov8" title="1">{
                if len(node.Labels) == 0 </span><span class="cov0" title="0">{
                        node.Labels = make(map[string]string)
                }</span>
                <span class="cov8" title="1">node.Labels[v1.LabelArchStable] = goruntime.GOARCH
                areRequiredLabelsNotPresent = true</span>
        }

        <span class="cov8" title="1">kl.setNodeStatus(ctx, node)

        changed := podCIDRChanged || nodeStatusHasChanged(&amp;originalNode.Status, &amp;node.Status) || areRequiredLabelsNotPresent
        return node, changed</span>
}

// patchNodeStatus patches node on the API server based on originalNode.
// It returns any potential error, or an updatedNode and refreshes the state of kubelet when successful.
func (kl *Kubelet) patchNodeStatus(originalNode, node *v1.Node) (*v1.Node, error) <span class="cov8" title="1">{
        // Patch the current status on the API server
        updatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">kl.lastStatusReportTime = kl.clock.Now()
        kl.setLastObservedNodeAddresses(updatedNode.Status.Addresses)

        readyIdx, readyCondition := nodeutil.GetNodeCondition(&amp;updatedNode.Status, v1.NodeReady)
        if readyIdx &gt;= 0 &amp;&amp; readyCondition.Status == v1.ConditionTrue </span><span class="cov0" title="0">{
                kl.nodeStartupLatencyTracker.RecordNodeReady()
        }</span>

        <span class="cov8" title="1">return updatedNode, nil</span>
}

// markVolumesFromNode updates volumeManager with VolumesInUse status from node.
//
// In the case of node status update being unnecessary, call with the fetched node.
// We must mark the volumes as ReportedInUse in volume manager's dsw even
// if no changes were made to the node status (no volumes were added or removed
// from the VolumesInUse list).
//
// The reason is that on a kubelet restart, the volume manager's dsw is
// repopulated and the volume ReportedInUse is initialized to false, while the
// VolumesInUse list from the Node object still contains the state from the
// previous kubelet instantiation.
//
// Once the volumes are added to the dsw, the ReportedInUse field needs to be
// synced from the VolumesInUse list in the Node.Status.
//
// The MarkVolumesAsReportedInUse() call cannot be performed in dsw directly
// because it does not have access to the Node object.
// This also cannot be populated on node status manager init because the volume
// may not have been added to dsw at that time.
//
// Or, after a successful node status update, call with updatedNode returned from
// the patch call, to mark the volumeInUse as reportedInUse to indicate
// those volumes are already updated in the node's status
func (kl *Kubelet) markVolumesFromNode(node *v1.Node) <span class="cov8" title="1">{
        kl.volumeManager.MarkVolumesAsReportedInUse(node.Status.VolumesInUse)
}</span>

// recordNodeStatusEvent records an event of the given type with the given
// message for the node.
func (kl *Kubelet) recordNodeStatusEvent(eventType, event string) <span class="cov8" title="1">{
        klog.V(2).InfoS("Recording event message for node", "node", klog.KRef("", string(kl.nodeName)), "event", event)
        kl.recorder.Eventf(kl.nodeRef, eventType, event, "Node %s status is now: %s", kl.nodeName, event)
}</span>

// recordEvent records an event for this node, the Kubelet's nodeRef is passed to the recorder
func (kl *Kubelet) recordEvent(eventType, event, message string) <span class="cov0" title="0">{
        kl.recorder.Eventf(kl.nodeRef, eventType, event, message)
}</span>

// record if node schedulable change.
func (kl *Kubelet) recordNodeSchedulableEvent(ctx context.Context, node *v1.Node) error <span class="cov8" title="1">{
        kl.lastNodeUnschedulableLock.Lock()
        defer kl.lastNodeUnschedulableLock.Unlock()
        if kl.lastNodeUnschedulable != node.Spec.Unschedulable </span><span class="cov0" title="0">{
                if node.Spec.Unschedulable </span><span class="cov0" title="0">{
                        kl.recordNodeStatusEvent(v1.EventTypeNormal, events.NodeNotSchedulable)
                }</span> else<span class="cov0" title="0"> {
                        kl.recordNodeStatusEvent(v1.EventTypeNormal, events.NodeSchedulable)
                }</span>
                <span class="cov0" title="0">kl.lastNodeUnschedulable = node.Spec.Unschedulable</span>
        }
        <span class="cov8" title="1">return nil</span>
}

// setNodeStatus fills in the Status fields of the given Node, overwriting
// any fields that are currently set.
// TODO(madhusudancs): Simplify the logic for setting node conditions and
// refactor the node status condition code out to a different file.
func (kl *Kubelet) setNodeStatus(ctx context.Context, node *v1.Node) <span class="cov8" title="1">{
        for i, f := range kl.setNodeStatusFuncs </span><span class="cov8" title="1">{
                klog.V(5).InfoS("Setting node status condition code", "position", i, "node", klog.KObj(node))
                if err := f(ctx, node); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Failed to set some node status fields", "node", klog.KObj(node))
                }</span>
        }
}

func (kl *Kubelet) setLastObservedNodeAddresses(addresses []v1.NodeAddress) <span class="cov8" title="1">{
        kl.lastObservedNodeAddressesMux.Lock()
        defer kl.lastObservedNodeAddressesMux.Unlock()
        kl.lastObservedNodeAddresses = addresses
}</span>
func (kl *Kubelet) getLastObservedNodeAddresses() []v1.NodeAddress <span class="cov0" title="0">{
        kl.lastObservedNodeAddressesMux.RLock()
        defer kl.lastObservedNodeAddressesMux.RUnlock()
        return kl.lastObservedNodeAddresses
}</span>

// defaultNodeStatusFuncs is a factory that generates the default set of
// setNodeStatus funcs
func (kl *Kubelet) defaultNodeStatusFuncs() []func(context.Context, *v1.Node) error <span class="cov8" title="1">{
        // if cloud is not nil, we expect the cloud resource sync manager to exist
        var nodeAddressesFunc func() ([]v1.NodeAddress, error)
        if kl.cloud != nil </span><span class="cov0" title="0">{
                nodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses
        }</span>
        <span class="cov8" title="1">var validateHostFunc func() error
        if kl.appArmorValidator != nil </span><span class="cov8" title="1">{
                validateHostFunc = kl.appArmorValidator.ValidateHost
        }</span>
        <span class="cov8" title="1">var setters []func(ctx context.Context, n *v1.Node) error
        setters = append(setters,
                nodestatus.NodeAddress(kl.nodeIPs, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),
                nodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,
                        kl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent, kl.supportLocalStorageCapacityIsolation()),
                nodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),
                nodestatus.DaemonEndpoints(kl.daemonEndpoints),
                nodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),
                nodestatus.GoRuntime(),
        )
        // Volume limits
        setters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))

        setters = append(setters,
                nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),
                nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),
                nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),
                nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors,
                        validateHostFunc, kl.containerManager.Status, kl.shutdownManager.ShutdownStatus, kl.recordNodeStatusEvent, kl.supportLocalStorageCapacityIsolation()),
                nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),
                // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event
                // and record state back to the Kubelet runtime object. In the future, I'd like to isolate
                // these side-effects by decoupling the decisions to send events and partial status recording
                // from the Node setters.
                kl.recordNodeSchedulableEvent,
        )
        return setters</span>
}

// DefaultNodeStatusFuncs returns the default set of setNodeStatus funcs
func (kl *Kubelet) DefaultNodeStatusFuncs() []func(context.Context, *v1.Node) error <span class="cov0" title="0">{
        // if cloud is not nil, we expect the cloud resource sync manager to exist
        var nodeAddressesFunc func() ([]v1.NodeAddress, error)
        if kl.cloud != nil </span><span class="cov0" title="0">{
                nodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses
        }</span>
        <span class="cov0" title="0">var validateHostFunc func() error
        if kl.appArmorValidator != nil </span><span class="cov0" title="0">{
                validateHostFunc = kl.appArmorValidator.ValidateHost
        }</span>
        <span class="cov0" title="0">var setters []func(ctx context.Context, n *v1.Node) error
        setters = append(setters,
                nodestatus.NodeAddress(kl.nodeIPs, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc),
                nodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity,
                        kl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent, kl.supportLocalStorageCapacityIsolation()),
                nodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version),
                nodestatus.DaemonEndpoints(kl.daemonEndpoints),
                nodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList),
                nodestatus.GoRuntime(),
        )
        // Volume limits
        setters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits))

        setters = append(setters,
                nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent),
                nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent),
                nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent),
                nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors,
                        validateHostFunc, kl.containerManager.Status, kl.shutdownManager.ShutdownStatus, kl.recordNodeStatusEvent, kl.supportLocalStorageCapacityIsolation()),
                nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse),
                // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event
                // and record state back to the Kubelet runtime object. In the future, I'd like to isolate
                // these side-effects by decoupling the decisions to send events and partial status recording
                // from the Node setters.
                kl.recordNodeSchedulableEvent,
        )
        return setters</span>
}

// Validate given node IP belongs to the current host
func validateNodeIP(nodeIP net.IP) error <span class="cov0" title="0">{
        // Honor IP limitations set in setNodeStatus()
        if nodeIP.To4() == nil &amp;&amp; nodeIP.To16() == nil </span><span class="cov0" title="0">{
                return fmt.Errorf("nodeIP must be a valid IP address")
        }</span>
        <span class="cov0" title="0">if nodeIP.IsLoopback() </span><span class="cov0" title="0">{
                return fmt.Errorf("nodeIP can't be loopback address")
        }</span>
        <span class="cov0" title="0">if nodeIP.IsMulticast() </span><span class="cov0" title="0">{
                return fmt.Errorf("nodeIP can't be a multicast address")
        }</span>
        <span class="cov0" title="0">if nodeIP.IsLinkLocalUnicast() </span><span class="cov0" title="0">{
                return fmt.Errorf("nodeIP can't be a link-local unicast address")
        }</span>
        <span class="cov0" title="0">if nodeIP.IsUnspecified() </span><span class="cov0" title="0">{
                return fmt.Errorf("nodeIP can't be an all zeros address")
        }</span>

        <span class="cov0" title="0">addrs, err := net.InterfaceAddrs()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">for _, addr := range addrs </span><span class="cov0" title="0">{
                var ip net.IP
                switch v := addr.(type) </span>{
                case *net.IPNet:<span class="cov0" title="0">
                        ip = v.IP</span>
                case *net.IPAddr:<span class="cov0" title="0">
                        ip = v.IP</span>
                }
                <span class="cov0" title="0">if ip != nil &amp;&amp; ip.Equal(nodeIP) </span><span class="cov0" title="0">{
                        return nil
                }</span>
        }
        <span class="cov0" title="0">return fmt.Errorf("node IP: %q not found in the host's network interfaces", nodeIP.String())</span>
}

// nodeStatusHasChanged compares the original node and current node's status and
// returns true if any change happens. The heartbeat timestamp is ignored.
func nodeStatusHasChanged(originalStatus *v1.NodeStatus, status *v1.NodeStatus) bool <span class="cov8" title="1">{
        if originalStatus == nil &amp;&amp; status == nil </span><span class="cov0" title="0">{
                return false
        }</span>
        <span class="cov8" title="1">if originalStatus == nil || status == nil </span><span class="cov0" title="0">{
                return true
        }</span>

        // Compare node conditions here because we need to ignore the heartbeat timestamp.
        <span class="cov8" title="1">if nodeConditionsHaveChanged(originalStatus.Conditions, status.Conditions) </span><span class="cov8" title="1">{
                return true
        }</span>

        // Compare other fields of NodeStatus.
        <span class="cov0" title="0">originalStatusCopy := originalStatus.DeepCopy()
        statusCopy := status.DeepCopy()
        originalStatusCopy.Conditions = nil
        statusCopy.Conditions = nil
        return !apiequality.Semantic.DeepEqual(originalStatusCopy, statusCopy)</span>
}

// nodeConditionsHaveChanged compares the original node and current node's
// conditions and returns true if any change happens. The heartbeat timestamp is
// ignored.
func nodeConditionsHaveChanged(originalConditions []v1.NodeCondition, conditions []v1.NodeCondition) bool <span class="cov8" title="1">{
        if len(originalConditions) != len(conditions) </span><span class="cov8" title="1">{
                return true
        }</span>

        <span class="cov0" title="0">originalConditionsCopy := make([]v1.NodeCondition, 0, len(originalConditions))
        originalConditionsCopy = append(originalConditionsCopy, originalConditions...)
        conditionsCopy := make([]v1.NodeCondition, 0, len(conditions))
        conditionsCopy = append(conditionsCopy, conditions...)

        sort.SliceStable(originalConditionsCopy, func(i, j int) bool </span><span class="cov0" title="0">{ return originalConditionsCopy[i].Type &lt; originalConditionsCopy[j].Type }</span>)
        <span class="cov0" title="0">sort.SliceStable(conditionsCopy, func(i, j int) bool </span><span class="cov0" title="0">{ return conditionsCopy[i].Type &lt; conditionsCopy[j].Type }</span>)

        <span class="cov0" title="0">replacedheartbeatTime := metav1.Time{}
        for i := range conditionsCopy </span><span class="cov0" title="0">{
                originalConditionsCopy[i].LastHeartbeatTime = replacedheartbeatTime
                conditionsCopy[i].LastHeartbeatTime = replacedheartbeatTime
                if !apiequality.Semantic.DeepEqual(&amp;originalConditionsCopy[i], &amp;conditionsCopy[i]) </span><span class="cov0" title="0">{
                        return true
                }</span>
        }
        <span class="cov0" title="0">return false</span>
}
</pre>
		
		<pre class="file" id="file6" style="display: none">//go:build !windows
// +build !windows

/*
Copyright 2019 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

func getOSSpecificLabels() (map[string]string, error) <span class="cov0" title="0">{
        return nil, nil
}</span>
</pre>
		
		<pre class="file" id="file7" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "bytes"
        "context"
        "fmt"
        "io"
        "net/http"
        "net/url"
        "os"
        "path/filepath"
        "runtime"
        "sort"
        "strings"

        "github.com/google/go-cmp/cmp"
        v1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/api/errors"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/labels"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/util/sets"
        utilvalidation "k8s.io/apimachinery/pkg/util/validation"
        utilfeature "k8s.io/apiserver/pkg/util/feature"
        runtimeapi "k8s.io/cri-api/pkg/apis/runtime/v1"
        "k8s.io/klog/v2"
        "k8s.io/kubelet/pkg/cri/streaming/portforward"
        remotecommandserver "k8s.io/kubelet/pkg/cri/streaming/remotecommand"
        podutil "k8s.io/kubernetes/pkg/api/v1/pod"
        "k8s.io/kubernetes/pkg/api/v1/resource"
        podshelper "k8s.io/kubernetes/pkg/apis/core/pods"
        v1helper "k8s.io/kubernetes/pkg/apis/core/v1/helper"
        v1qos "k8s.io/kubernetes/pkg/apis/core/v1/helper/qos"
        "k8s.io/kubernetes/pkg/features"
        "k8s.io/kubernetes/pkg/fieldpath"
        "k8s.io/kubernetes/pkg/kubelet/cm"
        kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
        "k8s.io/kubernetes/pkg/kubelet/envvars"
        "k8s.io/kubernetes/pkg/kubelet/images"
        "k8s.io/kubernetes/pkg/kubelet/metrics"
        "k8s.io/kubernetes/pkg/kubelet/status"
        kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
        "k8s.io/kubernetes/pkg/kubelet/util"
        utilpod "k8s.io/kubernetes/pkg/util/pod"
        volumeutil "k8s.io/kubernetes/pkg/volume/util"
        "k8s.io/kubernetes/pkg/volume/util/hostutil"
        "k8s.io/kubernetes/pkg/volume/util/subpath"
        "k8s.io/kubernetes/pkg/volume/util/volumepathhandler"
        volumevalidation "k8s.io/kubernetes/pkg/volume/validation"
        "k8s.io/kubernetes/third_party/forked/golang/expansion"
        utilnet "k8s.io/utils/net"
)

const (
        managedHostsHeader                = "# Kubernetes-managed hosts file.\n"
        managedHostsHeaderWithHostNetwork = "# Kubernetes-managed hosts file (host network).\n"
)

// Container state reason list
const (
        PodInitializing   = "PodInitializing"
        ContainerCreating = "ContainerCreating"
)

// Get a list of pods that have data directories.
func (kl *Kubelet) listPodsFromDisk() ([]types.UID, error) <span class="cov8" title="1">{
        podInfos, err := os.ReadDir(kl.getPodsDir())
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">pods := []types.UID{}
        for i := range podInfos </span><span class="cov8" title="1">{
                if podInfos[i].IsDir() </span><span class="cov8" title="1">{
                        pods = append(pods, types.UID(podInfos[i].Name()))
                }</span>
        }
        <span class="cov8" title="1">return pods, nil</span>
}

// GetActivePods returns pods that have been admitted to the kubelet that
// are not fully terminated. This is mapped to the "desired state" of the
// kubelet - what pods should be running.
//
// WARNING: Currently this list does not include pods that have been force
// deleted but may still be terminating, which means resources assigned to
// those pods during admission may still be in use. See
// https://github.com/kubernetes/kubernetes/issues/104824
func (kl *Kubelet) GetActivePods() []*v1.Pod <span class="cov8" title="1">{
        allPods := kl.podManager.GetPods()
        activePods := kl.filterOutInactivePods(allPods)
        return activePods
}</span>

// makeBlockVolumes maps the raw block devices specified in the path of the container
// Experimental
func (kl *Kubelet) makeBlockVolumes(pod *v1.Pod, container *v1.Container, podVolumes kubecontainer.VolumeMap, blkutil volumepathhandler.BlockVolumePathHandler) ([]kubecontainer.DeviceInfo, error) <span class="cov8" title="1">{
        var devices []kubecontainer.DeviceInfo
        for _, device := range container.VolumeDevices </span><span class="cov0" title="0">{
                // check path is absolute
                if !filepath.IsAbs(device.DevicePath) </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("error DevicePath `%s` must be an absolute path", device.DevicePath)
                }</span>
                <span class="cov0" title="0">vol, ok := podVolumes[device.Name]
                if !ok || vol.BlockVolumeMapper == nil </span><span class="cov0" title="0">{
                        klog.ErrorS(nil, "Block volume cannot be satisfied for container, because the volume is missing or the volume mapper is nil", "containerName", container.Name, "device", device)
                        return nil, fmt.Errorf("cannot find volume %q to pass into container %q", device.Name, container.Name)
                }</span>
                // Get a symbolic link associated to a block device under pod device path
                <span class="cov0" title="0">dirPath, volName := vol.BlockVolumeMapper.GetPodDeviceMapPath()
                symlinkPath := filepath.Join(dirPath, volName)
                if islinkExist, checkErr := blkutil.IsSymlinkExist(symlinkPath); checkErr != nil </span><span class="cov0" title="0">{
                        return nil, checkErr
                }</span> else<span class="cov0" title="0"> if islinkExist </span><span class="cov0" title="0">{
                        // Check readOnly in PVCVolumeSource and set read only permission if it's true.
                        permission := "mrw"
                        if vol.ReadOnly </span><span class="cov0" title="0">{
                                permission = "r"
                        }</span>
                        <span class="cov0" title="0">klog.V(4).InfoS("Device will be attached to container in the corresponding path on host", "containerName", container.Name, "path", symlinkPath)
                        devices = append(devices, kubecontainer.DeviceInfo{PathOnHost: symlinkPath, PathInContainer: device.DevicePath, Permissions: permission})</span>
                }
        }

        <span class="cov8" title="1">return devices, nil</span>
}

// shouldMountHostsFile checks if the nodes /etc/hosts should be mounted
// Kubernetes only mounts on /etc/hosts if:
// - container is not an infrastructure (pause) container
// - container is not already mounting on /etc/hosts
// Kubernetes will not mount /etc/hosts if:
// - when the Pod sandbox is being created, its IP is still unknown. Hence, PodIP will not have been set.
// - Windows pod contains a hostProcess container
func shouldMountHostsFile(pod *v1.Pod, podIPs []string) bool <span class="cov8" title="1">{
        shouldMount := len(podIPs) &gt; 0
        if runtime.GOOS == "windows" </span><span class="cov0" title="0">{
                return shouldMount &amp;&amp; !kubecontainer.HasWindowsHostProcessContainer(pod)
        }</span>
        <span class="cov8" title="1">return shouldMount</span>
}

// makeMounts determines the mount points for the given container.
func makeMounts(pod *v1.Pod, podDir string, container *v1.Container, hostName, hostDomain string, podIPs []string, podVolumes kubecontainer.VolumeMap, hu hostutil.HostUtils, subpather subpath.Interface, expandEnvs []kubecontainer.EnvVar) ([]kubecontainer.Mount, func(), error) <span class="cov8" title="1">{
        mountEtcHostsFile := shouldMountHostsFile(pod, podIPs)
        klog.V(3).InfoS("Creating hosts mount for container", "pod", klog.KObj(pod), "containerName", container.Name, "podIPs", podIPs, "path", mountEtcHostsFile)
        mounts := []kubecontainer.Mount{}
        var cleanupAction func()
        for i, mount := range container.VolumeMounts </span><span class="cov0" title="0">{
                // do not mount /etc/hosts if container is already mounting on the path
                mountEtcHostsFile = mountEtcHostsFile &amp;&amp; (mount.MountPath != etcHostsPath)
                vol, ok := podVolumes[mount.Name]
                if !ok || vol.Mounter == nil </span><span class="cov0" title="0">{
                        klog.ErrorS(nil, "Mount cannot be satisfied for the container, because the volume is missing or the volume mounter (vol.Mounter) is nil",
                                "containerName", container.Name, "ok", ok, "volumeMounter", mount)
                        return nil, cleanupAction, fmt.Errorf("cannot find volume %q to mount into container %q", mount.Name, container.Name)
                }</span>

                <span class="cov0" title="0">relabelVolume := false
                // If the volume supports SELinux and it has not been
                // relabeled already and it is not a read-only volume,
                // relabel it and mark it as labeled
                if vol.Mounter.GetAttributes().Managed &amp;&amp; vol.Mounter.GetAttributes().SELinuxRelabel &amp;&amp; !vol.SELinuxLabeled </span><span class="cov0" title="0">{
                        vol.SELinuxLabeled = true
                        relabelVolume = true
                }</span>
                <span class="cov0" title="0">hostPath, err := volumeutil.GetPath(vol.Mounter)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, cleanupAction, err
                }</span>

                <span class="cov0" title="0">subPath := mount.SubPath
                if mount.SubPathExpr != "" </span><span class="cov0" title="0">{
                        subPath, err = kubecontainer.ExpandContainerVolumeMounts(mount, expandEnvs)

                        if err != nil </span><span class="cov0" title="0">{
                                return nil, cleanupAction, err
                        }</span>
                }

                <span class="cov0" title="0">if subPath != "" </span><span class="cov0" title="0">{
                        if filepath.IsAbs(subPath) </span><span class="cov0" title="0">{
                                return nil, cleanupAction, fmt.Errorf("error SubPath `%s` must not be an absolute path", subPath)
                        }</span>

                        <span class="cov0" title="0">err = volumevalidation.ValidatePathNoBacksteps(subPath)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil, cleanupAction, fmt.Errorf("unable to provision SubPath `%s`: %v", subPath, err)
                        }</span>

                        <span class="cov0" title="0">volumePath := hostPath
                        hostPath = filepath.Join(volumePath, subPath)

                        if subPathExists, err := hu.PathExists(hostPath); err != nil </span><span class="cov0" title="0">{
                                klog.ErrorS(nil, "Could not determine if subPath exists, will not attempt to change its permissions", "path", hostPath)
                        }</span> else<span class="cov0" title="0"> if !subPathExists </span><span class="cov0" title="0">{
                                // Create the sub path now because if it's auto-created later when referenced, it may have an
                                // incorrect ownership and mode. For example, the sub path directory must have at least g+rwx
                                // when the pod specifies an fsGroup, and if the directory is not created here, Docker will
                                // later auto-create it with the incorrect mode 0750
                                // Make extra care not to escape the volume!
                                perm, err := hu.GetMode(volumePath)
                                if err != nil </span><span class="cov0" title="0">{
                                        return nil, cleanupAction, err
                                }</span>
                                <span class="cov0" title="0">if err := subpather.SafeMakeDir(subPath, volumePath, perm); err != nil </span><span class="cov0" title="0">{
                                        // Don't pass detailed error back to the user because it could give information about host filesystem
                                        klog.ErrorS(err, "Failed to create subPath directory for volumeMount of the container", "containerName", container.Name, "volumeMountName", mount.Name)
                                        return nil, cleanupAction, fmt.Errorf("failed to create subPath directory for volumeMount %q of container %q", mount.Name, container.Name)
                                }</span>
                        }
                        <span class="cov0" title="0">hostPath, cleanupAction, err = subpather.PrepareSafeSubpath(subpath.Subpath{
                                VolumeMountIndex: i,
                                Path:             hostPath,
                                VolumeName:       vol.InnerVolumeSpecName,
                                VolumePath:       volumePath,
                                PodDir:           podDir,
                                ContainerName:    container.Name,
                        })
                        if err != nil </span><span class="cov0" title="0">{
                                // Don't pass detailed error back to the user because it could give information about host filesystem
                                klog.ErrorS(err, "Failed to prepare subPath for volumeMount of the container", "containerName", container.Name, "volumeMountName", mount.Name)
                                return nil, cleanupAction, fmt.Errorf("failed to prepare subPath for volumeMount %q of container %q", mount.Name, container.Name)
                        }</span>
                }

                // Docker Volume Mounts fail on Windows if it is not of the form C:/
                <span class="cov0" title="0">if volumeutil.IsWindowsLocalPath(runtime.GOOS, hostPath) </span><span class="cov0" title="0">{
                        hostPath = volumeutil.MakeAbsolutePath(runtime.GOOS, hostPath)
                }</span>

                <span class="cov0" title="0">containerPath := mount.MountPath
                // IsAbs returns false for UNC path/SMB shares/named pipes in Windows. So check for those specifically and skip MakeAbsolutePath
                if !volumeutil.IsWindowsUNCPath(runtime.GOOS, containerPath) &amp;&amp; !filepath.IsAbs(containerPath) </span><span class="cov0" title="0">{
                        containerPath = volumeutil.MakeAbsolutePath(runtime.GOOS, containerPath)
                }</span>

                <span class="cov0" title="0">propagation, err := translateMountPropagation(mount.MountPropagation)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, cleanupAction, err
                }</span>
                <span class="cov0" title="0">klog.V(5).InfoS("Mount has propagation", "pod", klog.KObj(pod), "containerName", container.Name, "volumeMountName", mount.Name, "propagation", propagation)
                mustMountRO := vol.Mounter.GetAttributes().ReadOnly

                mounts = append(mounts, kubecontainer.Mount{
                        Name:           mount.Name,
                        ContainerPath:  containerPath,
                        HostPath:       hostPath,
                        ReadOnly:       mount.ReadOnly || mustMountRO,
                        SELinuxRelabel: relabelVolume,
                        Propagation:    propagation,
                })</span>
        }
        <span class="cov8" title="1">if mountEtcHostsFile </span><span class="cov8" title="1">{
                hostAliases := pod.Spec.HostAliases
                hostsMount, err := makeHostsMount(podDir, podIPs, hostName, hostDomain, hostAliases, pod.Spec.HostNetwork)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, cleanupAction, err
                }</span>
                <span class="cov8" title="1">mounts = append(mounts, *hostsMount)</span>
        }
        <span class="cov8" title="1">return mounts, cleanupAction, nil</span>
}

// translateMountPropagation transforms v1.MountPropagationMode to
// runtimeapi.MountPropagation.
func translateMountPropagation(mountMode *v1.MountPropagationMode) (runtimeapi.MountPropagation, error) <span class="cov0" title="0">{
        if runtime.GOOS == "windows" </span><span class="cov0" title="0">{
                // Windows containers doesn't support mount propagation, use private for it.
                // Refer https://docs.docker.com/storage/bind-mounts/#configure-bind-propagation.
                return runtimeapi.MountPropagation_PROPAGATION_PRIVATE, nil
        }</span>

        <span class="cov0" title="0">switch </span>{
        case mountMode == nil:<span class="cov0" title="0">
                // PRIVATE is the default
                return runtimeapi.MountPropagation_PROPAGATION_PRIVATE, nil</span>
        case *mountMode == v1.MountPropagationHostToContainer:<span class="cov0" title="0">
                return runtimeapi.MountPropagation_PROPAGATION_HOST_TO_CONTAINER, nil</span>
        case *mountMode == v1.MountPropagationBidirectional:<span class="cov0" title="0">
                return runtimeapi.MountPropagation_PROPAGATION_BIDIRECTIONAL, nil</span>
        case *mountMode == v1.MountPropagationNone:<span class="cov0" title="0">
                return runtimeapi.MountPropagation_PROPAGATION_PRIVATE, nil</span>
        default:<span class="cov0" title="0">
                return 0, fmt.Errorf("invalid MountPropagation mode: %q", *mountMode)</span>
        }
}

// getEtcHostsPath returns the full host-side path to a pod's generated /etc/hosts file
func getEtcHostsPath(podDir string) string <span class="cov8" title="1">{
        hostsFilePath := filepath.Join(podDir, "etc-hosts")
        // Volume Mounts fail on Windows if it is not of the form C:/
        return volumeutil.MakeAbsolutePath(runtime.GOOS, hostsFilePath)
}</span>

// makeHostsMount makes the mountpoint for the hosts file that the containers
// in a pod are injected with. podIPs is provided instead of podIP as podIPs
// are present even if dual-stack feature flag is not enabled.
func makeHostsMount(podDir string, podIPs []string, hostName, hostDomainName string, hostAliases []v1.HostAlias, useHostNetwork bool) (*kubecontainer.Mount, error) <span class="cov8" title="1">{
        hostsFilePath := getEtcHostsPath(podDir)
        if err := ensureHostsFile(hostsFilePath, podIPs, hostName, hostDomainName, hostAliases, useHostNetwork); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">return &amp;kubecontainer.Mount{
                Name:           "k8s-managed-etc-hosts",
                ContainerPath:  etcHostsPath,
                HostPath:       hostsFilePath,
                ReadOnly:       false,
                SELinuxRelabel: true,
        }, nil</span>
}

// ensureHostsFile ensures that the given host file has an up-to-date ip, host
// name, and domain name.
func ensureHostsFile(fileName string, hostIPs []string, hostName, hostDomainName string, hostAliases []v1.HostAlias, useHostNetwork bool) error <span class="cov8" title="1">{
        var hostsFileContent []byte
        var err error

        if useHostNetwork </span><span class="cov0" title="0">{
                // if Pod is using host network, read hosts file from the node's filesystem.
                // `etcHostsPath` references the location of the hosts file on the node.
                // `/etc/hosts` for *nix systems.
                hostsFileContent, err = nodeHostsFileContent(etcHostsPath, hostAliases)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        } else<span class="cov8" title="1"> {
                // if Pod is not using host network, create a managed hosts file with Pod IP and other information.
                hostsFileContent = managedHostsFileContent(hostIPs, hostName, hostDomainName, hostAliases)
        }</span>

        <span class="cov8" title="1">hostsFilePerm := os.FileMode(0644)
        if err := os.WriteFile(fileName, hostsFileContent, hostsFilePerm); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">return os.Chmod(fileName, hostsFilePerm)</span>
}

// nodeHostsFileContent reads the content of node's hosts file.
func nodeHostsFileContent(hostsFilePath string, hostAliases []v1.HostAlias) ([]byte, error) <span class="cov0" title="0">{
        hostsFileContent, err := os.ReadFile(hostsFilePath)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">var buffer bytes.Buffer
        buffer.WriteString(managedHostsHeaderWithHostNetwork)
        buffer.Write(hostsFileContent)
        buffer.Write(hostsEntriesFromHostAliases(hostAliases))
        return buffer.Bytes(), nil</span>
}

// managedHostsFileContent generates the content of the managed etc hosts based on Pod IPs and other
// information.
func managedHostsFileContent(hostIPs []string, hostName, hostDomainName string, hostAliases []v1.HostAlias) []byte <span class="cov8" title="1">{
        var buffer bytes.Buffer
        buffer.WriteString(managedHostsHeader)
        buffer.WriteString("127.0.0.1\tlocalhost\n")                      // ipv4 localhost
        buffer.WriteString("::1\tlocalhost ip6-localhost ip6-loopback\n") // ipv6 localhost
        buffer.WriteString("fe00::0\tip6-localnet\n")
        buffer.WriteString("fe00::0\tip6-mcastprefix\n")
        buffer.WriteString("fe00::1\tip6-allnodes\n")
        buffer.WriteString("fe00::2\tip6-allrouters\n")
        if len(hostDomainName) &gt; 0 </span><span class="cov0" title="0">{
                // host entry generated for all IPs in podIPs
                // podIPs field is populated for clusters even
                // dual-stack feature flag is not enabled.
                for _, hostIP := range hostIPs </span><span class="cov0" title="0">{
                        buffer.WriteString(fmt.Sprintf("%s\t%s.%s\t%s\n", hostIP, hostName, hostDomainName, hostName))
                }</span>
        } else<span class="cov8" title="1"> {
                for _, hostIP := range hostIPs </span><span class="cov8" title="1">{
                        buffer.WriteString(fmt.Sprintf("%s\t%s\n", hostIP, hostName))
                }</span>
        }
        <span class="cov8" title="1">buffer.Write(hostsEntriesFromHostAliases(hostAliases))
        return buffer.Bytes()</span>
}

func hostsEntriesFromHostAliases(hostAliases []v1.HostAlias) []byte <span class="cov8" title="1">{
        if len(hostAliases) == 0 </span><span class="cov8" title="1">{
                return []byte{}
        }</span>

        <span class="cov0" title="0">var buffer bytes.Buffer
        buffer.WriteString("\n")
        buffer.WriteString("# Entries added by HostAliases.\n")
        // for each IP, write all aliases onto single line in hosts file
        for _, hostAlias := range hostAliases </span><span class="cov0" title="0">{
                buffer.WriteString(fmt.Sprintf("%s\t%s\n", hostAlias.IP, strings.Join(hostAlias.Hostnames, "\t")))
        }</span>
        <span class="cov0" title="0">return buffer.Bytes()</span>
}

// truncatePodHostnameIfNeeded truncates the pod hostname if it's longer than 63 chars.
func truncatePodHostnameIfNeeded(podName, hostname string) (string, error) <span class="cov8" title="1">{
        // Cap hostname at 63 chars (specification is 64bytes which is 63 chars and the null terminating char).
        const hostnameMaxLen = 63
        if len(hostname) &lt;= hostnameMaxLen </span><span class="cov8" title="1">{
                return hostname, nil
        }</span>
        <span class="cov0" title="0">truncated := hostname[:hostnameMaxLen]
        klog.ErrorS(nil, "Hostname for pod was too long, truncated it", "podName", podName, "hostnameMaxLen", hostnameMaxLen, "truncatedHostname", truncated)
        // hostname should not end with '-' or '.'
        truncated = strings.TrimRight(truncated, "-.")
        if len(truncated) == 0 </span><span class="cov0" title="0">{
                // This should never happen.
                return "", fmt.Errorf("hostname for pod %q was invalid: %q", podName, hostname)
        }</span>
        <span class="cov0" title="0">return truncated, nil</span>
}

// GetOrCreateUserNamespaceMappings returns the configuration for the sandbox user namespace
func (kl *Kubelet) GetOrCreateUserNamespaceMappings(pod *v1.Pod) (*runtimeapi.UserNamespace, error) <span class="cov8" title="1">{
        return kl.usernsManager.GetOrCreateUserNamespaceMappings(pod)
}</span>

// GeneratePodHostNameAndDomain creates a hostname and domain name for a pod,
// given that pod's spec and annotations or returns an error.
func (kl *Kubelet) GeneratePodHostNameAndDomain(pod *v1.Pod) (string, string, error) <span class="cov8" title="1">{
        clusterDomain := kl.dnsConfigurer.ClusterDomain

        hostname := pod.Name
        if len(pod.Spec.Hostname) &gt; 0 </span><span class="cov0" title="0">{
                if msgs := utilvalidation.IsDNS1123Label(pod.Spec.Hostname); len(msgs) != 0 </span><span class="cov0" title="0">{
                        return "", "", fmt.Errorf("pod Hostname %q is not a valid DNS label: %s", pod.Spec.Hostname, strings.Join(msgs, ";"))
                }</span>
                <span class="cov0" title="0">hostname = pod.Spec.Hostname</span>
        }

        <span class="cov8" title="1">hostname, err := truncatePodHostnameIfNeeded(pod.Name, hostname)
        if err != nil </span><span class="cov0" title="0">{
                return "", "", err
        }</span>

        <span class="cov8" title="1">hostDomain := ""
        if len(pod.Spec.Subdomain) &gt; 0 </span><span class="cov0" title="0">{
                if msgs := utilvalidation.IsDNS1123Label(pod.Spec.Subdomain); len(msgs) != 0 </span><span class="cov0" title="0">{
                        return "", "", fmt.Errorf("pod Subdomain %q is not a valid DNS label: %s", pod.Spec.Subdomain, strings.Join(msgs, ";"))
                }</span>
                <span class="cov0" title="0">hostDomain = fmt.Sprintf("%s.%s.svc.%s", pod.Spec.Subdomain, pod.Namespace, clusterDomain)</span>
        }

        <span class="cov8" title="1">return hostname, hostDomain, nil</span>
}

// GetPodCgroupParent gets pod cgroup parent from container manager.
func (kl *Kubelet) GetPodCgroupParent(pod *v1.Pod) string <span class="cov8" title="1">{
        pcm := kl.containerManager.NewPodContainerManager()
        _, cgroupParent := pcm.GetPodContainerName(pod)
        return cgroupParent
}</span>

// GenerateRunContainerOptions generates the RunContainerOptions, which can be used by
// the container runtime to set parameters for launching a container.
func (kl *Kubelet) GenerateRunContainerOptions(ctx context.Context, pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) (*kubecontainer.RunContainerOptions, func(), error) <span class="cov8" title="1">{
        opts, err := kl.containerManager.GetResources(pod, container)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        // The value of hostname is the short host name and it is sent to makeMounts to create /etc/hosts file.
        <span class="cov8" title="1">hostname, hostDomainName, err := kl.GeneratePodHostNameAndDomain(pod)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        // nodename will be equal to hostname if SetHostnameAsFQDN is nil or false. If SetHostnameFQDN
        // is true and hostDomainName is defined, nodename will be the FQDN (hostname.hostDomainName)
        <span class="cov8" title="1">nodename, err := util.GetNodenameForKernel(hostname, hostDomainName, pod.Spec.SetHostnameAsFQDN)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">opts.Hostname = nodename
        podName := volumeutil.GetUniquePodName(pod)
        volumes := kl.volumeManager.GetMountedVolumesForPod(podName)

        blkutil := volumepathhandler.NewBlockVolumePathHandler()
        blkVolumes, err := kl.makeBlockVolumes(pod, container, volumes, blkutil)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">opts.Devices = append(opts.Devices, blkVolumes...)

        envs, err := kl.makeEnvironmentVariables(pod, container, podIP, podIPs)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">opts.Envs = append(opts.Envs, envs...)

        // only podIPs is sent to makeMounts, as podIPs is populated even if dual-stack feature flag is not enabled.
        mounts, cleanupAction, err := makeMounts(pod, kl.getPodDir(pod.UID), container, hostname, hostDomainName, podIPs, volumes, kl.hostutil, kl.subpather, opts.Envs)
        if err != nil </span><span class="cov0" title="0">{
                return nil, cleanupAction, err
        }</span>
        <span class="cov8" title="1">opts.Mounts = append(opts.Mounts, mounts...)

        // adding TerminationMessagePath on Windows is only allowed if ContainerD is used. Individual files cannot
        // be mounted as volumes using Docker for Windows.
        if len(container.TerminationMessagePath) != 0 </span><span class="cov0" title="0">{
                p := kl.getPodContainerDir(pod.UID, container.Name)
                if err := os.MkdirAll(p, 0750); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Error on creating dir", "path", p)
                }</span> else<span class="cov0" title="0"> {
                        opts.PodContainerDir = p
                }</span>
        }

        <span class="cov8" title="1">return opts, cleanupAction, nil</span>
}

var masterServices = sets.NewString("kubernetes")

// getServiceEnvVarMap makes a map[string]string of env vars for services a
// pod in namespace ns should see.
func (kl *Kubelet) getServiceEnvVarMap(ns string, enableServiceLinks bool) (map[string]string, error) <span class="cov8" title="1">{
        var (
                serviceMap = make(map[string]*v1.Service)
                m          = make(map[string]string)
        )

        // Get all service resources from the master (via a cache),
        // and populate them into service environment variables.
        if kl.serviceLister == nil </span><span class="cov0" title="0">{
                // Kubelets without masters (e.g. plain GCE ContainerVM) don't set env vars.
                return m, nil
        }</span>
        <span class="cov8" title="1">services, err := kl.serviceLister.List(labels.Everything())
        if err != nil </span><span class="cov0" title="0">{
                return m, fmt.Errorf("failed to list services when setting up env vars")
        }</span>

        // project the services in namespace ns onto the master services
        <span class="cov8" title="1">for i := range services </span><span class="cov0" title="0">{
                service := services[i]
                // ignore services where ClusterIP is "None" or empty
                if !v1helper.IsServiceIPSet(service) </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">serviceName := service.Name

                // We always want to add environment variabled for master services
                // from the default namespace, even if enableServiceLinks is false.
                // We also add environment variables for other services in the same
                // namespace, if enableServiceLinks is true.
                if service.Namespace == metav1.NamespaceDefault &amp;&amp; masterServices.Has(serviceName) </span><span class="cov0" title="0">{
                        if _, exists := serviceMap[serviceName]; !exists </span><span class="cov0" title="0">{
                                serviceMap[serviceName] = service
                        }</span>
                } else<span class="cov0" title="0"> if service.Namespace == ns &amp;&amp; enableServiceLinks </span><span class="cov0" title="0">{
                        serviceMap[serviceName] = service
                }</span>
        }

        <span class="cov8" title="1">mappedServices := []*v1.Service{}
        for key := range serviceMap </span><span class="cov0" title="0">{
                mappedServices = append(mappedServices, serviceMap[key])
        }</span>

        <span class="cov8" title="1">for _, e := range envvars.FromServices(mappedServices) </span><span class="cov0" title="0">{
                m[e.Name] = e.Value
        }</span>
        <span class="cov8" title="1">return m, nil</span>
}

// Make the environment variables for a pod in the given namespace.
func (kl *Kubelet) makeEnvironmentVariables(pod *v1.Pod, container *v1.Container, podIP string, podIPs []string) ([]kubecontainer.EnvVar, error) <span class="cov8" title="1">{
        if pod.Spec.EnableServiceLinks == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("nil pod.spec.enableServiceLinks encountered, cannot construct envvars")
        }</span>

        // If the pod originates from the kube-api, when we know that the kube-apiserver is responding and the kubelet's credentials are valid.
        // Knowing this, it is reasonable to wait until the service lister has synchronized at least once before attempting to build
        // a service env var map.  This doesn't present the race below from happening entirely, but it does prevent the "obvious"
        // failure case of services simply not having completed a list operation that can reasonably be expected to succeed.
        // One common case this prevents is a kubelet restart reading pods before services and some pod not having the
        // KUBERNETES_SERVICE_HOST injected because we didn't wait a short time for services to sync before proceeding.
        // The KUBERNETES_SERVICE_HOST link is special because it is unconditionally injected into pods and is read by the
        // in-cluster-config for pod clients
        <span class="cov8" title="1">if !kubetypes.IsStaticPod(pod) &amp;&amp; !kl.serviceHasSynced() </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("services have not yet been read at least once, cannot construct envvars")
        }</span>

        <span class="cov8" title="1">var result []kubecontainer.EnvVar
        // Note:  These are added to the docker Config, but are not included in the checksum computed
        // by kubecontainer.HashContainer(...).  That way, we can still determine whether an
        // v1.Container is already running by its hash. (We don't want to restart a container just
        // because some service changed.)
        //
        // Note that there is a race between Kubelet seeing the pod and kubelet seeing the service.
        // To avoid this users can: (1) wait between starting a service and starting; or (2) detect
        // missing service env var and exit and be restarted; or (3) use DNS instead of env vars
        // and keep trying to resolve the DNS name of the service (recommended).
        serviceEnv, err := kl.getServiceEnvVarMap(pod.Namespace, *pod.Spec.EnableServiceLinks)
        if err != nil </span><span class="cov0" title="0">{
                return result, err
        }</span>

        <span class="cov8" title="1">var (
                configMaps = make(map[string]*v1.ConfigMap)
                secrets    = make(map[string]*v1.Secret)
                tmpEnv     = make(map[string]string)
        )

        // Env will override EnvFrom variables.
        // Process EnvFrom first then allow Env to replace existing values.
        for _, envFrom := range container.EnvFrom </span><span class="cov0" title="0">{
                switch </span>{
                case envFrom.ConfigMapRef != nil:<span class="cov0" title="0">
                        cm := envFrom.ConfigMapRef
                        name := cm.Name
                        configMap, ok := configMaps[name]
                        if !ok </span><span class="cov0" title="0">{
                                if kl.kubeClient == nil </span><span class="cov0" title="0">{
                                        return result, fmt.Errorf("couldn't get configMap %v/%v, no kubeClient defined", pod.Namespace, name)
                                }</span>
                                <span class="cov0" title="0">optional := cm.Optional != nil &amp;&amp; *cm.Optional
                                configMap, err = kl.configMapManager.GetConfigMap(pod.Namespace, name)
                                if err != nil </span><span class="cov0" title="0">{
                                        if errors.IsNotFound(err) &amp;&amp; optional </span><span class="cov0" title="0">{
                                                // ignore error when marked optional
                                                continue</span>
                                        }
                                        <span class="cov0" title="0">return result, err</span>
                                }
                                <span class="cov0" title="0">configMaps[name] = configMap</span>
                        }

                        <span class="cov0" title="0">invalidKeys := []string{}
                        for k, v := range configMap.Data </span><span class="cov0" title="0">{
                                if len(envFrom.Prefix) &gt; 0 </span><span class="cov0" title="0">{
                                        k = envFrom.Prefix + k
                                }</span>
                                <span class="cov0" title="0">if errMsgs := utilvalidation.IsEnvVarName(k); len(errMsgs) != 0 </span><span class="cov0" title="0">{
                                        invalidKeys = append(invalidKeys, k)
                                        continue</span>
                                }
                                <span class="cov0" title="0">tmpEnv[k] = v</span>
                        }
                        <span class="cov0" title="0">if len(invalidKeys) &gt; 0 </span><span class="cov0" title="0">{
                                sort.Strings(invalidKeys)
                                kl.recorder.Eventf(pod, v1.EventTypeWarning, "InvalidEnvironmentVariableNames", "Keys [%s] from the EnvFrom configMap %s/%s were skipped since they are considered invalid environment variable names.", strings.Join(invalidKeys, ", "), pod.Namespace, name)
                        }</span>
                case envFrom.SecretRef != nil:<span class="cov0" title="0">
                        s := envFrom.SecretRef
                        name := s.Name
                        secret, ok := secrets[name]
                        if !ok </span><span class="cov0" title="0">{
                                if kl.kubeClient == nil </span><span class="cov0" title="0">{
                                        return result, fmt.Errorf("couldn't get secret %v/%v, no kubeClient defined", pod.Namespace, name)
                                }</span>
                                <span class="cov0" title="0">optional := s.Optional != nil &amp;&amp; *s.Optional
                                secret, err = kl.secretManager.GetSecret(pod.Namespace, name)
                                if err != nil </span><span class="cov0" title="0">{
                                        if errors.IsNotFound(err) &amp;&amp; optional </span><span class="cov0" title="0">{
                                                // ignore error when marked optional
                                                continue</span>
                                        }
                                        <span class="cov0" title="0">return result, err</span>
                                }
                                <span class="cov0" title="0">secrets[name] = secret</span>
                        }

                        <span class="cov0" title="0">invalidKeys := []string{}
                        for k, v := range secret.Data </span><span class="cov0" title="0">{
                                if len(envFrom.Prefix) &gt; 0 </span><span class="cov0" title="0">{
                                        k = envFrom.Prefix + k
                                }</span>
                                <span class="cov0" title="0">if errMsgs := utilvalidation.IsEnvVarName(k); len(errMsgs) != 0 </span><span class="cov0" title="0">{
                                        invalidKeys = append(invalidKeys, k)
                                        continue</span>
                                }
                                <span class="cov0" title="0">tmpEnv[k] = string(v)</span>
                        }
                        <span class="cov0" title="0">if len(invalidKeys) &gt; 0 </span><span class="cov0" title="0">{
                                sort.Strings(invalidKeys)
                                kl.recorder.Eventf(pod, v1.EventTypeWarning, "InvalidEnvironmentVariableNames", "Keys [%s] from the EnvFrom secret %s/%s were skipped since they are considered invalid environment variable names.", strings.Join(invalidKeys, ", "), pod.Namespace, name)
                        }</span>
                }
        }

        // Determine the final values of variables:
        //
        // 1.  Determine the final value of each variable:
        //     a.  If the variable's Value is set, expand the `$(var)` references to other
        //         variables in the .Value field; the sources of variables are the declared
        //         variables of the container and the service environment variables
        //     b.  If a source is defined for an environment variable, resolve the source
        // 2.  Create the container's environment in the order variables are declared
        // 3.  Add remaining service environment vars
        <span class="cov8" title="1">var (
                mappingFunc = expansion.MappingFuncFor(tmpEnv, serviceEnv)
        )
        for _, envVar := range container.Env </span><span class="cov0" title="0">{
                runtimeVal := envVar.Value
                if runtimeVal != "" </span><span class="cov0" title="0">{
                        // Step 1a: expand variable references
                        runtimeVal = expansion.Expand(runtimeVal, mappingFunc)
                }</span> else<span class="cov0" title="0"> if envVar.ValueFrom != nil </span><span class="cov0" title="0">{
                        // Step 1b: resolve alternate env var sources
                        switch </span>{
                        case envVar.ValueFrom.FieldRef != nil:<span class="cov0" title="0">
                                runtimeVal, err = kl.podFieldSelectorRuntimeValue(envVar.ValueFrom.FieldRef, pod, podIP, podIPs)
                                if err != nil </span><span class="cov0" title="0">{
                                        return result, err
                                }</span>
                        case envVar.ValueFrom.ResourceFieldRef != nil:<span class="cov0" title="0">
                                defaultedPod, defaultedContainer, err := kl.defaultPodLimitsForDownwardAPI(pod, container)
                                if err != nil </span><span class="cov0" title="0">{
                                        return result, err
                                }</span>
                                <span class="cov0" title="0">runtimeVal, err = containerResourceRuntimeValue(envVar.ValueFrom.ResourceFieldRef, defaultedPod, defaultedContainer)
                                if err != nil </span><span class="cov0" title="0">{
                                        return result, err
                                }</span>
                        case envVar.ValueFrom.ConfigMapKeyRef != nil:<span class="cov0" title="0">
                                cm := envVar.ValueFrom.ConfigMapKeyRef
                                name := cm.Name
                                key := cm.Key
                                optional := cm.Optional != nil &amp;&amp; *cm.Optional
                                configMap, ok := configMaps[name]
                                if !ok </span><span class="cov0" title="0">{
                                        if kl.kubeClient == nil </span><span class="cov0" title="0">{
                                                return result, fmt.Errorf("couldn't get configMap %v/%v, no kubeClient defined", pod.Namespace, name)
                                        }</span>
                                        <span class="cov0" title="0">configMap, err = kl.configMapManager.GetConfigMap(pod.Namespace, name)
                                        if err != nil </span><span class="cov0" title="0">{
                                                if errors.IsNotFound(err) &amp;&amp; optional </span><span class="cov0" title="0">{
                                                        // ignore error when marked optional
                                                        continue</span>
                                                }
                                                <span class="cov0" title="0">return result, err</span>
                                        }
                                        <span class="cov0" title="0">configMaps[name] = configMap</span>
                                }
                                <span class="cov0" title="0">runtimeVal, ok = configMap.Data[key]
                                if !ok </span><span class="cov0" title="0">{
                                        if optional </span><span class="cov0" title="0">{
                                                continue</span>
                                        }
                                        <span class="cov0" title="0">return result, fmt.Errorf("couldn't find key %v in ConfigMap %v/%v", key, pod.Namespace, name)</span>
                                }
                        case envVar.ValueFrom.SecretKeyRef != nil:<span class="cov0" title="0">
                                s := envVar.ValueFrom.SecretKeyRef
                                name := s.Name
                                key := s.Key
                                optional := s.Optional != nil &amp;&amp; *s.Optional
                                secret, ok := secrets[name]
                                if !ok </span><span class="cov0" title="0">{
                                        if kl.kubeClient == nil </span><span class="cov0" title="0">{
                                                return result, fmt.Errorf("couldn't get secret %v/%v, no kubeClient defined", pod.Namespace, name)
                                        }</span>
                                        <span class="cov0" title="0">secret, err = kl.secretManager.GetSecret(pod.Namespace, name)
                                        if err != nil </span><span class="cov0" title="0">{
                                                if errors.IsNotFound(err) &amp;&amp; optional </span><span class="cov0" title="0">{
                                                        // ignore error when marked optional
                                                        continue</span>
                                                }
                                                <span class="cov0" title="0">return result, err</span>
                                        }
                                        <span class="cov0" title="0">secrets[name] = secret</span>
                                }
                                <span class="cov0" title="0">runtimeValBytes, ok := secret.Data[key]
                                if !ok </span><span class="cov0" title="0">{
                                        if optional </span><span class="cov0" title="0">{
                                                continue</span>
                                        }
                                        <span class="cov0" title="0">return result, fmt.Errorf("couldn't find key %v in Secret %v/%v", key, pod.Namespace, name)</span>
                                }
                                <span class="cov0" title="0">runtimeVal = string(runtimeValBytes)</span>
                        }
                }

                <span class="cov0" title="0">tmpEnv[envVar.Name] = runtimeVal</span>
        }

        // Append the env vars
        <span class="cov8" title="1">for k, v := range tmpEnv </span><span class="cov0" title="0">{
                result = append(result, kubecontainer.EnvVar{Name: k, Value: v})
        }</span>

        // Append remaining service env vars.
        <span class="cov8" title="1">for k, v := range serviceEnv </span><span class="cov0" title="0">{
                // Accesses apiserver+Pods.
                // So, the master may set service env vars, or kubelet may.  In case both are doing
                // it, we skip the key from the kubelet-generated ones so we don't have duplicate
                // env vars.
                // TODO: remove this next line once all platforms use apiserver+Pods.
                if _, present := tmpEnv[k]; !present </span><span class="cov0" title="0">{
                        result = append(result, kubecontainer.EnvVar{Name: k, Value: v})
                }</span>
        }
        <span class="cov8" title="1">return result, nil</span>
}

// podFieldSelectorRuntimeValue returns the runtime value of the given
// selector for a pod.
func (kl *Kubelet) podFieldSelectorRuntimeValue(fs *v1.ObjectFieldSelector, pod *v1.Pod, podIP string, podIPs []string) (string, error) <span class="cov0" title="0">{
        internalFieldPath, _, err := podshelper.ConvertDownwardAPIFieldLabel(fs.APIVersion, fs.FieldPath, "")
        if err != nil </span><span class="cov0" title="0">{
                return "", err
        }</span>

        // make podIPs order match node IP family preference #97979
        <span class="cov0" title="0">podIPs = kl.sortPodIPs(podIPs)
        if len(podIPs) &gt; 0 </span><span class="cov0" title="0">{
                podIP = podIPs[0]
        }</span>

        <span class="cov0" title="0">switch internalFieldPath </span>{
        case "spec.nodeName":<span class="cov0" title="0">
                return pod.Spec.NodeName, nil</span>
        case "spec.serviceAccountName":<span class="cov0" title="0">
                return pod.Spec.ServiceAccountName, nil</span>
        case "status.hostIP":<span class="cov0" title="0">
                hostIPs, err := kl.getHostIPsAnyWay()
                if err != nil </span><span class="cov0" title="0">{
                        return "", err
                }</span>
                <span class="cov0" title="0">return hostIPs[0].String(), nil</span>
        case "status.hostIPs":<span class="cov0" title="0">
                if !utilfeature.DefaultFeatureGate.Enabled(features.PodHostIPs) </span><span class="cov0" title="0">{
                        return "", nil
                }</span>
                <span class="cov0" title="0">hostIPs, err := kl.getHostIPsAnyWay()
                if err != nil </span><span class="cov0" title="0">{
                        return "", err
                }</span>
                <span class="cov0" title="0">ips := make([]string, 0, len(hostIPs))
                for _, ip := range hostIPs </span><span class="cov0" title="0">{
                        ips = append(ips, ip.String())
                }</span>
                <span class="cov0" title="0">return strings.Join(ips, ","), nil</span>
        case "status.podIP":<span class="cov0" title="0">
                return podIP, nil</span>
        case "status.podIPs":<span class="cov0" title="0">
                return strings.Join(podIPs, ","), nil</span>
        }
        <span class="cov0" title="0">return fieldpath.ExtractFieldPathAsString(pod, internalFieldPath)</span>
}

// containerResourceRuntimeValue returns the value of the provided container resource
func containerResourceRuntimeValue(fs *v1.ResourceFieldSelector, pod *v1.Pod, container *v1.Container) (string, error) <span class="cov0" title="0">{
        containerName := fs.ContainerName
        if len(containerName) == 0 </span><span class="cov0" title="0">{
                return resource.ExtractContainerResourceValue(fs, container)
        }</span>
        <span class="cov0" title="0">return resource.ExtractResourceValueByContainerName(fs, pod, containerName)</span>
}

// killPod instructs the container runtime to kill the pod. This method requires that
// the pod status contains the result of the last syncPod, otherwise it may fail to
// terminate newly created containers and sandboxes.
func (kl *Kubelet) killPod(ctx context.Context, pod *v1.Pod, p kubecontainer.Pod, gracePeriodOverride *int64) error <span class="cov8" title="1">{
        // Call the container runtime KillPod method which stops all known running containers of the pod
        if err := kl.containerRuntime.KillPod(ctx, pod, p, gracePeriodOverride); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := kl.containerManager.UpdateQOSCgroups(); err != nil </span><span class="cov0" title="0">{
                klog.V(2).InfoS("Failed to update QoS cgroups while killing pod", "err", err)
        }</span>
        <span class="cov8" title="1">return nil</span>
}

// makePodDataDirs creates the dirs for the pod datas.
func (kl *Kubelet) makePodDataDirs(pod *v1.Pod) error <span class="cov8" title="1">{
        uid := pod.UID
        if err := os.MkdirAll(kl.getPodDir(uid), 0750); err != nil &amp;&amp; !os.IsExist(err) </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := os.MkdirAll(kl.getPodVolumesDir(uid), 0750); err != nil &amp;&amp; !os.IsExist(err) </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := os.MkdirAll(kl.getPodPluginsDir(uid), 0750); err != nil &amp;&amp; !os.IsExist(err) </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">return nil</span>
}

// getPullSecretsForPod inspects the Pod and retrieves the referenced pull
// secrets.
func (kl *Kubelet) getPullSecretsForPod(pod *v1.Pod) []v1.Secret <span class="cov8" title="1">{
        pullSecrets := []v1.Secret{}
        failedPullSecrets := []string{}

        for _, secretRef := range pod.Spec.ImagePullSecrets </span><span class="cov0" title="0">{
                if len(secretRef.Name) == 0 </span><span class="cov0" title="0">{
                        // API validation permitted entries with empty names (https://issue.k8s.io/99454#issuecomment-787838112).
                        // Ignore to avoid unnecessary warnings.
                        continue</span>
                }
                <span class="cov0" title="0">secret, err := kl.secretManager.GetSecret(pod.Namespace, secretRef.Name)
                if err != nil </span><span class="cov0" title="0">{
                        klog.InfoS("Unable to retrieve pull secret, the image pull may not succeed.", "pod", klog.KObj(pod), "secret", klog.KObj(secret), "err", err)
                        failedPullSecrets = append(failedPullSecrets, secretRef.Name)
                        continue</span>
                }

                <span class="cov0" title="0">pullSecrets = append(pullSecrets, *secret)</span>
        }

        <span class="cov8" title="1">if len(failedPullSecrets) &gt; 0 </span><span class="cov0" title="0">{
                kl.recorder.Eventf(pod, v1.EventTypeWarning, "FailedToRetrieveImagePullSecret", "Unable to retrieve some image pull secrets (%s); attempting to pull the image may not succeed.", strings.Join(failedPullSecrets, ", "))
        }</span>

        <span class="cov8" title="1">return pullSecrets</span>
}

// PodCouldHaveRunningContainers returns true if the pod with the given UID could still have running
// containers. This returns false if the pod has not yet been started or the pod is unknown.
func (kl *Kubelet) PodCouldHaveRunningContainers(pod *v1.Pod) bool <span class="cov0" title="0">{
        if kl.podWorkers.CouldHaveRunningContainers(pod.UID) </span><span class="cov0" title="0">{
                return true
        }</span>

        // Check if pod might need to unprepare resources before termination
        // NOTE: This is a temporary solution. This call is here to avoid changing
        // status manager and its tests.
        // TODO: extend PodDeletionSafetyProvider interface and implement it
        // in a separate Kubelet method.
        <span class="cov0" title="0">if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) </span><span class="cov0" title="0">{
                if kl.containerManager.PodMightNeedToUnprepareResources(pod.UID) </span><span class="cov0" title="0">{
                        return true
                }</span>
        }
        <span class="cov0" title="0">return false</span>
}

// PodIsFinished returns true if SyncTerminatedPod is finished, ie.
// all required node-level resources that a pod was consuming have
// been reclaimed by the kubelet.
func (kl *Kubelet) PodIsFinished(pod *v1.Pod) bool <span class="cov0" title="0">{
        return kl.podWorkers.ShouldPodBeFinished(pod.UID)
}</span>

// filterOutInactivePods returns pods that are not in a terminal phase
// or are known to be fully terminated. This method should only be used
// when the set of pods being filtered is upstream of the pod worker, i.e.
// the pods the pod manager is aware of.
func (kl *Kubelet) filterOutInactivePods(pods []*v1.Pod) []*v1.Pod <span class="cov8" title="1">{
        filteredPods := make([]*v1.Pod, 0, len(pods))
        for _, p := range pods </span><span class="cov8" title="1">{
                // if a pod is fully terminated by UID, it should be excluded from the
                // list of pods
                if kl.podWorkers.IsPodKnownTerminated(p.UID) </span><span class="cov8" title="1">{
                        continue</span>
                }

                // terminal pods are considered inactive UNLESS they are actively terminating
                <span class="cov8" title="1">if kl.isAdmittedPodTerminal(p) &amp;&amp; !kl.podWorkers.IsPodTerminationRequested(p.UID) </span><span class="cov8" title="1">{
                        continue</span>
                }

                <span class="cov8" title="1">filteredPods = append(filteredPods, p)</span>
        }
        <span class="cov8" title="1">return filteredPods</span>
}

func (kl *Kubelet) FilterOutInactivePods(pods []*v1.Pod) []*v1.Pod <span class="cov0" title="0">{
        filteredPods := make([]*v1.Pod, 0, len(pods))
        for _, p := range pods </span><span class="cov0" title="0">{
                // if a pod is fully terminated by UID, it should be excluded from the
                // list of pods
                if kl.podWorkers.IsPodKnownTerminated(p.UID) </span><span class="cov0" title="0">{
                        continue</span>
                }

                // terminal pods are considered inactive UNLESS they are actively terminating
                <span class="cov0" title="0">if kl.isAdmittedPodTerminal(p) &amp;&amp; !kl.podWorkers.IsPodTerminationRequested(p.UID) </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov0" title="0">filteredPods = append(filteredPods, p)</span>
        }
        <span class="cov0" title="0">return filteredPods</span>
}

// isAdmittedPodTerminal returns true if the provided config source pod is in
// a terminal phase, or if the Kubelet has already indicated the pod has reached
// a terminal phase but the config source has not accepted it yet. This method
// should only be used within the pod configuration loops that notify the pod
// worker, other components should treat the pod worker as authoritative.
func (kl *Kubelet) isAdmittedPodTerminal(pod *v1.Pod) bool <span class="cov8" title="1">{
        // pods are considered inactive if the config source has observed a
        // terminal phase (if the Kubelet recorded that the pod reached a terminal
        // phase the pod should never be restarted)
        if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed </span><span class="cov8" title="1">{
                return true
        }</span>
        // a pod that has been marked terminal within the Kubelet is considered
        // inactive (may have been rejected by Kubelet admission)
        <span class="cov8" title="1">if status, ok := kl.statusManager.GetPodStatus(pod.UID); ok </span><span class="cov8" title="1">{
                if status.Phase == v1.PodSucceeded || status.Phase == v1.PodFailed </span><span class="cov8" title="1">{
                        return true
                }</span>
        }
        <span class="cov8" title="1">return false</span>
}

// removeOrphanedPodStatuses removes obsolete entries in podStatus where
// the pod is no longer considered bound to this node.
func (kl *Kubelet) removeOrphanedPodStatuses(pods []*v1.Pod, mirrorPods []*v1.Pod) <span class="cov8" title="1">{
        podUIDs := make(map[types.UID]bool)
        for _, pod := range pods </span><span class="cov8" title="1">{
                podUIDs[pod.UID] = true
        }</span>
        <span class="cov8" title="1">for _, pod := range mirrorPods </span><span class="cov8" title="1">{
                podUIDs[pod.UID] = true
        }</span>
        <span class="cov8" title="1">kl.statusManager.RemoveOrphanedStatuses(podUIDs)</span>
}

// HandlePodCleanups performs a series of cleanup work, including terminating
// pod workers, killing unwanted pods, and removing orphaned volumes/pod
// directories. No config changes are sent to pod workers while this method
// is executing which means no new pods can appear. After this method completes
// the desired state of the kubelet should be reconciled with the actual state
// in the pod worker and other pod-related components.
//
// This function is executed by the main sync loop, so it must execute quickly
// and all nested calls should be asynchronous. Any slow reconciliation actions
// should be performed by other components (like the volume manager). The duration
// of this call is the minimum latency for static pods to be restarted if they
// are updated with a fixed UID (most should use a dynamic UID), and no config
// updates are delivered to the pod workers while this method is running.
func (kl *Kubelet) HandlePodCleanups(ctx context.Context) error <span class="cov8" title="1">{
        // The kubelet lacks checkpointing, so we need to introspect the set of pods
        // in the cgroup tree prior to inspecting the set of pods in our pod manager.
        // this ensures our view of the cgroup tree does not mistakenly observe pods
        // that are added after the fact...
        var (
                cgroupPods map[types.UID]cm.CgroupName
                err        error
        )
        if kl.cgroupsPerQOS </span><span class="cov8" title="1">{
                pcm := kl.containerManager.NewPodContainerManager()
                cgroupPods, err = pcm.GetAllPodsFromCgroups()
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("failed to get list of pods that still exist on cgroup mounts: %v", err)
                }</span>
        }

        <span class="cov8" title="1">allPods, mirrorPods, orphanedMirrorPodFullnames := kl.podManager.GetPodsAndMirrorPods()

        // Pod phase progresses monotonically. Once a pod has reached a final state,
        // it should never leave regardless of the restart policy. The statuses
        // of such pods should not be changed, and there is no need to sync them.
        // TODO: the logic here does not handle two cases:
        //   1. If the containers were removed immediately after they died, kubelet
        //      may fail to generate correct statuses, let alone filtering correctly.
        //   2. If kubelet restarted before writing the terminated status for a pod
        //      to the apiserver, it could still restart the terminated pod (even
        //      though the pod was not considered terminated by the apiserver).
        // These two conditions could be alleviated by checkpointing kubelet.

        // Stop the workers for terminated pods not in the config source
        klog.V(3).InfoS("Clean up pod workers for terminated pods")
        workingPods := kl.podWorkers.SyncKnownPods(allPods)

        // Reconcile: At this point the pod workers have been pruned to the set of
        // desired pods. Pods that must be restarted due to UID reuse, or leftover
        // pods from previous runs, are not known to the pod worker.

        allPodsByUID := make(map[types.UID]*v1.Pod)
        for _, pod := range allPods </span><span class="cov8" title="1">{
                allPodsByUID[pod.UID] = pod
        }</span>

        // Identify the set of pods that have workers, which should be all pods
        // from config that are not terminated, as well as any terminating pods
        // that have already been removed from config. Pods that are terminating
        // will be added to possiblyRunningPods, to prevent overly aggressive
        // cleanup of pod cgroups.
        <span class="cov8" title="1">stringIfTrue := func(t bool) string </span><span class="cov8" title="1">{
                if t </span><span class="cov8" title="1">{
                        return "true"
                }</span>
                <span class="cov8" title="1">return ""</span>
        }
        <span class="cov8" title="1">runningPods := make(map[types.UID]sets.Empty)
        possiblyRunningPods := make(map[types.UID]sets.Empty)
        for uid, sync := range workingPods </span><span class="cov0" title="0">{
                switch sync.State </span>{
                case SyncPod:<span class="cov0" title="0">
                        runningPods[uid] = struct{}{}
                        possiblyRunningPods[uid] = struct{}{}</span>
                case TerminatingPod:<span class="cov0" title="0">
                        possiblyRunningPods[uid] = struct{}{}</span>
                default:<span class="cov0" title="0"></span>
                }
        }

        // Retrieve the list of running containers from the runtime to perform cleanup.
        // We need the latest state to avoid delaying restarts of static pods that reuse
        // a UID.
        <span class="cov8" title="1">if err := kl.runtimeCache.ForceUpdateIfOlder(ctx, kl.clock.Now()); err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Error listing containers")
                return err
        }</span>
        <span class="cov8" title="1">runningRuntimePods, err := kl.runtimeCache.GetPods(ctx)
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Error listing containers")
                return err
        }</span>

        // Stop probing pods that are not running
        <span class="cov8" title="1">klog.V(3).InfoS("Clean up probes for terminated pods")
        kl.probeManager.CleanupPods(possiblyRunningPods)

        // Remove orphaned pod statuses not in the total list of known config pods
        klog.V(3).InfoS("Clean up orphaned pod statuses")
        kl.removeOrphanedPodStatuses(allPods, mirrorPods)

        // Remove orphaned pod user namespace allocations (if any).
        klog.V(3).InfoS("Clean up orphaned pod user namespace allocations")
        if err = kl.usernsManager.CleanupOrphanedPodUsernsAllocations(allPods, runningRuntimePods); err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Failed cleaning up orphaned pod user namespaces allocations")
        }</span>

        // Remove orphaned volumes from pods that are known not to have any
        // containers. Note that we pass all pods (including terminated pods) to
        // the function, so that we don't remove volumes associated with terminated
        // but not yet deleted pods.
        // TODO: this method could more aggressively cleanup terminated pods
        // in the future (volumes, mount dirs, logs, and containers could all be
        // better separated)
        <span class="cov8" title="1">klog.V(3).InfoS("Clean up orphaned pod directories")
        err = kl.cleanupOrphanedPodDirs(allPods, runningRuntimePods)
        if err != nil </span><span class="cov0" title="0">{
                // We want all cleanup tasks to be run even if one of them failed. So
                // we just log an error here and continue other cleanup tasks.
                // This also applies to the other clean up tasks.
                klog.ErrorS(err, "Failed cleaning up orphaned pod directories")
        }</span>

        // Remove any orphaned mirror pods (mirror pods are tracked by name via the
        // pod worker)
        <span class="cov8" title="1">klog.V(3).InfoS("Clean up orphaned mirror pods")
        for _, podFullname := range orphanedMirrorPodFullnames </span><span class="cov8" title="1">{
                if !kl.podWorkers.IsPodForMirrorPodTerminatingByFullName(podFullname) </span><span class="cov8" title="1">{
                        _, err := kl.mirrorPodClient.DeleteMirrorPod(podFullname, nil)
                        if err != nil </span><span class="cov0" title="0">{
                                klog.ErrorS(err, "Encountered error when deleting mirror pod", "podName", podFullname)
                        }</span> else<span class="cov8" title="1"> {
                                klog.V(3).InfoS("Deleted mirror pod", "podName", podFullname)
                        }</span>
                }
        }

        // After pruning pod workers for terminated pods get the list of active pods for
        // metrics and to determine restarts.
        <span class="cov8" title="1">activePods := kl.filterOutInactivePods(allPods)
        allRegularPods, allStaticPods := splitPodsByStatic(allPods)
        activeRegularPods, activeStaticPods := splitPodsByStatic(activePods)
        metrics.DesiredPodCount.WithLabelValues("").Set(float64(len(allRegularPods)))
        metrics.DesiredPodCount.WithLabelValues("true").Set(float64(len(allStaticPods)))
        metrics.ActivePodCount.WithLabelValues("").Set(float64(len(activeRegularPods)))
        metrics.ActivePodCount.WithLabelValues("true").Set(float64(len(activeStaticPods)))
        metrics.MirrorPodCount.Set(float64(len(mirrorPods)))

        // At this point, the pod worker is aware of which pods are not desired (SyncKnownPods).
        // We now look through the set of active pods for those that the pod worker is not aware of
        // and deliver an update. The most common reason a pod is not known is because the pod was
        // deleted and recreated with the same UID while the pod worker was driving its lifecycle (very
        // very rare for API pods, common for static pods with fixed UIDs). Containers that may still
        // be running from a previous execution must be reconciled by the pod worker's sync method.
        // We must use active pods because that is the set of admitted pods (podManager includes pods
        // that will never be run, and statusManager tracks already rejected pods).
        var restartCount, restartCountStatic int
        for _, desiredPod := range activePods </span><span class="cov8" title="1">{
                if _, knownPod := workingPods[desiredPod.UID]; knownPod </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov8" title="1">klog.V(3).InfoS("Pod will be restarted because it is in the desired set and not known to the pod workers (likely due to UID reuse)", "podUID", desiredPod.UID)
                isStatic := kubetypes.IsStaticPod(desiredPod)
                pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(desiredPod)
                if pod == nil || wasMirror </span><span class="cov0" title="0">{
                        klog.V(2).InfoS("Programmer error, restartable pod was a mirror pod but activePods should never contain a mirror pod", "podUID", desiredPod.UID)
                        continue</span>
                }
                <span class="cov8" title="1">kl.podWorkers.UpdatePod(UpdatePodOptions{
                        UpdateType: kubetypes.SyncPodCreate,
                        Pod:        pod,
                        MirrorPod:  mirrorPod,
                })

                // the desired pod is now known as well
                workingPods[desiredPod.UID] = PodWorkerSync{State: SyncPod, HasConfig: true, Static: isStatic}
                if isStatic </span><span class="cov0" title="0">{
                        // restartable static pods are the normal case
                        restartCountStatic++
                }</span> else<span class="cov8" title="1"> {
                        // almost certainly means shenanigans, as API pods should never have the same UID after being deleted and recreated
                        // unless there is a major API violation
                        restartCount++
                }</span>
        }
        <span class="cov8" title="1">metrics.RestartedPodTotal.WithLabelValues("true").Add(float64(restartCountStatic))
        metrics.RestartedPodTotal.WithLabelValues("").Add(float64(restartCount))

        // Complete termination of deleted pods that are not runtime pods (don't have
        // running containers), are terminal, and are not known to pod workers.
        // An example is pods rejected during kubelet admission that have never
        // started before (i.e. does not have an orphaned pod).
        // Adding the pods with SyncPodKill to pod workers allows to proceed with
        // force-deletion of such pods, yet preventing re-entry of the routine in the
        // next invocation of HandlePodCleanups.
        for _, pod := range kl.filterTerminalPodsToDelete(allPods, runningRuntimePods, workingPods) </span><span class="cov0" title="0">{
                klog.V(3).InfoS("Handling termination and deletion of the pod to pod workers", "pod", klog.KObj(pod), "podUID", pod.UID)
                kl.podWorkers.UpdatePod(UpdatePodOptions{
                        UpdateType: kubetypes.SyncPodKill,
                        Pod:        pod,
                })
        }</span>

        // Finally, terminate any pods that are observed in the runtime but not present in the list of
        // known running pods from config. If we do terminate running runtime pods that will happen
        // asynchronously in the background and those will be processed in the next invocation of
        // HandlePodCleanups.
        <span class="cov8" title="1">var orphanCount int
        for _, runningPod := range runningRuntimePods </span><span class="cov8" title="1">{
                // If there are orphaned pod resources in CRI that are unknown to the pod worker, terminate them
                // now. Since housekeeping is exclusive to other pod worker updates, we know that no pods have
                // been added to the pod worker in the meantime. Note that pods that are not visible in the runtime
                // but which were previously known are terminated by SyncKnownPods().
                _, knownPod := workingPods[runningPod.ID]
                if !knownPod </span><span class="cov8" title="1">{
                        one := int64(1)
                        killPodOptions := &amp;KillPodOptions{
                                PodTerminationGracePeriodSecondsOverride: &amp;one,
                        }
                        klog.V(2).InfoS("Clean up containers for orphaned pod we had not seen before", "podUID", runningPod.ID, "killPodOptions", killPodOptions)
                        kl.podWorkers.UpdatePod(UpdatePodOptions{
                                UpdateType:     kubetypes.SyncPodKill,
                                RunningPod:     runningPod,
                                KillPodOptions: killPodOptions,
                        })

                        // the running pod is now known as well
                        workingPods[runningPod.ID] = PodWorkerSync{State: TerminatingPod, Orphan: true}
                        orphanCount++
                }</span>
        }
        <span class="cov8" title="1">metrics.OrphanedRuntimePodTotal.Add(float64(orphanCount))

        // Now that we have recorded any terminating pods, and added new pods that should be running,
        // record a summary here. Not all possible combinations of PodWorkerSync values are valid.
        counts := make(map[PodWorkerSync]int)
        for _, sync := range workingPods </span><span class="cov8" title="1">{
                counts[sync]++
        }</span>
        <span class="cov8" title="1">for validSync, configState := range map[PodWorkerSync]string{
                {HasConfig: true, Static: true}:                "desired",
                {HasConfig: true, Static: false}:               "desired",
                {Orphan: true, HasConfig: true, Static: true}:  "orphan",
                {Orphan: true, HasConfig: true, Static: false}: "orphan",
                {Orphan: true, HasConfig: false}:               "runtime_only",
        } </span><span class="cov8" title="1">{
                for _, state := range []PodWorkerState{SyncPod, TerminatingPod, TerminatedPod} </span><span class="cov8" title="1">{
                        validSync.State = state
                        count := counts[validSync]
                        delete(counts, validSync)
                        staticString := stringIfTrue(validSync.Static)
                        if !validSync.HasConfig </span><span class="cov8" title="1">{
                                staticString = "unknown"
                        }</span>
                        <span class="cov8" title="1">metrics.WorkingPodCount.WithLabelValues(state.String(), configState, staticString).Set(float64(count))</span>
                }
        }
        <span class="cov8" title="1">if len(counts) &gt; 0 </span><span class="cov0" title="0">{
                // in case a combination is lost
                klog.V(3).InfoS("Programmer error, did not report a kubelet_working_pods metric for a value returned by SyncKnownPods", "counts", counts)
        }</span>

        // Remove any cgroups in the hierarchy for pods that are definitely no longer
        // running (not in the container runtime).
        <span class="cov8" title="1">if kl.cgroupsPerQOS </span><span class="cov8" title="1">{
                pcm := kl.containerManager.NewPodContainerManager()
                klog.V(3).InfoS("Clean up orphaned pod cgroups")
                kl.cleanupOrphanedPodCgroups(pcm, cgroupPods, possiblyRunningPods)
        }</span>

        // Cleanup any backoff entries.
        <span class="cov8" title="1">kl.backOff.GC()
        return nil</span>
}

// filterTerminalPodsToDelete returns terminal pods which are ready to be
// deleted by the status manager, but are not in pod workers.
// First, the check for deletionTimestamp is a performance optimization as we
// don't need to do anything with terminal pods without deletionTimestamp.
// Second, the check for terminal pods is to avoid race conditions of triggering
// deletion on Pending pods which are not yet added to pod workers.
// Third, the check to skip pods known to pod workers is that the lifecycle of
// such pods is already handled by pod workers.
// Finally, we skip runtime pods as their termination is handled separately in
// the HandlePodCleanups routine.
func (kl *Kubelet) filterTerminalPodsToDelete(allPods []*v1.Pod, runningRuntimePods []*kubecontainer.Pod, workingPods map[types.UID]PodWorkerSync) map[types.UID]*v1.Pod <span class="cov8" title="1">{
        terminalPodsToDelete := make(map[types.UID]*v1.Pod)
        for _, pod := range allPods </span><span class="cov8" title="1">{
                if pod.DeletionTimestamp == nil </span><span class="cov8" title="1">{
                        // skip pods which don't have a deletion timestamp
                        continue</span>
                }
                <span class="cov0" title="0">if !podutil.IsPodPhaseTerminal(pod.Status.Phase) </span><span class="cov0" title="0">{
                        // skip the non-terminal pods
                        continue</span>
                }
                <span class="cov0" title="0">if _, knownPod := workingPods[pod.UID]; knownPod </span><span class="cov0" title="0">{
                        // skip pods known to pod workers
                        continue</span>
                }
                <span class="cov0" title="0">terminalPodsToDelete[pod.UID] = pod</span>
        }
        <span class="cov8" title="1">for _, runningRuntimePod := range runningRuntimePods </span><span class="cov8" title="1">{
                // skip running runtime pods - they are handled by a dedicated routine
                // which terminates the containers
                delete(terminalPodsToDelete, runningRuntimePod.ID)
        }</span>
        <span class="cov8" title="1">return terminalPodsToDelete</span>
}

// splitPodsByStatic separates a list of desired pods from the pod manager into
// regular or static pods. Mirror pods are not valid config sources (a mirror pod
// being created cannot cause the Kubelet to start running a static pod) and are
// excluded.
func splitPodsByStatic(pods []*v1.Pod) (regular, static []*v1.Pod) <span class="cov8" title="1">{
        regular, static = make([]*v1.Pod, 0, len(pods)), make([]*v1.Pod, 0, len(pods))
        for _, pod := range pods </span><span class="cov8" title="1">{
                if kubetypes.IsMirrorPod(pod) </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov8" title="1">if kubetypes.IsStaticPod(pod) </span><span class="cov0" title="0">{
                        static = append(static, pod)
                }</span> else<span class="cov8" title="1"> {
                        regular = append(regular, pod)
                }</span>
        }
        <span class="cov8" title="1">return regular, static</span>
}

// validateContainerLogStatus returns the container ID for the desired container to retrieve logs for, based on the state
// of the container. The previous flag will only return the logs for the last terminated container, otherwise, the current
// running container is preferred over a previous termination. If info about the container is not available then a specific
// error is returned to the end user.
func (kl *Kubelet) validateContainerLogStatus(podName string, podStatus *v1.PodStatus, containerName string, previous bool) (containerID kubecontainer.ContainerID, err error) <span class="cov8" title="1">{
        var cID string

        cStatus, found := podutil.GetContainerStatus(podStatus.ContainerStatuses, containerName)
        if !found </span><span class="cov8" title="1">{
                cStatus, found = podutil.GetContainerStatus(podStatus.InitContainerStatuses, containerName)
        }</span>
        <span class="cov8" title="1">if !found </span><span class="cov8" title="1">{
                cStatus, found = podutil.GetContainerStatus(podStatus.EphemeralContainerStatuses, containerName)
        }</span>
        <span class="cov8" title="1">if !found </span><span class="cov8" title="1">{
                return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is not available", containerName, podName)
        }</span>
        <span class="cov8" title="1">lastState := cStatus.LastTerminationState
        waiting, running, terminated := cStatus.State.Waiting, cStatus.State.Running, cStatus.State.Terminated

        switch </span>{
        case previous:<span class="cov8" title="1">
                if lastState.Terminated == nil || lastState.Terminated.ContainerID == "" </span><span class="cov8" title="1">{
                        return kubecontainer.ContainerID{}, fmt.Errorf("previous terminated container %q in pod %q not found", containerName, podName)
                }</span>
                <span class="cov8" title="1">cID = lastState.Terminated.ContainerID</span>

        case running != nil:<span class="cov8" title="1">
                cID = cStatus.ContainerID</span>

        case terminated != nil:<span class="cov8" title="1">
                // in cases where the next container didn't start, terminated.ContainerID will be empty, so get logs from the lastState.Terminated.
                if terminated.ContainerID == "" </span><span class="cov8" title="1">{
                        if lastState.Terminated != nil &amp;&amp; lastState.Terminated.ContainerID != "" </span><span class="cov8" title="1">{
                                cID = lastState.Terminated.ContainerID
                        }</span> else<span class="cov8" title="1"> {
                                return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is terminated", containerName, podName)
                        }</span>
                } else<span class="cov8" title="1"> {
                        cID = terminated.ContainerID
                }</span>

        case lastState.Terminated != nil:<span class="cov0" title="0">
                if lastState.Terminated.ContainerID == "" </span><span class="cov0" title="0">{
                        return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is terminated", containerName, podName)
                }</span>
                <span class="cov0" title="0">cID = lastState.Terminated.ContainerID</span>

        case waiting != nil:<span class="cov8" title="1">
                // output some info for the most common pending failures
                switch reason := waiting.Reason; reason </span>{
                case images.ErrImagePull.Error():<span class="cov8" title="1">
                        return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is waiting to start: image can't be pulled", containerName, podName)</span>
                case images.ErrImagePullBackOff.Error():<span class="cov0" title="0">
                        return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is waiting to start: trying and failing to pull image", containerName, podName)</span>
                default:<span class="cov8" title="1">
                        return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is waiting to start: %v", containerName, podName, reason)</span>
                }
        default:<span class="cov0" title="0">
                // unrecognized state
                return kubecontainer.ContainerID{}, fmt.Errorf("container %q in pod %q is waiting to start - no logs yet", containerName, podName)</span>
        }

        <span class="cov8" title="1">return kubecontainer.ParseContainerID(cID), nil</span>
}

// ValidateContainerLogStatus returns the container ID for the desired container to retrieve logs for, based on the state
func (kl *Kubelet) ValidateContainerLogStatus(podName string, podStatus *v1.PodStatus, containerName string, previous bool) (containerID kubecontainer.ContainerID, err error) <span class="cov0" title="0">{
        return kl.validateContainerLogStatus(podName, podStatus, containerName, previous)
}</span>

// GetKubeletContainerLogs returns logs from the container
// TODO: this method is returning logs of random container attempts, when it should be returning the most recent attempt
// or all of them.
func (kl *Kubelet) GetKubeletContainerLogs(ctx context.Context, podFullName, containerName string, logOptions *v1.PodLogOptions, stdout, stderr io.Writer) error <span class="cov0" title="0">{
        // Pod workers periodically write status to statusManager. If status is not
        // cached there, something is wrong (or kubelet just restarted and hasn't
        // caught up yet). Just assume the pod is not ready yet.
        name, namespace, err := kubecontainer.ParsePodFullName(podFullName)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("unable to parse pod full name %q: %v", podFullName, err)
        }</span>

        <span class="cov0" title="0">pod, ok := kl.GetPodByName(namespace, name)
        if !ok </span><span class="cov0" title="0">{
                return fmt.Errorf("pod %q cannot be found - no logs available", name)
        }</span>

        // TODO: this should be using the podWorker's pod store as authoritative, since
        // the mirrorPod might still exist, the pod may have been force deleted but
        // is still terminating (users should be able to view logs of force deleted static pods
        // based on full name).
        <span class="cov0" title="0">var podUID types.UID
        pod, mirrorPod, wasMirror := kl.podManager.GetPodAndMirrorPod(pod)
        if wasMirror </span><span class="cov0" title="0">{
                if pod == nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("mirror pod %q does not have a corresponding pod", name)
                }</span>
                <span class="cov0" title="0">podUID = mirrorPod.UID</span>
        } else<span class="cov0" title="0"> {
                podUID = pod.UID
        }</span>

        <span class="cov0" title="0">podStatus, found := kl.statusManager.GetPodStatus(podUID)
        if !found </span><span class="cov0" title="0">{
                // If there is no cached status, use the status from the
                // config source (apiserver). This is useful if kubelet
                // has recently been restarted.
                podStatus = pod.Status
        }</span>

        // TODO: Consolidate the logic here with kuberuntime.GetContainerLogs, here we convert container name to containerID,
        // but inside kuberuntime we convert container id back to container name and restart count.
        // TODO: After separate container log lifecycle management, we should get log based on the existing log files
        // instead of container status.
        <span class="cov0" title="0">containerID, err := kl.validateContainerLogStatus(pod.Name, &amp;podStatus, containerName, logOptions.Previous)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Do a zero-byte write to stdout before handing off to the container runtime.
        // This ensures at least one Write call is made to the writer when copying starts,
        // even if we then block waiting for log output from the container.
        <span class="cov0" title="0">if _, err := stdout.Write([]byte{}); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov0" title="0">return kl.containerRuntime.GetContainerLogs(ctx, pod, containerID, logOptions, stdout, stderr)</span>
}

// getPhase returns the phase of a pod given its container info.
func getPhase(pod *v1.Pod, info []v1.ContainerStatus, podIsTerminal bool) v1.PodPhase <span class="cov8" title="1">{
        spec := pod.Spec
        pendingInitialization := 0
        failedInitialization := 0

        // regular init containers
        for _, container := range spec.InitContainers </span><span class="cov8" title="1">{
                if kubetypes.IsRestartableInitContainer(&amp;container) </span><span class="cov0" title="0">{
                        // Skip the restartable init containers here to handle them separately as
                        // they are slightly different from the init containers in terms of the
                        // pod phase.
                        continue</span>
                }

                <span class="cov8" title="1">containerStatus, ok := podutil.GetContainerStatus(info, container.Name)
                if !ok </span><span class="cov0" title="0">{
                        pendingInitialization++
                        continue</span>
                }

                <span class="cov8" title="1">switch </span>{
                case containerStatus.State.Running != nil:<span class="cov8" title="1">
                        pendingInitialization++</span>
                case containerStatus.State.Terminated != nil:<span class="cov8" title="1">
                        if containerStatus.State.Terminated.ExitCode != 0 </span><span class="cov8" title="1">{
                                failedInitialization++
                        }</span>
                case containerStatus.State.Waiting != nil:<span class="cov8" title="1">
                        if containerStatus.LastTerminationState.Terminated != nil </span><span class="cov8" title="1">{
                                if containerStatus.LastTerminationState.Terminated.ExitCode != 0 </span><span class="cov8" title="1">{
                                        failedInitialization++
                                }</span>
                        } else<span class="cov8" title="1"> {
                                pendingInitialization++
                        }</span>
                default:<span class="cov0" title="0">
                        pendingInitialization++</span>
                }
        }

        // counters for restartable init and regular containers
        <span class="cov8" title="1">unknown := 0
        running := 0
        waiting := 0
        stopped := 0
        succeeded := 0

        // restartable init containers
        for _, container := range spec.InitContainers </span><span class="cov8" title="1">{
                if !kubetypes.IsRestartableInitContainer(&amp;container) </span><span class="cov8" title="1">{
                        // Skip the regular init containers, as they have been handled above.
                        continue</span>
                }
                <span class="cov0" title="0">containerStatus, ok := podutil.GetContainerStatus(info, container.Name)
                if !ok </span><span class="cov0" title="0">{
                        unknown++
                        continue</span>
                }

                <span class="cov0" title="0">switch </span>{
                case containerStatus.State.Running != nil:<span class="cov0" title="0">
                        if containerStatus.Started == nil || !*containerStatus.Started </span><span class="cov0" title="0">{
                                pendingInitialization++
                        }</span>
                        <span class="cov0" title="0">running++</span>
                case containerStatus.State.Terminated != nil:<span class="cov0" title="0"></span>
                        // Do nothing here, as terminated restartable init containers are not
                        // taken into account for the pod phase.
                case containerStatus.State.Waiting != nil:<span class="cov0" title="0">
                        if containerStatus.LastTerminationState.Terminated != nil </span>{<span class="cov0" title="0">
                                // Do nothing here, as terminated restartable init containers are not
                                // taken into account for the pod phase.
                        }</span> else<span class="cov0" title="0"> {
                                pendingInitialization++
                                waiting++
                        }</span>
                default:<span class="cov0" title="0">
                        pendingInitialization++
                        unknown++</span>
                }
        }

        <span class="cov8" title="1">for _, container := range spec.Containers </span><span class="cov8" title="1">{
                containerStatus, ok := podutil.GetContainerStatus(info, container.Name)
                if !ok </span><span class="cov0" title="0">{
                        unknown++
                        continue</span>
                }

                <span class="cov8" title="1">switch </span>{
                case containerStatus.State.Running != nil:<span class="cov8" title="1">
                        running++</span>
                case containerStatus.State.Terminated != nil:<span class="cov8" title="1">
                        stopped++
                        if containerStatus.State.Terminated.ExitCode == 0 </span><span class="cov8" title="1">{
                                succeeded++
                        }</span>
                case containerStatus.State.Waiting != nil:<span class="cov8" title="1">
                        if containerStatus.LastTerminationState.Terminated != nil </span><span class="cov8" title="1">{
                                stopped++
                        }</span> else<span class="cov8" title="1"> {
                                waiting++
                        }</span>
                default:<span class="cov0" title="0">
                        unknown++</span>
                }
        }

        <span class="cov8" title="1">if failedInitialization &gt; 0 &amp;&amp; spec.RestartPolicy == v1.RestartPolicyNever </span><span class="cov8" title="1">{
                return v1.PodFailed
        }</span>

        <span class="cov8" title="1">switch </span>{
        case pendingInitialization &gt; 0 &amp;&amp;
                // This is needed to handle the case where the pod has been initialized but
                // the restartable init containers are restarting and the pod should not be
                // placed back into v1.PodPending since the regular containers have run.
                !kubecontainer.HasAnyRegularContainerStarted(&amp;spec, info):<span class="cov8" title="1">
                fallthrough</span>
        case waiting &gt; 0:<span class="cov8" title="1">
                klog.V(5).InfoS("Pod waiting &gt; 0, pending")
                // One or more containers has not been started
                return v1.PodPending</span>
        case running &gt; 0 &amp;&amp; unknown == 0:<span class="cov8" title="1">
                // All containers have been started, and at least
                // one container is running
                return v1.PodRunning</span>
        case running == 0 &amp;&amp; stopped &gt; 0 &amp;&amp; unknown == 0:<span class="cov8" title="1">
                // The pod is terminal so its containers won't be restarted regardless
                // of the restart policy.
                if podIsTerminal </span><span class="cov0" title="0">{
                        // TODO(#116484): Also assign terminal phase to static pods.
                        if !kubetypes.IsStaticPod(pod) </span><span class="cov0" title="0">{
                                // All regular containers are terminated in success and all restartable
                                // init containers are stopped.
                                if stopped == succeeded </span><span class="cov0" title="0">{
                                        return v1.PodSucceeded
                                }</span>
                                // There is at least one failure
                                <span class="cov0" title="0">return v1.PodFailed</span>
                        }
                }
                // All containers are terminated
                <span class="cov8" title="1">if spec.RestartPolicy == v1.RestartPolicyAlways </span><span class="cov8" title="1">{
                        // All containers are in the process of restarting
                        return v1.PodRunning
                }</span>
                <span class="cov8" title="1">if stopped == succeeded </span><span class="cov0" title="0">{
                        // RestartPolicy is not Always, all containers are terminated in success
                        // and all restartable init containers are stopped.
                        return v1.PodSucceeded
                }</span>
                <span class="cov8" title="1">if spec.RestartPolicy == v1.RestartPolicyNever </span><span class="cov8" title="1">{
                        // RestartPolicy is Never, and all containers are
                        // terminated with at least one in failure
                        return v1.PodFailed
                }</span>
                // RestartPolicy is OnFailure, and at least one in failure
                // and in the process of restarting
                <span class="cov8" title="1">return v1.PodRunning</span>
        default:<span class="cov8" title="1">
                klog.V(5).InfoS("Pod default case, pending")
                return v1.PodPending</span>
        }
}

func deleteCustomResourceFromResourceRequirements(target *v1.ResourceRequirements) <span class="cov8" title="1">{
        for resource := range target.Limits </span><span class="cov0" title="0">{
                if resource != v1.ResourceCPU &amp;&amp; resource != v1.ResourceMemory &amp;&amp; resource != v1.ResourceEphemeralStorage </span><span class="cov0" title="0">{
                        delete(target.Limits, resource)
                }</span>
        }
        <span class="cov8" title="1">for resource := range target.Requests </span><span class="cov8" title="1">{
                if resource != v1.ResourceCPU &amp;&amp; resource != v1.ResourceMemory &amp;&amp; resource != v1.ResourceEphemeralStorage </span><span class="cov0" title="0">{
                        delete(target.Requests, resource)
                }</span>
        }
}

func (kl *Kubelet) determinePodResizeStatus(pod *v1.Pod, podStatus *v1.PodStatus) v1.PodResizeStatus <span class="cov8" title="1">{
        var podResizeStatus v1.PodResizeStatus
        specStatusDiffer := false
        for _, c := range pod.Spec.Containers </span><span class="cov8" title="1">{
                if cs, ok := podutil.GetContainerStatus(podStatus.ContainerStatuses, c.Name); ok </span><span class="cov8" title="1">{
                        cResourceCopy := c.Resources.DeepCopy()
                        // for both requests and limits, we only compare the cpu, memory and ephemeralstorage
                        // which are included in convertToAPIContainerStatuses
                        deleteCustomResourceFromResourceRequirements(cResourceCopy)
                        csResourceCopy := cs.Resources.DeepCopy()
                        if csResourceCopy != nil &amp;&amp; !cmp.Equal(*cResourceCopy, *csResourceCopy) </span><span class="cov0" title="0">{
                                specStatusDiffer = true
                                break</span>
                        }
                }
        }
        <span class="cov8" title="1">if !specStatusDiffer </span><span class="cov8" title="1">{
                // Clear last resize state from checkpoint
                if err := kl.statusManager.SetPodResizeStatus(pod.UID, ""); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "SetPodResizeStatus failed", "pod", pod.Name)
                }</span>
        } else<span class="cov0" title="0"> {
                if resizeStatus, found := kl.statusManager.GetPodResizeStatus(string(pod.UID)); found </span><span class="cov0" title="0">{
                        podResizeStatus = resizeStatus
                }</span>
        }
        <span class="cov8" title="1">return podResizeStatus</span>
}

// generateAPIPodStatus creates the final API pod status for a pod, given the
// internal pod status. This method should only be called from within sync*Pod methods.
func (kl *Kubelet) generateAPIPodStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus, podIsTerminal bool) v1.PodStatus <span class="cov8" title="1">{
        klog.V(3).InfoS("Generating pod status", "podIsTerminal", podIsTerminal, "pod", klog.KObj(pod))
        // use the previous pod status, or the api status, as the basis for this pod
        oldPodStatus, found := kl.statusManager.GetPodStatus(pod.UID)
        if !found </span><span class="cov8" title="1">{
                oldPodStatus = pod.Status
        }</span>
        <span class="cov8" title="1">s := kl.convertStatusToAPIStatus(pod, podStatus, oldPodStatus)
        if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) </span><span class="cov8" title="1">{
                s.Resize = kl.determinePodResizeStatus(pod, s)
        }</span>
        // calculate the next phase and preserve reason
        <span class="cov8" title="1">allStatus := append(append([]v1.ContainerStatus{}, s.ContainerStatuses...), s.InitContainerStatuses...)
        s.Phase = getPhase(pod, allStatus, podIsTerminal)
        klog.V(4).InfoS("Got phase for pod", "pod", klog.KObj(pod), "oldPhase", oldPodStatus.Phase, "phase", s.Phase)

        // Perform a three-way merge between the statuses from the status manager,
        // runtime, and generated status to ensure terminal status is correctly set.
        if s.Phase != v1.PodFailed &amp;&amp; s.Phase != v1.PodSucceeded </span><span class="cov8" title="1">{
                switch </span>{
                case oldPodStatus.Phase == v1.PodFailed || oldPodStatus.Phase == v1.PodSucceeded:<span class="cov8" title="1">
                        klog.V(4).InfoS("Status manager phase was terminal, updating phase to match", "pod", klog.KObj(pod), "phase", oldPodStatus.Phase)
                        s.Phase = oldPodStatus.Phase</span>
                case pod.Status.Phase == v1.PodFailed || pod.Status.Phase == v1.PodSucceeded:<span class="cov0" title="0">
                        klog.V(4).InfoS("API phase was terminal, updating phase to match", "pod", klog.KObj(pod), "phase", pod.Status.Phase)
                        s.Phase = pod.Status.Phase</span>
                }
        }

        <span class="cov8" title="1">if s.Phase == oldPodStatus.Phase </span><span class="cov8" title="1">{
                // preserve the reason and message which is associated with the phase
                s.Reason = oldPodStatus.Reason
                s.Message = oldPodStatus.Message
                if len(s.Reason) == 0 </span><span class="cov8" title="1">{
                        s.Reason = pod.Status.Reason
                }</span>
                <span class="cov8" title="1">if len(s.Message) == 0 </span><span class="cov8" title="1">{
                        s.Message = pod.Status.Message
                }</span>
        }

        // check if an internal module has requested the pod is evicted and override the reason and message
        <span class="cov8" title="1">for _, podSyncHandler := range kl.PodSyncHandlers </span><span class="cov8" title="1">{
                if result := podSyncHandler.ShouldEvict(pod); result.Evict </span><span class="cov8" title="1">{
                        s.Phase = v1.PodFailed
                        s.Reason = result.Reason
                        s.Message = result.Message
                        break</span>
                }
        }

        // pods are not allowed to transition out of terminal phases
        <span class="cov8" title="1">if pod.Status.Phase == v1.PodFailed || pod.Status.Phase == v1.PodSucceeded </span><span class="cov0" title="0">{
                // API server shows terminal phase; transitions are not allowed
                if s.Phase != pod.Status.Phase </span><span class="cov0" title="0">{
                        klog.ErrorS(nil, "Pod attempted illegal phase transition", "pod", klog.KObj(pod), "originalStatusPhase", pod.Status.Phase, "apiStatusPhase", s.Phase, "apiStatus", s)
                        // Force back to phase from the API server
                        s.Phase = pod.Status.Phase
                }</span>
        }

        // ensure the probe managers have up to date status for containers
        <span class="cov8" title="1">kl.probeManager.UpdatePodStatus(pod, s)

        // preserve all conditions not owned by the kubelet
        s.Conditions = make([]v1.PodCondition, 0, len(pod.Status.Conditions)+1)
        for _, c := range pod.Status.Conditions </span><span class="cov0" title="0">{
                if !kubetypes.PodConditionByKubelet(c.Type) </span><span class="cov0" title="0">{
                        s.Conditions = append(s.Conditions, c)
                }</span>
        }

        <span class="cov8" title="1">if utilfeature.DefaultFeatureGate.Enabled(features.PodDisruptionConditions) </span><span class="cov8" title="1">{
                // copy over the pod disruption conditions from state which is already
                // updated during the eviciton (due to either node resource pressure or
                // node graceful shutdown). We do not re-generate the conditions based
                // on the container statuses as they are added based on one-time events.
                cType := v1.DisruptionTarget
                if _, condition := podutil.GetPodConditionFromList(oldPodStatus.Conditions, cType); condition != nil </span><span class="cov0" title="0">{
                        s.Conditions = utilpod.ReplaceOrAppendPodCondition(s.Conditions, condition)
                }</span>
        }

        // set all Kubelet-owned conditions
        <span class="cov8" title="1">if utilfeature.DefaultFeatureGate.Enabled(features.PodReadyToStartContainersCondition) </span><span class="cov8" title="1">{
                s.Conditions = append(s.Conditions, status.GeneratePodReadyToStartContainersCondition(pod, podStatus))
        }</span>
        <span class="cov8" title="1">allContainerStatuses := append(s.InitContainerStatuses, s.ContainerStatuses...)
        s.Conditions = append(s.Conditions, status.GeneratePodInitializedCondition(&amp;pod.Spec, allContainerStatuses, s.Phase))
        s.Conditions = append(s.Conditions, status.GeneratePodReadyCondition(&amp;pod.Spec, s.Conditions, allContainerStatuses, s.Phase))
        s.Conditions = append(s.Conditions, status.GenerateContainersReadyCondition(&amp;pod.Spec, allContainerStatuses, s.Phase))
        s.Conditions = append(s.Conditions, v1.PodCondition{
                Type:   v1.PodScheduled,
                Status: v1.ConditionTrue,
        })
        // set HostIP/HostIPs and initialize PodIP/PodIPs for host network pods
        if kl.kubeClient != nil </span><span class="cov8" title="1">{
                hostIPs, err := kl.getHostIPsAnyWay()
                if err != nil </span><span class="cov8" title="1">{
                        klog.V(4).InfoS("Cannot get host IPs", "err", err)
                }</span> else<span class="cov8" title="1"> {
                        if s.HostIP != "" </span><span class="cov0" title="0">{
                                if utilnet.IPFamilyOfString(s.HostIP) != utilnet.IPFamilyOf(hostIPs[0]) </span><span class="cov0" title="0">{
                                        kl.recorder.Eventf(pod, v1.EventTypeWarning, "HostIPsIPFamilyMismatch",
                                                "Kubelet detected an IPv%s node IP (%s), but the cloud provider selected an IPv%s node IP (%s); pass an explicit `--node-ip` to kubelet to fix this.",
                                                utilnet.IPFamilyOfString(s.HostIP), s.HostIP, utilnet.IPFamilyOf(hostIPs[0]), hostIPs[0].String())
                                }</span>
                        }
                        <span class="cov8" title="1">s.HostIP = hostIPs[0].String()
                        if utilfeature.DefaultFeatureGate.Enabled(features.PodHostIPs) </span><span class="cov8" title="1">{
                                s.HostIPs = []v1.HostIP{{IP: s.HostIP}}
                                if len(hostIPs) == 2 </span><span class="cov8" title="1">{
                                        s.HostIPs = append(s.HostIPs, v1.HostIP{IP: hostIPs[1].String()})
                                }</span>
                        }

                        // HostNetwork Pods inherit the node IPs as PodIPs. They are immutable once set,
                        // other than that if the node becomes dual-stack, we add the secondary IP.
                        <span class="cov8" title="1">if kubecontainer.IsHostNetworkPod(pod) </span><span class="cov8" title="1">{
                                // Primary IP is not set
                                if s.PodIP == "" </span><span class="cov8" title="1">{
                                        s.PodIP = hostIPs[0].String()
                                        s.PodIPs = []v1.PodIP{{IP: s.PodIP}}
                                }</span>
                                // Secondary IP is not set #105320
                                <span class="cov8" title="1">if len(hostIPs) == 2 &amp;&amp; len(s.PodIPs) == 1 </span><span class="cov8" title="1">{
                                        if utilnet.IPFamilyOfString(s.PodIPs[0].IP) != utilnet.IPFamilyOf(hostIPs[1]) </span><span class="cov8" title="1">{
                                                s.PodIPs = append(s.PodIPs, v1.PodIP{IP: hostIPs[1].String()})
                                        }</span>
                                }
                        }
                }
        }

        <span class="cov8" title="1">return *s</span>
}

func (kl *Kubelet) GenerateAPIPodStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus, podIsTerminal bool) v1.PodStatus <span class="cov0" title="0">{

        return kl.generateAPIPodStatus(pod, podStatus, podIsTerminal)
}</span>

// sortPodIPs return the PodIPs sorted and truncated by the cluster IP family preference.
// The runtime pod status may have an arbitrary number of IPs, in an arbitrary order.
// PodIPs are obtained by: func (m *kubeGenericRuntimeManager) determinePodSandboxIPs()
// Pick out the first returned IP of the same IP family as the node IP
// first, followed by the first IP of the opposite IP family (if any)
// and use them for the Pod.Status.PodIPs and the Downward API environment variables
func (kl *Kubelet) sortPodIPs(podIPs []string) []string <span class="cov8" title="1">{
        ips := make([]string, 0, 2)
        var validPrimaryIP, validSecondaryIP func(ip string) bool
        if len(kl.nodeIPs) == 0 || utilnet.IsIPv4(kl.nodeIPs[0]) </span><span class="cov8" title="1">{
                validPrimaryIP = utilnet.IsIPv4String
                validSecondaryIP = utilnet.IsIPv6String
        }</span> else<span class="cov0" title="0"> {
                validPrimaryIP = utilnet.IsIPv6String
                validSecondaryIP = utilnet.IsIPv4String
        }</span>
        <span class="cov8" title="1">for _, ip := range podIPs </span><span class="cov0" title="0">{
                if validPrimaryIP(ip) </span><span class="cov0" title="0">{
                        ips = append(ips, ip)
                        break</span>
                }
        }
        <span class="cov8" title="1">for _, ip := range podIPs </span><span class="cov0" title="0">{
                if validSecondaryIP(ip) </span><span class="cov0" title="0">{
                        ips = append(ips, ip)
                        break</span>
                }
        }
        <span class="cov8" title="1">return ips</span>
}

// convertStatusToAPIStatus initialize an api PodStatus for the given pod from
// the given internal pod status and the previous state of the pod from the API.
// It is purely transformative and does not alter the kubelet state at all.
func (kl *Kubelet) convertStatusToAPIStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus, oldPodStatus v1.PodStatus) *v1.PodStatus <span class="cov8" title="1">{
        var apiPodStatus v1.PodStatus

        // copy pod status IPs to avoid race conditions with PodStatus #102806
        podIPs := make([]string, len(podStatus.IPs))
        copy(podIPs, podStatus.IPs)

        // make podIPs order match node IP family preference #97979
        podIPs = kl.sortPodIPs(podIPs)
        for _, ip := range podIPs </span><span class="cov0" title="0">{
                apiPodStatus.PodIPs = append(apiPodStatus.PodIPs, v1.PodIP{IP: ip})
        }</span>
        <span class="cov8" title="1">if len(apiPodStatus.PodIPs) &gt; 0 </span><span class="cov0" title="0">{
                apiPodStatus.PodIP = apiPodStatus.PodIPs[0].IP
        }</span>

        // set status for Pods created on versions of kube older than 1.6
        <span class="cov8" title="1">apiPodStatus.QOSClass = v1qos.GetPodQOS(pod)

        apiPodStatus.ContainerStatuses = kl.convertToAPIContainerStatuses(
                pod, podStatus,
                oldPodStatus.ContainerStatuses,
                pod.Spec.Containers,
                len(pod.Spec.InitContainers) &gt; 0,
                false,
        )
        apiPodStatus.InitContainerStatuses = kl.convertToAPIContainerStatuses(
                pod, podStatus,
                oldPodStatus.InitContainerStatuses,
                pod.Spec.InitContainers,
                len(pod.Spec.InitContainers) &gt; 0,
                true,
        )
        var ecSpecs []v1.Container
        for i := range pod.Spec.EphemeralContainers </span><span class="cov0" title="0">{
                ecSpecs = append(ecSpecs, v1.Container(pod.Spec.EphemeralContainers[i].EphemeralContainerCommon))
        }</span>

        // #80875: By now we've iterated podStatus 3 times. We could refactor this to make a single
        // pass through podStatus.ContainerStatuses
        <span class="cov8" title="1">apiPodStatus.EphemeralContainerStatuses = kl.convertToAPIContainerStatuses(
                pod, podStatus,
                oldPodStatus.EphemeralContainerStatuses,
                ecSpecs,
                len(pod.Spec.InitContainers) &gt; 0,
                false,
        )

        return &amp;apiPodStatus</span>
}

// convertToAPIContainerStatuses converts the given internal container
// statuses into API container statuses.
func (kl *Kubelet) convertToAPIContainerStatuses(pod *v1.Pod, podStatus *kubecontainer.PodStatus, previousStatus []v1.ContainerStatus, containers []v1.Container, hasInitContainers, isInitContainer bool) []v1.ContainerStatus <span class="cov8" title="1">{
        convertContainerStatus := func(cs *kubecontainer.Status, oldStatus *v1.ContainerStatus) *v1.ContainerStatus </span><span class="cov8" title="1">{
                cid := cs.ID.String()
                status := &amp;v1.ContainerStatus{
                        Name:         cs.Name,
                        RestartCount: int32(cs.RestartCount),
                        Image:        cs.Image,
                        ImageID:      cs.ImageID,
                        ContainerID:  cid,
                }
                switch </span>{
                case cs.State == kubecontainer.ContainerStateRunning:<span class="cov8" title="1">
                        status.State.Running = &amp;v1.ContainerStateRunning{StartedAt: metav1.NewTime(cs.StartedAt)}</span>
                case cs.State == kubecontainer.ContainerStateCreated:<span class="cov0" title="0">
                        // containers that are created but not running are "waiting to be running"
                        status.State.Waiting = &amp;v1.ContainerStateWaiting{}</span>
                case cs.State == kubecontainer.ContainerStateExited:<span class="cov8" title="1">
                        status.State.Terminated = &amp;v1.ContainerStateTerminated{
                                ExitCode:    int32(cs.ExitCode),
                                Reason:      cs.Reason,
                                Message:     cs.Message,
                                StartedAt:   metav1.NewTime(cs.StartedAt),
                                FinishedAt:  metav1.NewTime(cs.FinishedAt),
                                ContainerID: cid,
                        }</span>

                case cs.State == kubecontainer.ContainerStateUnknown &amp;&amp;
                        oldStatus != nil &amp;&amp; // we have an old status
                        oldStatus.State.Running != nil:<span class="cov8" title="1"> // our previous status was running
                        // if this happens, then we know that this container was previously running and isn't anymore (assuming the CRI isn't failing to return running containers).
                        // you can imagine this happening in cases where a container failed and the kubelet didn't ask about it in time to see the result.
                        // in this case, the container should not to into waiting state immediately because that can make cases like runonce pods actually run
                        // twice. "container never ran" is different than "container ran and failed".  This is handled differently in the kubelet
                        // and it is handled differently in higher order logic like crashloop detection and handling
                        status.State.Terminated = &amp;v1.ContainerStateTerminated{
                                Reason:   "ContainerStatusUnknown",
                                Message:  "The container could not be located when the pod was terminated",
                                ExitCode: 137, // this code indicates an error
                        }
                        // the restart count normally comes from the CRI (see near the top of this method), but since this is being added explicitly
                        // for the case where the CRI did not return a status, we need to manually increment the restart count to be accurate.
                        status.RestartCount = oldStatus.RestartCount + 1</span>

                default:<span class="cov8" title="1">
                        // this collapses any unknown state to container waiting.  If any container is waiting, then the pod status moves to pending even if it is running.
                        // if I'm reading this correctly, then any failure to read status on any container results in the entire pod going pending even if the containers
                        // are actually running.
                        // see https://github.com/kubernetes/kubernetes/blob/5d1b3e26af73dde33ecb6a3e69fb5876ceab192f/pkg/kubelet/kuberuntime/kuberuntime_container.go#L497 to
                        // https://github.com/kubernetes/kubernetes/blob/8976e3620f8963e72084971d9d4decbd026bf49f/pkg/kubelet/kuberuntime/helpers.go#L58-L71
                        // and interpreted here https://github.com/kubernetes/kubernetes/blob/b27e78f590a0d43e4a23ca3b2bf1739ca4c6e109/pkg/kubelet/kubelet_pods.go#L1434-L1439
                        status.State.Waiting = &amp;v1.ContainerStateWaiting{}</span>
                }
                <span class="cov8" title="1">return status</span>
        }

        <span class="cov8" title="1">convertContainerStatusResources := func(cName string, status *v1.ContainerStatus, cStatus *kubecontainer.Status, oldStatuses map[string]v1.ContainerStatus) *v1.ResourceRequirements </span><span class="cov0" title="0">{
                var requests, limits v1.ResourceList
                // oldStatus should always exist if container is running
                oldStatus, oldStatusFound := oldStatuses[cName]
                // Initialize limits/requests from container's spec upon transition to Running state
                // For cpu &amp; memory, values queried from runtime via CRI always supercedes spec values
                // For ephemeral-storage, a running container's status.limit/request equals spec.limit/request
                determineResource := func(rName v1.ResourceName, v1ContainerResource, oldStatusResource, resource v1.ResourceList) </span><span class="cov0" title="0">{
                        if oldStatusFound </span><span class="cov0" title="0">{
                                if oldStatus.State.Running == nil || status.ContainerID != oldStatus.ContainerID </span><span class="cov0" title="0">{
                                        if r, exists := v1ContainerResource[rName]; exists </span><span class="cov0" title="0">{
                                                resource[rName] = r.DeepCopy()
                                        }</span>
                                } else<span class="cov0" title="0"> {
                                        if oldStatusResource != nil </span><span class="cov0" title="0">{
                                                if r, exists := oldStatusResource[rName]; exists </span><span class="cov0" title="0">{
                                                        resource[rName] = r.DeepCopy()
                                                }</span>
                                        }
                                }
                        }
                }
                <span class="cov0" title="0">container := kubecontainer.GetContainerSpec(pod, cName)
                // AllocatedResources values come from checkpoint. It is the source-of-truth.
                found := false
                status.AllocatedResources, found = kl.statusManager.GetContainerResourceAllocation(string(pod.UID), cName)
                if !(container.Resources.Requests == nil &amp;&amp; container.Resources.Limits == nil) &amp;&amp; !found </span><span class="cov0" title="0">{
                        // Log error and fallback to AllocatedResources in oldStatus if it exists
                        klog.ErrorS(nil, "resource allocation not found in checkpoint store", "pod", pod.Name, "container", cName)
                        if oldStatusFound </span><span class="cov0" title="0">{
                                status.AllocatedResources = oldStatus.AllocatedResources
                        }</span>
                }
                <span class="cov0" title="0">if oldStatus.Resources == nil </span><span class="cov0" title="0">{
                        oldStatus.Resources = &amp;v1.ResourceRequirements{}
                }</span>
                // Convert Limits
                <span class="cov0" title="0">if container.Resources.Limits != nil </span><span class="cov0" title="0">{
                        limits = make(v1.ResourceList)
                        if cStatus.Resources != nil &amp;&amp; cStatus.Resources.CPULimit != nil </span><span class="cov0" title="0">{
                                limits[v1.ResourceCPU] = cStatus.Resources.CPULimit.DeepCopy()
                        }</span> else<span class="cov0" title="0"> {
                                determineResource(v1.ResourceCPU, container.Resources.Limits, oldStatus.Resources.Limits, limits)
                        }</span>
                        <span class="cov0" title="0">if cStatus.Resources != nil &amp;&amp; cStatus.Resources.MemoryLimit != nil </span><span class="cov0" title="0">{
                                limits[v1.ResourceMemory] = cStatus.Resources.MemoryLimit.DeepCopy()
                        }</span> else<span class="cov0" title="0"> {
                                determineResource(v1.ResourceMemory, container.Resources.Limits, oldStatus.Resources.Limits, limits)
                        }</span>
                        <span class="cov0" title="0">if ephemeralStorage, found := container.Resources.Limits[v1.ResourceEphemeralStorage]; found </span><span class="cov0" title="0">{
                                limits[v1.ResourceEphemeralStorage] = ephemeralStorage.DeepCopy()
                        }</span>
                }
                // Convert Requests
                <span class="cov0" title="0">if status.AllocatedResources != nil </span><span class="cov0" title="0">{
                        requests = make(v1.ResourceList)
                        if cStatus.Resources != nil &amp;&amp; cStatus.Resources.CPURequest != nil </span><span class="cov0" title="0">{
                                requests[v1.ResourceCPU] = cStatus.Resources.CPURequest.DeepCopy()
                        }</span> else<span class="cov0" title="0"> {
                                determineResource(v1.ResourceCPU, status.AllocatedResources, oldStatus.Resources.Requests, requests)
                        }</span>
                        <span class="cov0" title="0">if memory, found := status.AllocatedResources[v1.ResourceMemory]; found </span><span class="cov0" title="0">{
                                requests[v1.ResourceMemory] = memory.DeepCopy()
                        }</span>
                        <span class="cov0" title="0">if ephemeralStorage, found := status.AllocatedResources[v1.ResourceEphemeralStorage]; found </span><span class="cov0" title="0">{
                                requests[v1.ResourceEphemeralStorage] = ephemeralStorage.DeepCopy()
                        }</span>
                }
                //TODO(vinaykul,derekwaynecarr,InPlacePodVerticalScaling): Update this to include extended resources in
                // addition to CPU, memory, ephemeral storage. Add test case for extended resources.
                <span class="cov0" title="0">resources := &amp;v1.ResourceRequirements{
                        Limits:   limits,
                        Requests: requests,
                }
                return resources</span>
        }

        // Fetch old containers statuses from old pod status.
        <span class="cov8" title="1">oldStatuses := make(map[string]v1.ContainerStatus, len(containers))
        for _, status := range previousStatus </span><span class="cov8" title="1">{
                oldStatuses[status.Name] = status
        }</span>

        // Set all container statuses to default waiting state
        <span class="cov8" title="1">statuses := make(map[string]*v1.ContainerStatus, len(containers))
        defaultWaitingState := v1.ContainerState{Waiting: &amp;v1.ContainerStateWaiting{Reason: ContainerCreating}}
        if hasInitContainers </span><span class="cov8" title="1">{
                defaultWaitingState = v1.ContainerState{Waiting: &amp;v1.ContainerStateWaiting{Reason: PodInitializing}}
        }</span>

        <span class="cov8" title="1">for _, container := range containers </span><span class="cov8" title="1">{
                status := &amp;v1.ContainerStatus{
                        Name:  container.Name,
                        Image: container.Image,
                        State: defaultWaitingState,
                }
                oldStatus, found := oldStatuses[container.Name]
                if found </span><span class="cov8" title="1">{
                        if oldStatus.State.Terminated != nil </span><span class="cov0" title="0">{
                                status = &amp;oldStatus
                        }</span> else<span class="cov8" title="1"> {
                                // Apply some values from the old statuses as the default values.
                                status.RestartCount = oldStatus.RestartCount
                                status.LastTerminationState = oldStatus.LastTerminationState
                        }</span>
                }
                <span class="cov8" title="1">statuses[container.Name] = status</span>
        }

        <span class="cov8" title="1">for _, container := range containers </span><span class="cov8" title="1">{
                found := false
                for _, cStatus := range podStatus.ContainerStatuses </span><span class="cov8" title="1">{
                        if container.Name == cStatus.Name </span><span class="cov8" title="1">{
                                found = true
                                break</span>
                        }
                }
                <span class="cov8" title="1">if found </span><span class="cov8" title="1">{
                        continue</span>
                }
                // if no container is found, then assuming it should be waiting seems plausible, but the status code requires
                // that a previous termination be present.  If we're offline long enough or something removed the container, then
                // the previous termination may not be present.  This next code block ensures that if the container was previously running
                // then when that container status disappears, we can infer that it terminated even if we don't know the status code.
                // By setting the lasttermination state we are able to leave the container status waiting and present more accurate
                // data via the API.

                <span class="cov8" title="1">oldStatus, ok := oldStatuses[container.Name]
                if !ok </span><span class="cov8" title="1">{
                        continue</span>
                }
                <span class="cov8" title="1">if oldStatus.State.Terminated != nil </span><span class="cov0" title="0">{
                        // if the old container status was terminated, the lasttermination status is correct
                        continue</span>
                }
                <span class="cov8" title="1">if oldStatus.State.Running == nil </span><span class="cov8" title="1">{
                        // if the old container status isn't running, then waiting is an appropriate status and we have nothing to do
                        continue</span>
                }

                // If we're here, we know the pod was previously running, but doesn't have a terminated status. We will check now to
                // see if it's in a pending state.
                <span class="cov8" title="1">status := statuses[container.Name]
                // If the status we're about to write indicates the default, the Waiting status will force this pod back into Pending.
                // That isn't true, we know the pod was previously running.
                isDefaultWaitingStatus := status.State.Waiting != nil &amp;&amp; status.State.Waiting.Reason == ContainerCreating
                if hasInitContainers </span><span class="cov8" title="1">{
                        isDefaultWaitingStatus = status.State.Waiting != nil &amp;&amp; status.State.Waiting.Reason == PodInitializing
                }</span>
                <span class="cov8" title="1">if !isDefaultWaitingStatus </span><span class="cov0" title="0">{
                        // the status was written, don't override
                        continue</span>
                }
                <span class="cov8" title="1">if status.LastTerminationState.Terminated != nil </span><span class="cov0" title="0">{
                        // if we already have a termination state, nothing to do
                        continue</span>
                }

                // setting this value ensures that we show as stopped here, not as waiting:
                // https://github.com/kubernetes/kubernetes/blob/90c9f7b3e198e82a756a68ffeac978a00d606e55/pkg/kubelet/kubelet_pods.go#L1440-L1445
                // This prevents the pod from becoming pending
                <span class="cov8" title="1">status.LastTerminationState.Terminated = &amp;v1.ContainerStateTerminated{
                        Reason:   "ContainerStatusUnknown",
                        Message:  "The container could not be located when the pod was deleted.  The container used to be Running",
                        ExitCode: 137,
                }

                // If the pod was not deleted, then it's been restarted. Increment restart count.
                if pod.DeletionTimestamp == nil </span><span class="cov8" title="1">{
                        status.RestartCount += 1
                }</span>

                <span class="cov8" title="1">statuses[container.Name] = status</span>
        }

        // Copy the slice before sorting it
        <span class="cov8" title="1">containerStatusesCopy := make([]*kubecontainer.Status, len(podStatus.ContainerStatuses))
        copy(containerStatusesCopy, podStatus.ContainerStatuses)

        // Make the latest container status comes first.
        sort.Sort(sort.Reverse(kubecontainer.SortContainerStatusesByCreationTime(containerStatusesCopy)))
        // Set container statuses according to the statuses seen in pod status
        containerSeen := map[string]int{}
        for _, cStatus := range containerStatusesCopy </span><span class="cov8" title="1">{
                cName := cStatus.Name
                if _, ok := statuses[cName]; !ok </span><span class="cov8" title="1">{
                        // This would also ignore the infra container.
                        continue</span>
                }
                <span class="cov8" title="1">if containerSeen[cName] &gt;= 2 </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov8" title="1">var oldStatusPtr *v1.ContainerStatus
                if oldStatus, ok := oldStatuses[cName]; ok </span><span class="cov8" title="1">{
                        oldStatusPtr = &amp;oldStatus
                }</span>
                <span class="cov8" title="1">status := convertContainerStatus(cStatus, oldStatusPtr)
                if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) </span><span class="cov0" title="0">{
                        if status.State.Running != nil </span><span class="cov0" title="0">{
                                status.Resources = convertContainerStatusResources(cName, status, cStatus, oldStatuses)
                        }</span>
                }
                <span class="cov8" title="1">if containerSeen[cName] == 0 </span><span class="cov8" title="1">{
                        statuses[cName] = status
                }</span> else<span class="cov8" title="1"> {
                        statuses[cName].LastTerminationState = status.State
                }</span>
                <span class="cov8" title="1">containerSeen[cName] = containerSeen[cName] + 1</span>
        }

        // Handle the containers failed to be started, which should be in Waiting state.
        <span class="cov8" title="1">for _, container := range containers </span><span class="cov8" title="1">{
                if isInitContainer </span><span class="cov8" title="1">{
                        // If the init container is terminated with exit code 0, it won't be restarted.
                        // TODO(random-liu): Handle this in a cleaner way.
                        s := podStatus.FindContainerStatusByName(container.Name)
                        if s != nil &amp;&amp; s.State == kubecontainer.ContainerStateExited &amp;&amp; s.ExitCode == 0 </span><span class="cov8" title="1">{
                                continue</span>
                        }
                }
                // If a container should be restarted in next syncpod, it is *Waiting*.
                <span class="cov8" title="1">if !kubecontainer.ShouldContainerBeRestarted(&amp;container, pod, podStatus) </span><span class="cov8" title="1">{
                        continue</span>
                }
                <span class="cov8" title="1">status := statuses[container.Name]
                reason, ok := kl.reasonCache.Get(pod.UID, container.Name)
                if !ok </span><span class="cov8" title="1">{
                        // In fact, we could also apply Waiting state here, but it is less informative,
                        // and the container will be restarted soon, so we prefer the original state here.
                        // Note that with the current implementation of ShouldContainerBeRestarted the original state here
                        // could be:
                        //   * Waiting: There is no associated historical container and start failure reason record.
                        //   * Terminated: The container is terminated.
                        continue</span>
                }
                <span class="cov8" title="1">if status.State.Terminated != nil </span><span class="cov8" title="1">{
                        status.LastTerminationState = status.State
                }</span>
                <span class="cov8" title="1">status.State = v1.ContainerState{
                        Waiting: &amp;v1.ContainerStateWaiting{
                                Reason:  reason.Err.Error(),
                                Message: reason.Message,
                        },
                }
                statuses[container.Name] = status</span>
        }

        // Sort the container statuses since clients of this interface expect the list
        // of containers in a pod has a deterministic order.
        <span class="cov8" title="1">if isInitContainer </span><span class="cov8" title="1">{
                return kubetypes.SortStatusesOfInitContainers(pod, statuses)
        }</span>
        <span class="cov8" title="1">containerStatuses := make([]v1.ContainerStatus, 0, len(statuses))
        for _, status := range statuses </span><span class="cov8" title="1">{
                containerStatuses = append(containerStatuses, *status)
        }</span>

        <span class="cov8" title="1">sort.Sort(kubetypes.SortedContainerStatuses(containerStatuses))
        return containerStatuses</span>
}

// ServeLogs returns logs of current machine.
func (kl *Kubelet) ServeLogs(w http.ResponseWriter, req *http.Request) <span class="cov0" title="0">{
        // TODO: allowlist logs we are willing to serve
        kl.logServer.ServeHTTP(w, req)
}</span>

// findContainer finds and returns the container with the given pod ID, full name, and container name.
// It returns nil if not found.
func (kl *Kubelet) findContainer(ctx context.Context, podFullName string, podUID types.UID, containerName string) (*kubecontainer.Container, error) <span class="cov8" title="1">{
        pods, err := kl.containerRuntime.GetPods(ctx, false)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        // Resolve and type convert back again.
        // We need the static pod UID but the kubecontainer API works with types.UID.
        <span class="cov8" title="1">podUID = types.UID(kl.podManager.TranslatePodUID(podUID))
        pod := kubecontainer.Pods(pods).FindPod(podFullName, podUID)
        return pod.FindContainerByName(containerName), nil</span>
}

// RunInContainer runs a command in a container, returns the combined stdout, stderr as an array of bytes
func (kl *Kubelet) RunInContainer(ctx context.Context, podFullName string, podUID types.UID, containerName string, cmd []string) ([]byte, error) <span class="cov0" title="0">{
        container, err := kl.findContainer(ctx, podFullName, podUID, containerName)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">if container == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("container not found (%q)", containerName)
        }</span>
        // TODO(tallclair): Pass a proper timeout value.
        <span class="cov0" title="0">return kl.runner.RunInContainer(ctx, container.ID, cmd, 0)</span>
}

// GetExec gets the URL the exec will be served from, or nil if the Kubelet will serve it.
func (kl *Kubelet) GetExec(ctx context.Context, podFullName string, podUID types.UID, containerName string, cmd []string, streamOpts remotecommandserver.Options) (*url.URL, error) <span class="cov0" title="0">{
        container, err := kl.findContainer(ctx, podFullName, podUID, containerName)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">if container == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("container not found (%q)", containerName)
        }</span>
        <span class="cov0" title="0">return kl.streamingRuntime.GetExec(ctx, container.ID, cmd, streamOpts.Stdin, streamOpts.Stdout, streamOpts.Stderr, streamOpts.TTY)</span>
}

// GetAttach gets the URL the attach will be served from, or nil if the Kubelet will serve it.
func (kl *Kubelet) GetAttach(ctx context.Context, podFullName string, podUID types.UID, containerName string, streamOpts remotecommandserver.Options) (*url.URL, error) <span class="cov0" title="0">{
        container, err := kl.findContainer(ctx, podFullName, podUID, containerName)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">if container == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("container %s not found in pod %s", containerName, podFullName)
        }</span>

        // The TTY setting for attach must match the TTY setting in the initial container configuration,
        // since whether the process is running in a TTY cannot be changed after it has started.  We
        // need the api.Pod to get the TTY status.
        <span class="cov0" title="0">pod, found := kl.GetPodByFullName(podFullName)
        if !found || (string(podUID) != "" &amp;&amp; pod.UID != podUID) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("pod %s not found", podFullName)
        }</span>
        <span class="cov0" title="0">containerSpec := kubecontainer.GetContainerSpec(pod, containerName)
        if containerSpec == nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("container %s not found in pod %s", containerName, podFullName)
        }</span>
        <span class="cov0" title="0">tty := containerSpec.TTY

        return kl.streamingRuntime.GetAttach(ctx, container.ID, streamOpts.Stdin, streamOpts.Stdout, streamOpts.Stderr, tty)</span>
}

// GetPortForward gets the URL the port-forward will be served from, or nil if the Kubelet will serve it.
func (kl *Kubelet) GetPortForward(ctx context.Context, podName, podNamespace string, podUID types.UID, portForwardOpts portforward.V4Options) (*url.URL, error) <span class="cov0" title="0">{
        pods, err := kl.containerRuntime.GetPods(ctx, false)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        // Resolve and type convert back again.
        // We need the static pod UID but the kubecontainer API works with types.UID.
        <span class="cov0" title="0">podUID = types.UID(kl.podManager.TranslatePodUID(podUID))
        podFullName := kubecontainer.BuildPodFullName(podName, podNamespace)
        pod := kubecontainer.Pods(pods).FindPod(podFullName, podUID)
        if pod.IsEmpty() </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("pod not found (%q)", podFullName)
        }</span>

        <span class="cov0" title="0">return kl.streamingRuntime.GetPortForward(ctx, podName, podNamespace, podUID, portForwardOpts.Ports)</span>
}

// cleanupOrphanedPodCgroups removes cgroups that should no longer exist.
// it reconciles the cached state of cgroupPods with the specified list of runningPods
func (kl *Kubelet) cleanupOrphanedPodCgroups(pcm cm.PodContainerManager, cgroupPods map[types.UID]cm.CgroupName, possiblyRunningPods map[types.UID]sets.Empty) <span class="cov8" title="1">{
        // Iterate over all the found pods to verify if they should be running
        for uid, val := range cgroupPods </span><span class="cov8" title="1">{
                // if the pod is in the running set, its not a candidate for cleanup
                if _, ok := possiblyRunningPods[uid]; ok </span><span class="cov0" title="0">{
                        continue</span>
                }

                // If volumes have not been unmounted/detached, do not delete the cgroup
                // so any memory backed volumes don't have their charges propagated to the
                // parent croup.  If the volumes still exist, reduce the cpu shares for any
                // process in the cgroup to the minimum value while we wait.  if the kubelet
                // is configured to keep terminated volumes, we will delete the cgroup and not block.
                <span class="cov8" title="1">if podVolumesExist := kl.podVolumesExist(uid); podVolumesExist &amp;&amp; !kl.keepTerminatedPodVolumes </span><span class="cov0" title="0">{
                        klog.V(3).InfoS("Orphaned pod found, but volumes not yet removed.  Reducing cpu to minimum", "podUID", uid)
                        if err := pcm.ReduceCPULimits(val); err != nil </span><span class="cov0" title="0">{
                                klog.InfoS("Failed to reduce cpu time for pod pending volume cleanup", "podUID", uid, "err", err)
                        }</span>
                        <span class="cov0" title="0">continue</span>
                }
                <span class="cov8" title="1">klog.V(3).InfoS("Orphaned pod found, removing pod cgroups", "podUID", uid)
                // Destroy all cgroups of pod that should not be running,
                // by first killing all the attached processes to these cgroups.
                // We ignore errors thrown by the method, as the housekeeping loop would
                // again try to delete these unwanted pod cgroups
                go pcm.Destroy(val)</span>
        }
}
</pre>
		
		<pre class="file" id="file8" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "fmt"

        "k8s.io/klog/v2"

        "k8s.io/api/core/v1"
        "k8s.io/kubernetes/pkg/api/v1/resource"
)

// defaultPodLimitsForDownwardAPI copies the input pod, and optional container,
// and applies default resource limits. it returns a copy of the input pod,
// and a copy of the input container (if specified) with default limits
// applied. if a container has no limit specified, it will default the limit to
// the node allocatable.
// TODO: if/when we have pod level resources, we need to update this function
// to use those limits instead of node allocatable.
func (kl *Kubelet) defaultPodLimitsForDownwardAPI(pod *v1.Pod, container *v1.Container) (*v1.Pod, *v1.Container, error) <span class="cov0" title="0">{
        if pod == nil </span><span class="cov0" title="0">{
                return nil, nil, fmt.Errorf("invalid input, pod cannot be nil")
        }</span>

        <span class="cov0" title="0">node, err := kl.getNodeAnyWay()
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, fmt.Errorf("failed to find node object, expected a node")
        }</span>
        <span class="cov0" title="0">allocatable := node.Status.Allocatable
        klog.InfoS("Allocatable", "allocatable", allocatable)
        outputPod := pod.DeepCopy()
        for idx := range outputPod.Spec.Containers </span><span class="cov0" title="0">{
                resource.MergeContainerResourceLimits(&amp;outputPod.Spec.Containers[idx], allocatable)
        }</span>

        <span class="cov0" title="0">var outputContainer *v1.Container
        if container != nil </span><span class="cov0" title="0">{
                outputContainer = container.DeepCopy()
                resource.MergeContainerResourceLimits(outputContainer, allocatable)
        }</span>
        <span class="cov0" title="0">return outputPod, outputContainer, nil</span>
}
</pre>
		
		<pre class="file" id="file9" style="display: none">/*
Copyright 2022 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "compress/gzip"
        "context"
        "errors"
        "fmt"
        "io"
        "net/http"
        "net/url"
        "os"
        "os/exec"
        "reflect"
        "regexp"
        "regexp/syntax"
        "runtime"
        "strconv"
        "strings"
        "time"

        securejoin "github.com/cyphar/filepath-securejoin"

        utilvalidation "k8s.io/apimachinery/pkg/util/validation"
        "k8s.io/apimachinery/pkg/util/validation/field"
)

const (
        dateLayout       = "2006-1-2 15:4:5"
        maxTailLines     = 100000
        maxServiceLength = 256
        maxServices      = 4
        nodeLogDir       = "/var/log/"
)

var (
        journal = journalServer{}
        // The set of known safe characters to pass to journalctl / GetWinEvent flags - only add to this list if the
        // character cannot be used to create invalid sequences. This is intended as a broad defense against malformed
        // input that could cause an escape.
        reServiceNameUnsafeCharacters = regexp.MustCompile(`[^a-zA-Z\-_.:0-9@]+`)
)

// journalServer returns text output from the OS specific service logger to view
// from the client. It runs with the privileges of the calling  process
// (the kubelet) and should only be allowed to be invoked by a root user.
type journalServer struct{}

// ServeHTTP translates HTTP query parameters into arguments to be passed
// to journalctl on the current system. It supports content-encoding of
// gzip to reduce total content size.
func (journalServer) ServeHTTP(w http.ResponseWriter, req *http.Request) <span class="cov0" title="0">{
        var out io.Writer = w

        nlq, errs := newNodeLogQuery(req.URL.Query())
        if len(errs) &gt; 0 </span><span class="cov0" title="0">{
                http.Error(w, errs.ToAggregate().Error(), http.StatusBadRequest)
                return
        }</span>

        // TODO: Also set a response header that indicates how the request's query was resolved,
        // e.g. "kube-log-source: journal://foobar?arg1=value" or "kube-log-source: file:///var/log/foobar.log"
        <span class="cov0" title="0">w.Header().Set("Content-Type", "text/plain;charset=UTF-8")
        if req.Header.Get("Accept-Encoding") == "gzip" </span><span class="cov0" title="0">{
                w.Header().Set("Content-Encoding", "gzip")

                gz, err := gzip.NewWriterLevel(out, gzip.BestSpeed)
                if err != nil </span><span class="cov0" title="0">{
                        fmt.Fprintf(w, "\nfailed to get gzip writer: %v\n", err)
                        return
                }</span>
                <span class="cov0" title="0">defer gz.Close()
                out = gz</span>
        }
        <span class="cov0" title="0">nlq.Copy(out)</span>
}

// nodeLogQuery encapsulates the log query request
type nodeLogQuery struct {
        // Services are the list of services to be queried
        Services []string
        // Files are the list of files
        Files []string
        options
}

// options encapsulates the query options for services
type options struct {
        // SinceTime is an RFC3339 timestamp from which to show logs.
        SinceTime *time.Time
        // UntilTime is an RFC3339 timestamp until which to show logs.
        UntilTime *time.Time
        // TailLines is used to retrieve the specified number of lines (not more than 100k) from the end of the log.
        // Support for this is implementation specific and only available for service logs.
        TailLines *int
        // Boot show messages from a specific boot. Allowed values are less than 1. Passing an invalid boot offset will fail
        // retrieving logs and return an error. Support for this is implementation specific
        Boot *int
        // Pattern filters log entries by the provided regex pattern. On Linux nodes, this pattern will be read as a
        // PCRE2 regex, on Windows nodes it will be read as a PowerShell regex. Support for this is implementation specific.
        Pattern string
}

// newNodeLogQuery parses query values and converts all known options into nodeLogQuery
func newNodeLogQuery(query url.Values) (*nodeLogQuery, field.ErrorList) <span class="cov0" title="0">{
        allErrs := field.ErrorList{}
        var nlq nodeLogQuery
        var err error

        queries, ok := query["query"]
        if len(queries) &gt; 0 </span><span class="cov0" title="0">{
                for _, q := range queries </span><span class="cov0" title="0">{
                        // The presence of / or \ is a hint that the query is for a log file. If the query is for foo.log without a
                        // slash prefix, the heuristics will still return the file contents.
                        if strings.ContainsAny(q, `/\`) </span><span class="cov0" title="0">{
                                nlq.Files = append(nlq.Files, q)
                        }</span> else<span class="cov0" title="0"> if strings.TrimSpace(q) != "" </span><span class="cov0" title="0">{ // Prevent queries with just spaces
                                nlq.Services = append(nlq.Services, q)
                        }</span>
                }
        }

        // Prevent specifying  an empty or blank space query.
        // Example: kubectl get --raw /api/v1/nodes/$node/proxy/logs?query="   "
        <span class="cov0" title="0">if ok &amp;&amp; (len(nlq.Files) == 0 &amp;&amp; len(nlq.Services) == 0) </span><span class="cov0" title="0">{
                allErrs = append(allErrs, field.Invalid(field.NewPath("query"), queries, "query cannot be empty"))
        }</span>

        <span class="cov0" title="0">var sinceTime time.Time
        sinceTimeValue := query.Get("sinceTime")
        if len(sinceTimeValue) &gt; 0 </span><span class="cov0" title="0">{
                sinceTime, err = time.Parse(time.RFC3339, sinceTimeValue)
                if err != nil </span><span class="cov0" title="0">{
                        allErrs = append(allErrs, field.Invalid(field.NewPath("sinceTime"), sinceTimeValue, "invalid time format"))
                }</span> else<span class="cov0" title="0"> {
                        nlq.SinceTime = &amp;sinceTime
                }</span>
        }

        <span class="cov0" title="0">var untilTime time.Time
        untilTimeValue := query.Get("untilTime")
        if len(untilTimeValue) &gt; 0 </span><span class="cov0" title="0">{
                untilTime, err = time.Parse(time.RFC3339, untilTimeValue)
                if err != nil </span><span class="cov0" title="0">{
                        allErrs = append(allErrs, field.Invalid(field.NewPath("untilTime"), untilTimeValue, "invalid time format"))
                }</span> else<span class="cov0" title="0"> {
                        nlq.UntilTime = &amp;untilTime
                }</span>
        }

        <span class="cov0" title="0">var boot int
        bootValue := query.Get("boot")
        if len(bootValue) &gt; 0 </span><span class="cov0" title="0">{
                boot, err = strconv.Atoi(bootValue)
                if err != nil </span><span class="cov0" title="0">{
                        allErrs = append(allErrs, field.Invalid(field.NewPath("boot"), bootValue, err.Error()))
                }</span> else<span class="cov0" title="0"> {
                        nlq.Boot = &amp;boot
                }</span>
        }

        <span class="cov0" title="0">var tailLines int
        tailLinesValue := query.Get("tailLines")
        if len(tailLinesValue) &gt; 0 </span><span class="cov0" title="0">{
                tailLines, err = strconv.Atoi(tailLinesValue)
                if err != nil </span><span class="cov0" title="0">{
                        allErrs = append(allErrs, field.Invalid(field.NewPath("tailLines"), tailLinesValue, err.Error()))
                }</span> else<span class="cov0" title="0"> {
                        nlq.TailLines = &amp;tailLines
                }</span>
        }

        <span class="cov0" title="0">pattern := query.Get("pattern")
        if len(pattern) &gt; 0 </span><span class="cov0" title="0">{
                nlq.Pattern = pattern
        }</span>

        <span class="cov0" title="0">if len(allErrs) &gt; 0 </span><span class="cov0" title="0">{
                return nil, allErrs
        }</span>

        <span class="cov0" title="0">if reflect.DeepEqual(nlq, nodeLogQuery{}) </span><span class="cov0" title="0">{
                return nil, allErrs
        }</span>

        <span class="cov0" title="0">return &amp;nlq, allErrs</span>
}

func validateServices(services []string) field.ErrorList <span class="cov0" title="0">{
        allErrs := field.ErrorList{}

        for _, s := range services </span><span class="cov0" title="0">{
                if err := safeServiceName(s); err != nil </span><span class="cov0" title="0">{
                        allErrs = append(allErrs, field.Invalid(field.NewPath("query"), s, err.Error()))
                }</span>
        }

        <span class="cov0" title="0">if len(services) &gt; maxServices </span><span class="cov0" title="0">{
                allErrs = append(allErrs, field.TooMany(field.NewPath("query"), len(services), maxServices))
        }</span>
        <span class="cov0" title="0">return allErrs</span>
}

func (n *nodeLogQuery) validate() field.ErrorList <span class="cov0" title="0">{
        allErrs := validateServices(n.Services)
        switch </span>{
        case len(n.Files) == 0 &amp;&amp; len(n.Services) == 0:<span class="cov0" title="0">
                allErrs = append(allErrs, field.Required(field.NewPath("query"), "cannot be empty with options"))</span>
        case len(n.Files) &gt; 0 &amp;&amp; len(n.Services) &gt; 0:<span class="cov0" title="0">
                allErrs = append(allErrs, field.Invalid(field.NewPath("query"), fmt.Sprintf("%v, %v", n.Files, n.Services),
                        "cannot specify a file and service"))</span>
        case len(n.Files) &gt; 1:<span class="cov0" title="0">
                allErrs = append(allErrs, field.Invalid(field.NewPath("query"), n.Files, "cannot specify more than one file"))</span>
        case len(n.Files) == 1 &amp;&amp; n.options != (options{}):<span class="cov0" title="0">
                allErrs = append(allErrs, field.Invalid(field.NewPath("query"), n.Files, "cannot specify file with options"))</span>
        case len(n.Files) == 1:<span class="cov0" title="0">
                if fullLogFilename, err := securejoin.SecureJoin(nodeLogDir, n.Files[0]); err != nil </span><span class="cov0" title="0">{
                        allErrs = append(allErrs, field.Invalid(field.NewPath("query"), n.Files, err.Error()))
                }</span> else<span class="cov0" title="0"> if _, err := os.Stat(fullLogFilename); err != nil </span><span class="cov0" title="0">{
                        allErrs = append(allErrs, field.Invalid(field.NewPath("query"), n.Files, err.Error()))
                }</span>
        }

        <span class="cov0" title="0">if n.SinceTime != nil &amp;&amp; n.UntilTime != nil &amp;&amp; (n.SinceTime.After(*n.UntilTime)) </span><span class="cov0" title="0">{
                allErrs = append(allErrs, field.Invalid(field.NewPath("untilTime"), n.UntilTime, "must be after `sinceTime`"))
        }</span>

        <span class="cov0" title="0">if n.Boot != nil &amp;&amp; runtime.GOOS == "windows" </span><span class="cov0" title="0">{
                allErrs = append(allErrs, field.Invalid(field.NewPath("boot"), *n.Boot, "boot is not supported on Windows"))
        }</span>

        <span class="cov0" title="0">if n.Boot != nil &amp;&amp; *n.Boot &gt; 0 </span><span class="cov0" title="0">{
                allErrs = append(allErrs, field.Invalid(field.NewPath("boot"), *n.Boot, "must be less than 1"))
        }</span>

        <span class="cov0" title="0">if n.TailLines != nil </span><span class="cov0" title="0">{
                if err := utilvalidation.IsInRange((int)(*n.TailLines), 0, maxTailLines); err != nil </span><span class="cov0" title="0">{
                        allErrs = append(allErrs, field.Invalid(field.NewPath("tailLines"), *n.TailLines, err[0]))
                }</span>
        }

        <span class="cov0" title="0">if _, err := syntax.Parse(n.Pattern, syntax.Perl); err != nil </span><span class="cov0" title="0">{
                allErrs = append(allErrs, field.Invalid(field.NewPath("pattern"), n.Pattern, err.Error()))
        }</span>

        <span class="cov0" title="0">return allErrs</span>
}

// Copy streams the contents of the OS specific logging command executed  with the current args to the provided
// writer. If an error occurs a line is written to the output.
func (n *nodeLogQuery) Copy(w io.Writer) <span class="cov0" title="0">{
        // set the deadline to the maximum across both runs
        ctx, cancel := context.WithDeadline(context.Background(), time.Now().Add(30*time.Second))
        defer cancel()
        boot := 0
        if n.Boot != nil </span><span class="cov0" title="0">{
                boot = *n.Boot
        }</span>
        <span class="cov0" title="0">n.copyForBoot(ctx, w, boot)</span>
}

// copyForBoot invokes the OS specific logging command with the  provided args
func (n *nodeLogQuery) copyForBoot(ctx context.Context, w io.Writer, previousBoot int) <span class="cov0" title="0">{
        if ctx.Err() != nil </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov0" title="0">nativeLoggers, fileLoggers := n.splitNativeVsFileLoggers(ctx)
        if len(nativeLoggers) &gt; 0 </span><span class="cov0" title="0">{
                n.copyServiceLogs(ctx, w, nativeLoggers, previousBoot)
        }</span>

        <span class="cov0" title="0">if len(fileLoggers) &gt; 0 &amp;&amp; n.options != (options{}) </span><span class="cov0" title="0">{
                fmt.Fprintf(w, "\noptions present and query resolved to log files for %v\ntry without specifying options\n",
                        fileLoggers)
                return
        }</span>

        <span class="cov0" title="0">if len(fileLoggers) &gt; 0 </span><span class="cov0" title="0">{
                copyFileLogs(ctx, w, fileLoggers)
        }</span>
}

// splitNativeVsFileLoggers checks if each service logs to native OS logs or to a file and returns a list of services
// that log natively vs maybe to a file
func (n *nodeLogQuery) splitNativeVsFileLoggers(ctx context.Context) ([]string, []string) <span class="cov0" title="0">{
        var nativeLoggers []string
        var fileLoggers []string

        for _, service := range n.Services </span><span class="cov0" title="0">{
                // Check the journalctl output to figure if the service is using journald or not. This is not needed in the
                // Get-WinEvent case as the command returns an error if a service is not logging to the Application provider.
                if checkForNativeLogger(ctx, service) </span><span class="cov0" title="0">{
                        nativeLoggers = append(nativeLoggers, service)
                }</span> else<span class="cov0" title="0"> {
                        fileLoggers = append(fileLoggers, service)
                }</span>
        }
        <span class="cov0" title="0">return nativeLoggers, fileLoggers</span>
}

// copyServiceLogs invokes journalctl or Get-WinEvent with the provided args. Note that
// services are explicitly passed here to account for the heuristics.
func (n *nodeLogQuery) copyServiceLogs(ctx context.Context, w io.Writer, services []string, previousBoot int) <span class="cov0" title="0">{
        cmdStr, args, err := getLoggingCmd(n, services)
        if err != nil </span><span class="cov0" title="0">{
                fmt.Fprintf(w, "\nfailed to get logging cmd: %v\n", err)
                return
        }</span>
        <span class="cov0" title="0">cmd := exec.CommandContext(ctx, cmdStr, args...)
        cmd.Stdout = w
        cmd.Stderr = w

        if err := cmd.Run(); err != nil </span><span class="cov0" title="0">{
                if _, ok := err.(*exec.ExitError); ok </span><span class="cov0" title="0">{
                        return
                }</span>
                <span class="cov0" title="0">if previousBoot == 0 </span><span class="cov0" title="0">{
                        fmt.Fprintf(w, "\nerror: journal output not available\n")
                }</span>
        }
}

// copyFileLogs loops over all the services and attempts to collect the file logs of each service
func copyFileLogs(ctx context.Context, w io.Writer, services []string) <span class="cov0" title="0">{
        if ctx.Err() != nil </span><span class="cov0" title="0">{
                fmt.Fprintf(w, "\ncontext error: %v\n", ctx.Err())
                return
        }</span>

        <span class="cov0" title="0">for _, service := range services </span><span class="cov0" title="0">{
                heuristicsCopyFileLogs(ctx, w, service)
        }</span>
}

// heuristicsCopyFileLogs attempts to collect logs from either
// /var/log/service
// /var/log/service.log or
// /var/log/service/service.log or
// in that order stopping on first success.
func heuristicsCopyFileLogs(ctx context.Context, w io.Writer, service string) <span class="cov0" title="0">{
        logFileNames := [3]string{
                service,
                fmt.Sprintf("%s.log", service),
                fmt.Sprintf("%s/%s.log", service, service),
        }

        var err error
        for _, logFileName := range logFileNames </span><span class="cov0" title="0">{
                var logFile string
                logFile, err = securejoin.SecureJoin(nodeLogDir, logFileName)
                if err != nil </span><span class="cov0" title="0">{
                        break</span>
                }
                <span class="cov0" title="0">err = heuristicsCopyFileLog(ctx, w, logFile)
                if err == nil </span><span class="cov0" title="0">{
                        break</span>
                } else<span class="cov0" title="0"> if errors.Is(err, os.ErrNotExist) </span><span class="cov0" title="0">{
                        continue</span>
                } else<span class="cov0" title="0"> {
                        break</span>
                }
        }

        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                // If the last error was file not found it implies that no log file was found for the service
                if errors.Is(err, os.ErrNotExist) </span><span class="cov0" title="0">{
                        fmt.Fprintf(w, "\nlog not found for %s\n", service)
                        return
                }</span>
                <span class="cov0" title="0">fmt.Fprintf(w, "\nerror getting log for %s: %v\n", service, err)</span>
        }
}

// readerCtx is the interface that wraps io.Reader with a context
type readerCtx struct {
        ctx context.Context
        io.Reader
}

func (r *readerCtx) Read(p []byte) (n int, err error) <span class="cov0" title="0">{
        if err := r.ctx.Err(); err != nil </span><span class="cov0" title="0">{
                return 0, err
        }</span>
        <span class="cov0" title="0">return r.Reader.Read(p)</span>
}

// newReaderCtx gets a context-aware io.Reader
func newReaderCtx(ctx context.Context, r io.Reader) io.Reader <span class="cov0" title="0">{
        return &amp;readerCtx{
                ctx:    ctx,
                Reader: r,
        }
}</span>

// heuristicsCopyFileLog returns the contents of the given logFile
func heuristicsCopyFileLog(ctx context.Context, w io.Writer, logFile string) error <span class="cov0" title="0">{
        fInfo, err := os.Stat(logFile)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        // This is to account for the heuristics where logs for service foo
        // could be in /var/log/foo/
        <span class="cov0" title="0">if fInfo.IsDir() </span><span class="cov0" title="0">{
                return os.ErrNotExist
        }</span>

        <span class="cov0" title="0">f, err := os.Open(logFile)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">defer f.Close()

        if _, err := io.Copy(w, newReaderCtx(ctx, f)); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov0" title="0">return nil</span>
}

func safeServiceName(s string) error <span class="cov0" title="0">{
        // Max length of a service name is 256 across supported OSes
        if len(s) &gt; maxServiceLength </span><span class="cov0" title="0">{
                return fmt.Errorf("length must be less than 100")
        }</span>

        <span class="cov0" title="0">if reServiceNameUnsafeCharacters.MatchString(s) </span><span class="cov0" title="0">{
                return fmt.Errorf("input contains unsupported characters")
        }</span>
        <span class="cov0" title="0">return nil</span>
}
</pre>
		
		<pre class="file" id="file10" style="display: none">//go:build linux

/*
Copyright 2022 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "context"
        "fmt"
        "os/exec"
        "strings"
)

// getLoggingCmd returns the journalctl cmd and arguments for the given nodeLogQuery and boot. Note that
// services are explicitly passed here to account for the heuristics
func getLoggingCmd(n *nodeLogQuery, services []string) (string, []string, error) <span class="cov0" title="0">{
        args := []string{
                "--utc",
                "--no-pager",
                "--output=short-precise",
        }
        if n.SinceTime != nil </span><span class="cov0" title="0">{
                args = append(args, fmt.Sprintf("--since=%s", n.SinceTime.Format(dateLayout)))
        }</span>
        <span class="cov0" title="0">if n.UntilTime != nil </span><span class="cov0" title="0">{
                args = append(args, fmt.Sprintf("--until=%s", n.SinceTime.Format(dateLayout)))
        }</span>
        <span class="cov0" title="0">if n.TailLines != nil </span><span class="cov0" title="0">{
                args = append(args, "--pager-end", fmt.Sprintf("--lines=%d", *n.TailLines))
        }</span>
        <span class="cov0" title="0">for _, service := range services </span><span class="cov0" title="0">{
                if len(service) &gt; 0 </span><span class="cov0" title="0">{
                        args = append(args, "--unit="+service)
                }</span>
        }
        <span class="cov0" title="0">if len(n.Pattern) &gt; 0 </span><span class="cov0" title="0">{
                args = append(args, "--grep="+n.Pattern)
        }</span>

        <span class="cov0" title="0">if n.Boot != nil </span><span class="cov0" title="0">{
                args = append(args, "--boot", fmt.Sprintf("%d", *n.Boot))
        }</span>

        <span class="cov0" title="0">return "journalctl", args, nil</span>
}

// checkForNativeLogger checks journalctl output for a service
func checkForNativeLogger(ctx context.Context, service string) bool <span class="cov0" title="0">{
        // This will return all the journald units
        cmd := exec.CommandContext(ctx, "journalctl", []string{"--field", "_SYSTEMD_UNIT"}...)
        output, err := cmd.CombinedOutput()
        if err != nil </span><span class="cov0" title="0">{
                // Returning false to allow checking if the service is logging to a file
                return false
        }</span>

        // journalctl won't return an error if we try to fetch logs for a non-existent service,
        // hence we search for it in the list of services known to journalctl
        <span class="cov0" title="0">return strings.Contains(string(output), service+".service")</span>
}
</pre>
		
		<pre class="file" id="file11" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "fmt"
        "os"
        "path/filepath"
        "syscall"

        v1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/types"
        utilerrors "k8s.io/apimachinery/pkg/util/errors"
        "k8s.io/apimachinery/pkg/util/sets"
        "k8s.io/klog/v2"
        kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
        "k8s.io/kubernetes/pkg/kubelet/metrics"
        "k8s.io/kubernetes/pkg/util/removeall"
        "k8s.io/kubernetes/pkg/volume"
        volumetypes "k8s.io/kubernetes/pkg/volume/util/types"
)

// ListVolumesForPod returns a map of the mounted volumes for the given pod.
// The key in the map is the OuterVolumeSpecName (i.e. pod.Spec.Volumes[x].Name)
func (kl *Kubelet) ListVolumesForPod(podUID types.UID) (map[string]volume.Volume, bool) <span class="cov0" title="0">{
        volumesToReturn := make(map[string]volume.Volume)
        podVolumes := kl.volumeManager.GetMountedVolumesForPod(
                volumetypes.UniquePodName(podUID))
        for outerVolumeSpecName, volume := range podVolumes </span><span class="cov0" title="0">{
                // TODO: volume.Mounter could be nil if volume object is recovered
                // from reconciler's sync state process. PR 33616 will fix this problem
                // to create Mounter object when recovering volume state.
                if volume.Mounter == nil </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">volumesToReturn[outerVolumeSpecName] = volume.Mounter</span>
        }

        <span class="cov0" title="0">return volumesToReturn, len(volumesToReturn) &gt; 0</span>
}

// ListBlockVolumesForPod returns a map of the mounted volumes for the given
// pod. The key in the map is the OuterVolumeSpecName (i.e.
// pod.Spec.Volumes[x].Name)
func (kl *Kubelet) ListBlockVolumesForPod(podUID types.UID) (map[string]volume.BlockVolume, bool) <span class="cov0" title="0">{
        volumesToReturn := make(map[string]volume.BlockVolume)
        podVolumes := kl.volumeManager.GetMountedVolumesForPod(
                volumetypes.UniquePodName(podUID))
        for outerVolumeSpecName, volume := range podVolumes </span><span class="cov0" title="0">{
                // TODO: volume.Mounter could be nil if volume object is recovered
                // from reconciler's sync state process. PR 33616 will fix this problem
                // to create Mounter object when recovering volume state.
                if volume.BlockVolumeMapper == nil </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">volumesToReturn[outerVolumeSpecName] = volume.BlockVolumeMapper</span>
        }

        <span class="cov0" title="0">return volumesToReturn, len(volumesToReturn) &gt; 0</span>
}

// podVolumesExist checks with the volume manager and returns true any of the
// pods for the specified volume are mounted or are uncertain.
func (kl *Kubelet) podVolumesExist(podUID types.UID) bool <span class="cov8" title="1">{
        if mountedVolumes :=
                kl.volumeManager.GetPossiblyMountedVolumesForPod(
                        volumetypes.UniquePodName(podUID)); len(mountedVolumes) &gt; 0 </span><span class="cov0" title="0">{
                return true
        }</span>
        // TODO: This checks pod volume paths and whether they are mounted. If checking returns error, podVolumesExist will return true
        // which means we consider volumes might exist and requires further checking.
        // There are some volume plugins such as flexvolume might not have mounts. See issue #61229
        <span class="cov8" title="1">volumePaths, err := kl.getMountedVolumePathListFromDisk(podUID)
        if err != nil </span><span class="cov0" title="0">{
                klog.ErrorS(err, "Pod found, but error occurred during checking mounted volumes from disk", "podUID", podUID)
                return true
        }</span>
        <span class="cov8" title="1">if len(volumePaths) &gt; 0 </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod found, but volumes are still mounted on disk", "podUID", podUID, "paths", volumePaths)
                return true
        }</span>

        <span class="cov8" title="1">return false</span>
}

// newVolumeMounterFromPlugins attempts to find a plugin by volume spec, pod
// and volume options and then creates a Mounter.
// Returns a valid mounter or an error.
func (kl *Kubelet) newVolumeMounterFromPlugins(spec *volume.Spec, pod *v1.Pod, opts volume.VolumeOptions) (volume.Mounter, error) <span class="cov0" title="0">{
        plugin, err := kl.volumePluginMgr.FindPluginBySpec(spec)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("can't use volume plugins for %s: %v", spec.Name(), err)
        }</span>
        <span class="cov0" title="0">physicalMounter, err := plugin.NewMounter(spec, pod, opts)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("failed to instantiate mounter for volume: %s using plugin: %s with a root cause: %v", spec.Name(), plugin.GetPluginName(), err)
        }</span>
        <span class="cov0" title="0">klog.V(10).InfoS("Using volume plugin for mount", "volumePluginName", plugin.GetPluginName(), "volumeName", spec.Name())
        return physicalMounter, nil</span>
}

// removeOrphanedPodVolumeDirs attempts to remove the pod volumes directory and
// its subdirectories. There should be no files left under normal conditions
// when this is called, so it effectively does a recursive rmdir instead of
// RemoveAll to ensure it only removes empty directories and files that were
// used as mount points, but not content of the mount points.
func (kl *Kubelet) removeOrphanedPodVolumeDirs(uid types.UID) []error <span class="cov8" title="1">{
        orphanVolumeErrors := []error{}

        // If there are still volume directories, attempt to rmdir them
        volumePaths, err := kl.getPodVolumePathListFromDisk(uid)
        if err != nil </span><span class="cov0" title="0">{
                orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf("orphaned pod %q found, but error occurred during reading volume dir from disk: %v", uid, err))
                return orphanVolumeErrors
        }</span>
        <span class="cov8" title="1">if len(volumePaths) &gt; 0 </span><span class="cov0" title="0">{
                for _, volumePath := range volumePaths </span><span class="cov0" title="0">{
                        if err := syscall.Rmdir(volumePath); err != nil </span><span class="cov0" title="0">{
                                orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf("orphaned pod %q found, but failed to rmdir() volume at path %v: %v", uid, volumePath, err))
                        }</span> else<span class="cov0" title="0"> {
                                klog.InfoS("Cleaned up orphaned volume from pod", "podUID", uid, "path", volumePath)
                        }</span>
                }
        }

        // If there are any volume-subpaths, attempt to remove them
        <span class="cov8" title="1">subpathVolumePaths, err := kl.getPodVolumeSubpathListFromDisk(uid)
        if err != nil </span><span class="cov0" title="0">{
                orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf("orphaned pod %q found, but error occurred during reading of volume-subpaths dir from disk: %v", uid, err))
                return orphanVolumeErrors
        }</span>
        <span class="cov8" title="1">if len(subpathVolumePaths) &gt; 0 </span><span class="cov0" title="0">{
                for _, subpathVolumePath := range subpathVolumePaths </span><span class="cov0" title="0">{
                        // Remove both files and empty directories here, as the subpath may have been a bind-mount of a file or a directory.
                        if err := os.Remove(subpathVolumePath); err != nil </span><span class="cov0" title="0">{
                                orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf("orphaned pod %q found, but failed to rmdir() subpath at path %v: %v", uid, subpathVolumePath, err))
                        }</span> else<span class="cov0" title="0"> {
                                klog.InfoS("Cleaned up orphaned volume subpath from pod", "podUID", uid, "path", subpathVolumePath)
                        }</span>
                }
        }

        // Remove any remaining subdirectories along with the volumes directory itself.
        // Fail if any regular files are encountered.
        <span class="cov8" title="1">podVolDir := kl.getPodVolumesDir(uid)
        if err := removeall.RemoveDirsOneFilesystem(kl.mounter, podVolDir); err != nil </span><span class="cov0" title="0">{
                orphanVolumeErrors = append(orphanVolumeErrors, fmt.Errorf("orphaned pod %q found, but error occurred when trying to remove the volumes dir: %v", uid, err))
        }</span> else<span class="cov8" title="1"> {
                klog.InfoS("Cleaned up orphaned pod volumes dir", "podUID", uid, "path", podVolDir)
        }</span>

        <span class="cov8" title="1">return orphanVolumeErrors</span>
}

// cleanupOrphanedPodDirs removes the volumes of pods that should not be
// running and that have no containers running.  Note that we roll up logs here since it runs in the main loop.
func (kl *Kubelet) cleanupOrphanedPodDirs(pods []*v1.Pod, runningPods []*kubecontainer.Pod) error <span class="cov8" title="1">{
        allPods := sets.NewString()
        for _, pod := range pods </span><span class="cov8" title="1">{
                allPods.Insert(string(pod.UID))
        }</span>
        <span class="cov8" title="1">for _, pod := range runningPods </span><span class="cov8" title="1">{
                allPods.Insert(string(pod.ID))
        }</span>

        <span class="cov8" title="1">found, err := kl.listPodsFromDisk()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">orphanRemovalErrors := []error{}
        orphanVolumeErrors := []error{}
        var totalPods, errorPods int

        for _, uid := range found </span><span class="cov8" title="1">{
                if allPods.Has(string(uid)) </span><span class="cov8" title="1">{
                        continue</span>
                }

                <span class="cov8" title="1">totalPods++

                // If volumes have not been unmounted/detached, do not delete directory.
                // Doing so may result in corruption of data.
                // TODO: getMountedVolumePathListFromDisk() call may be redundant with
                // kl.getPodVolumePathListFromDisk(). Can this be cleaned up?
                if podVolumesExist := kl.podVolumesExist(uid); podVolumesExist </span><span class="cov0" title="0">{
                        errorPods++
                        klog.V(3).InfoS("Orphaned pod found, but volumes are not cleaned up", "podUID", uid)
                        continue</span>
                }

                // Attempt to remove the pod volumes directory and its subdirs
                <span class="cov8" title="1">podVolumeErrors := kl.removeOrphanedPodVolumeDirs(uid)
                if len(podVolumeErrors) &gt; 0 </span><span class="cov0" title="0">{
                        errorPods++
                        orphanVolumeErrors = append(orphanVolumeErrors, podVolumeErrors...)
                        // Not all volumes were removed, so don't clean up the pod directory yet. It is likely
                        // that there are still mountpoints or files left which could cause removal of the pod
                        // directory to fail below.
                        // Errors for all removal operations have already been recorded, so don't add another
                        // one here.
                        continue</span>
                }

                // Call RemoveAllOneFilesystem for remaining subdirs under the pod directory
                <span class="cov8" title="1">podDir := kl.getPodDir(uid)
                podSubdirs, err := os.ReadDir(podDir)
                if err != nil </span><span class="cov0" title="0">{
                        errorPods++
                        klog.ErrorS(err, "Could not read directory", "path", podDir)
                        orphanRemovalErrors = append(orphanRemovalErrors, fmt.Errorf("orphaned pod %q found, but error occurred during reading the pod dir from disk: %v", uid, err))
                        continue</span>
                }

                <span class="cov8" title="1">var cleanupFailed bool
                for _, podSubdir := range podSubdirs </span><span class="cov8" title="1">{
                        podSubdirName := podSubdir.Name()
                        podSubdirPath := filepath.Join(podDir, podSubdirName)
                        // Never attempt RemoveAllOneFilesystem on the volumes directory,
                        // as this could lead to data loss in some situations. The volumes
                        // directory should have been removed by removeOrphanedPodVolumeDirs.
                        if podSubdirName == "volumes" </span><span class="cov0" title="0">{
                                cleanupFailed = true
                                err := fmt.Errorf("volumes subdir was found after it was removed")
                                klog.ErrorS(err, "Orphaned pod found, but failed to remove volumes subdir", "podUID", uid, "path", podSubdirPath)
                                continue</span>
                        }
                        <span class="cov8" title="1">if err := removeall.RemoveAllOneFilesystem(kl.mounter, podSubdirPath); err != nil </span><span class="cov0" title="0">{
                                cleanupFailed = true
                                klog.ErrorS(err, "Failed to remove orphaned pod subdir", "podUID", uid, "path", podSubdirPath)
                                orphanRemovalErrors = append(orphanRemovalErrors, fmt.Errorf("orphaned pod %q found, but error occurred when trying to remove subdir %q: %v", uid, podSubdirPath, err))
                        }</span>
                }

                // Rmdir the pod dir, which should be empty if everything above was successful
                <span class="cov8" title="1">klog.V(3).InfoS("Orphaned pod found, removing", "podUID", uid)
                if err := syscall.Rmdir(podDir); err != nil </span><span class="cov0" title="0">{
                        cleanupFailed = true
                        klog.ErrorS(err, "Failed to remove orphaned pod dir", "podUID", uid)
                        orphanRemovalErrors = append(orphanRemovalErrors, fmt.Errorf("orphaned pod %q found, but error occurred when trying to remove the pod directory: %v", uid, err))
                }</span>
                <span class="cov8" title="1">if cleanupFailed </span><span class="cov0" title="0">{
                        errorPods++
                }</span>
        }

        <span class="cov8" title="1">logSpew := func(errs []error) </span><span class="cov8" title="1">{
                if len(errs) &gt; 0 </span><span class="cov0" title="0">{
                        klog.ErrorS(errs[0], "There were many similar errors. Turn up verbosity to see them.", "numErrs", len(errs))
                        for _, err := range errs </span><span class="cov0" title="0">{
                                klog.V(5).InfoS("Orphan pod", "err", err)
                        }</span>
                }
        }
        <span class="cov8" title="1">logSpew(orphanVolumeErrors)
        logSpew(orphanRemovalErrors)
        metrics.OrphanPodCleanedVolumes.Set(float64(totalPods))
        metrics.OrphanPodCleanedVolumesErrors.Set(float64(errorPods))
        return utilerrors.NewAggregate(orphanRemovalErrors)</span>
}
</pre>
		
		<pre class="file" id="file12" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "context"
        "sort"

        "k8s.io/apimachinery/pkg/util/wait"
        "k8s.io/klog/v2"
        kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
)

const (
        // The limit on the number of buffered container deletion requests
        // This number is a bit arbitrary and may be adjusted in the future.
        containerDeletorBufferLimit = 50
)

type containerStatusbyCreatedList []*kubecontainer.Status

type podContainerDeletor struct {
        worker           chan&lt;- kubecontainer.ContainerID
        containersToKeep int
}

func (a containerStatusbyCreatedList) Len() int      <span class="cov0" title="0">{ return len(a) }</span>
func (a containerStatusbyCreatedList) Swap(i, j int) <span class="cov0" title="0">{ a[i], a[j] = a[j], a[i] }</span>
func (a containerStatusbyCreatedList) Less(i, j int) bool <span class="cov0" title="0">{
        return a[i].CreatedAt.After(a[j].CreatedAt)
}</span>

func newPodContainerDeletor(runtime kubecontainer.Runtime, containersToKeep int) *podContainerDeletor <span class="cov8" title="1">{
        buffer := make(chan kubecontainer.ContainerID, containerDeletorBufferLimit)
        go wait.Until(func() </span><span class="cov8" title="1">{
                for </span><span class="cov8" title="1">{
                        id := &lt;-buffer
                        if err := runtime.DeleteContainer(context.Background(), id); err != nil </span><span class="cov0" title="0">{
                                klog.InfoS("DeleteContainer returned error", "containerID", id, "err", err)
                        }</span>
                }
        }, 0, wait.NeverStop)

        <span class="cov8" title="1">return &amp;podContainerDeletor{
                worker:           buffer,
                containersToKeep: containersToKeep,
        }</span>
}

// getContainersToDeleteInPod returns the exited containers in a pod whose name matches the name inferred from filterContainerId (if not empty), ordered by the creation time from the latest to the earliest.
// If filterContainerID is empty, all dead containers in the pod are returned.
func getContainersToDeleteInPod(filterContainerID string, podStatus *kubecontainer.PodStatus, containersToKeep int) containerStatusbyCreatedList <span class="cov0" title="0">{
        matchedContainer := func(filterContainerId string, podStatus *kubecontainer.PodStatus) *kubecontainer.Status </span><span class="cov0" title="0">{
                if filterContainerId == "" </span><span class="cov0" title="0">{
                        return nil
                }</span>
                <span class="cov0" title="0">for _, containerStatus := range podStatus.ContainerStatuses </span><span class="cov0" title="0">{
                        if containerStatus.ID.ID == filterContainerId </span><span class="cov0" title="0">{
                                return containerStatus
                        }</span>
                }
                <span class="cov0" title="0">return nil</span>
        }(filterContainerID, podStatus)

        <span class="cov0" title="0">if filterContainerID != "" &amp;&amp; matchedContainer == nil </span><span class="cov0" title="0">{
                klog.InfoS("Container not found in pod's containers", "containerID", filterContainerID)
                return containerStatusbyCreatedList{}
        }</span>

        // Find the exited containers whose name matches the name of the container with id being filterContainerId
        <span class="cov0" title="0">var candidates containerStatusbyCreatedList
        for _, containerStatus := range podStatus.ContainerStatuses </span><span class="cov0" title="0">{
                if containerStatus.State != kubecontainer.ContainerStateExited </span><span class="cov0" title="0">{
                        continue</span>
                }
                <span class="cov0" title="0">if matchedContainer == nil || matchedContainer.Name == containerStatus.Name </span><span class="cov0" title="0">{
                        candidates = append(candidates, containerStatus)
                }</span>
        }

        <span class="cov0" title="0">if len(candidates) &lt;= containersToKeep </span><span class="cov0" title="0">{
                return containerStatusbyCreatedList{}
        }</span>
        <span class="cov0" title="0">sort.Sort(candidates)
        return candidates[containersToKeep:]</span>
}

// deleteContainersInPod issues container deletion requests for containers selected by getContainersToDeleteInPod.
func (p *podContainerDeletor) deleteContainersInPod(filterContainerID string, podStatus *kubecontainer.PodStatus, removeAll bool) <span class="cov0" title="0">{
        containersToKeep := p.containersToKeep
        if removeAll </span><span class="cov0" title="0">{
                containersToKeep = 0
                filterContainerID = ""
        }</span>

        <span class="cov0" title="0">for _, candidate := range getContainersToDeleteInPod(filterContainerID, podStatus, containersToKeep) </span><span class="cov0" title="0">{
                select </span>{
                case p.worker &lt;- candidate.ID:<span class="cov0" title="0"></span>
                default:<span class="cov0" title="0">
                        klog.InfoS("Failed to issue the request to remove container", "containerID", candidate.ID)</span>
                }
        }
}
</pre>
		
		<pre class="file" id="file13" style="display: none">/*
Copyright 2014 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "context"
        "fmt"
        "strings"
        "sync"
        "time"

        v1 "k8s.io/api/core/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/util/runtime"
        "k8s.io/apimachinery/pkg/util/wait"
        "k8s.io/client-go/tools/record"
        runtimeapi "k8s.io/cri-api/pkg/apis/runtime/v1"
        "k8s.io/klog/v2"
        kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
        "k8s.io/kubernetes/pkg/kubelet/events"
        "k8s.io/kubernetes/pkg/kubelet/eviction"
        "k8s.io/kubernetes/pkg/kubelet/metrics"
        kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
        "k8s.io/kubernetes/pkg/kubelet/util/queue"
        "k8s.io/utils/clock"
)

// OnCompleteFunc is a function that is invoked when an operation completes.
// If err is non-nil, the operation did not complete successfully.
type OnCompleteFunc func(err error)

// PodStatusFunc is a function that is invoked to override the pod status when a pod is killed.
type PodStatusFunc func(podStatus *v1.PodStatus)

// KillPodOptions are options when performing a pod update whose update type is kill.
type KillPodOptions struct {
        // CompletedCh is closed when the kill request completes (syncTerminatingPod has completed
        // without error) or if the pod does not exist, or if the pod has already terminated. This
        // could take an arbitrary amount of time to be closed, but is never left open once
        // CouldHaveRunningContainers() returns false.
        CompletedCh chan&lt;- struct{}
        // Evict is true if this is a pod triggered eviction - once a pod is evicted some resources are
        // more aggressively reaped than during normal pod operation (stopped containers).
        Evict bool
        // PodStatusFunc is invoked (if set) and overrides the status of the pod at the time the pod is killed.
        // The provided status is populated from the latest state.
        PodStatusFunc PodStatusFunc
        // PodTerminationGracePeriodSecondsOverride is optional override to use if a pod is being killed as part of kill operation.
        PodTerminationGracePeriodSecondsOverride *int64
}

// UpdatePodOptions is an options struct to pass to a UpdatePod operation.
type UpdatePodOptions struct {
        // The type of update (create, update, sync, kill).
        UpdateType kubetypes.SyncPodType
        // StartTime is an optional timestamp for when this update was created. If set,
        // when this update is fully realized by the pod worker it will be recorded in
        // the PodWorkerDuration metric.
        StartTime time.Time
        // Pod to update. Required.
        Pod *v1.Pod
        // MirrorPod is the mirror pod if Pod is a static pod. Optional when UpdateType
        // is kill or terminated.
        MirrorPod *v1.Pod
        // RunningPod is a runtime pod that is no longer present in config. Required
        // if Pod is nil, ignored if Pod is set.
        RunningPod *kubecontainer.Pod
        // KillPodOptions is used to override the default termination behavior of the
        // pod or to update the pod status after an operation is completed. Since a
        // pod can be killed for multiple reasons, PodStatusFunc is invoked in order
        // and later kills have an opportunity to override the status (i.e. a preemption
        // may be later turned into an eviction).
        KillPodOptions *KillPodOptions
}

// PodWorkType classifies the status of pod as seen by the pod worker - setup (sync),
// teardown of containers (terminating), or cleanup (terminated).
type PodWorkerState int

const (
        // SyncPod is when the pod is expected to be started and running.
        SyncPod PodWorkerState = iota
        // TerminatingPod is when the pod is no longer being set up, but some
        // containers may be running and are being torn down.
        TerminatingPod
        // TerminatedPod indicates the pod is stopped, can have no more running
        // containers, and any foreground cleanup can be executed.
        TerminatedPod
)

func (state PodWorkerState) String() string <span class="cov8" title="1">{
        switch state </span>{
        case SyncPod:<span class="cov8" title="1">
                return "sync"</span>
        case TerminatingPod:<span class="cov8" title="1">
                return "terminating"</span>
        case TerminatedPod:<span class="cov8" title="1">
                return "terminated"</span>
        default:<span class="cov0" title="0">
                panic(fmt.Sprintf("the state %d is not defined", state))</span>
        }
}

// PodWorkerSync is the summarization of a single pod worker for sync. Values
// besides state are used to provide metric counts for operators.
type PodWorkerSync struct {
        // State of the pod.
        State PodWorkerState
        // Orphan is true if the pod is no longer in the desired set passed to SyncKnownPods.
        Orphan bool
        // HasConfig is true if we have a historical pod spec for this pod.
        HasConfig bool
        // Static is true if we have config and the pod came from a static source.
        Static bool
}

// podWork is the internal changes
type podWork struct {
        // WorkType is the type of sync to perform - sync (create), terminating (stop
        // containers), terminated (clean up and write status).
        WorkType PodWorkerState

        // Options contains the data to sync.
        Options UpdatePodOptions
}

// PodWorkers is an abstract interface for testability.
type PodWorkers interface {
        // UpdatePod notifies the pod worker of a change to a pod, which will then
        // be processed in FIFO order by a goroutine per pod UID. The state of the
        // pod will be passed to the syncPod method until either the pod is marked
        // as deleted, it reaches a terminal phase (Succeeded/Failed), or the pod
        // is evicted by the kubelet. Once that occurs the syncTerminatingPod method
        // will be called until it exits successfully, and after that all further
        // UpdatePod() calls will be ignored for that pod until it has been forgotten
        // due to significant time passing. A pod that is terminated will never be
        // restarted.
        UpdatePod(options UpdatePodOptions)
        // SyncKnownPods removes workers for pods that are not in the desiredPods set
        // and have been terminated for a significant period of time. Once this method
        // has been called once, the workers are assumed to be fully initialized and
        // subsequent calls to ShouldPodContentBeRemoved on unknown pods will return
        // true. It returns a map describing the state of each known pod worker. It
        // is the responsibility of the caller to re-add any desired pods that are not
        // returned as knownPods.
        SyncKnownPods(desiredPods []*v1.Pod) (knownPods map[types.UID]PodWorkerSync)

        // IsPodKnownTerminated returns true once SyncTerminatingPod completes
        // successfully - the provided pod UID it is known by the pod
        // worker to be terminated. If the pod has been force deleted and the pod worker
        // has completed termination this method will return false, so this method should
        // only be used to filter out pods from the desired set such as in admission.
        //
        // Intended for use by the kubelet config loops, but not subsystems, which should
        // use ShouldPod*().
        IsPodKnownTerminated(uid types.UID) bool
        // CouldHaveRunningContainers returns true before the pod workers have synced,
        // once the pod workers see the pod (syncPod could be called), and returns false
        // after the pod has been terminated (running containers guaranteed stopped).
        //
        // Intended for use by the kubelet config loops, but not subsystems, which should
        // use ShouldPod*().
        CouldHaveRunningContainers(uid types.UID) bool

        // ShouldPodBeFinished returns true once SyncTerminatedPod completes
        // successfully - the provided pod UID it is known to the pod worker to
        // be terminated and have resources reclaimed. It returns false before the
        // pod workers have synced (syncPod could be called). Once the pod workers
        // have synced it returns false if the pod has a sync status until
        // SyncTerminatedPod completes successfully. If the pod workers have synced,
        // but the pod does not have a status it returns true.
        //
        // Intended for use by subsystem sync loops to avoid performing background setup
        // after termination has been requested for a pod. Callers must ensure that the
        // syncPod method is non-blocking when their data is absent.
        ShouldPodBeFinished(uid types.UID) bool
        // IsPodTerminationRequested returns true when pod termination has been requested
        // until the termination completes and the pod is removed from config. This should
        // not be used in cleanup loops because it will return false if the pod has already
        // been cleaned up - use ShouldPodContainersBeTerminating instead. Also, this method
        // may return true while containers are still being initialized by the pod worker.
        //
        // Intended for use by the kubelet sync* methods, but not subsystems, which should
        // use ShouldPod*().
        IsPodTerminationRequested(uid types.UID) bool

        // ShouldPodContainersBeTerminating returns false before pod workers have synced,
        // or once a pod has started terminating. This check is similar to
        // ShouldPodRuntimeBeRemoved but is also true after pod termination is requested.
        //
        // Intended for use by subsystem sync loops to avoid performing background setup
        // after termination has been requested for a pod. Callers must ensure that the
        // syncPod method is non-blocking when their data is absent.
        ShouldPodContainersBeTerminating(uid types.UID) bool
        // ShouldPodRuntimeBeRemoved returns true if runtime managers within the Kubelet
        // should aggressively cleanup pod resources that are not containers or on disk
        // content, like attached volumes. This is true when a pod is not yet observed
        // by a worker after the first sync (meaning it can't be running yet) or after
        // all running containers are stopped.
        // TODO: Once pod logs are separated from running containers, this method should
        // be used to gate whether containers are kept.
        //
        // Intended for use by subsystem sync loops to know when to start tearing down
        // resources that are used by running containers. Callers should ensure that
        // runtime content they own is not required for post-termination - for instance
        // containers are required in docker to preserve pod logs until after the pod
        // is deleted.
        ShouldPodRuntimeBeRemoved(uid types.UID) bool
        // ShouldPodContentBeRemoved returns true if resource managers within the Kubelet
        // should aggressively cleanup all content related to the pod. This is true
        // during pod eviction (when we wish to remove that content to free resources)
        // as well as after the request to delete a pod has resulted in containers being
        // stopped (which is a more graceful action). Note that a deleting pod can still
        // be evicted.
        //
        // Intended for use by subsystem sync loops to know when to start tearing down
        // resources that are used by non-deleted pods. Content is generally preserved
        // until deletion+removal_from_etcd or eviction, although garbage collection
        // can free content when this method returns false.
        ShouldPodContentBeRemoved(uid types.UID) bool
        // IsPodForMirrorPodTerminatingByFullName returns true if a static pod with the
        // provided pod name is currently terminating and has yet to complete. It is
        // intended to be used only during orphan mirror pod cleanup to prevent us from
        // deleting a terminating static pod from the apiserver before the pod is shut
        // down.
        IsPodForMirrorPodTerminatingByFullName(podFullname string) bool
}

// podSyncer describes the core lifecyle operations of the pod state machine. A pod is first
// synced until it naturally reaches termination (true is returned) or an external agent decides
// the pod should be terminated. Once a pod should be terminating, SyncTerminatingPod is invoked
// until it returns no error. Then the SyncTerminatedPod method is invoked until it exits without
// error, and the pod is considered terminal. Implementations of this interface must be threadsafe
// for simultaneous invocation of these methods for multiple pods.
type podSyncer interface {
        // SyncPod configures the pod and starts and restarts all containers. If it returns true, the
        // pod has reached a terminal state and the presence of the error indicates succeeded or failed.
        // If an error is returned, the sync was not successful and should be rerun in the future. This
        // is a long running method and should exit early with context.Canceled if the context is canceled.
        SyncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod *v1.Pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, error)
        // SyncTerminatingPod attempts to ensure the pod's containers are no longer running and to collect
        // any final status. This method is repeatedly invoked with diminishing grace periods until it exits
        // without error. Once this method exits with no error other components are allowed to tear down
        // supporting resources like volumes and devices. If the context is canceled, the method should
        // return context.Canceled unless it has successfully finished, which may occur when a shorter
        // grace period is detected.
        SyncTerminatingPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus, gracePeriod *int64, podStatusFn func(*v1.PodStatus)) error
        // SyncTerminatingRuntimePod is invoked when running containers are found that correspond to
        // a pod that is no longer known to the kubelet to terminate those containers. It should not
        // exit without error unless all containers are known to be stopped.
        SyncTerminatingRuntimePod(ctx context.Context, runningPod *kubecontainer.Pod) error
        // SyncTerminatedPod is invoked after all running containers are stopped and is responsible
        // for releasing resources that should be executed right away rather than in the background.
        // Once it exits without error the pod is considered finished on the node.
        SyncTerminatedPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus) error
}

// SyncPodFnType represents a function type for syncing pods.
type SyncPodFnType func(ctx context.Context, updateType kubetypes.SyncPodType, pod *v1.Pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, error)
type syncPodFnType func(ctx context.Context, updateType kubetypes.SyncPodType, pod *v1.Pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, error)
type syncTerminatingPodFnType func(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus, gracePeriod *int64, podStatusFn func(*v1.PodStatus)) error
type syncTerminatingRuntimePodFnType func(ctx context.Context, runningPod *kubecontainer.Pod) error
type syncTerminatedPodFnType func(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus) error

// podSyncerFuncs implements podSyncer and accepts functions for each method.
type podSyncerFuncs struct {
        syncPod                   syncPodFnType
        syncTerminatingPod        syncTerminatingPodFnType
        syncTerminatingRuntimePod syncTerminatingRuntimePodFnType
        syncTerminatedPod         syncTerminatedPodFnType
}

type PodSyncerFuncs struct {
        syncPod                   syncPodFnType
        syncTerminatingPod        syncTerminatingPodFnType
        syncTerminatingRuntimePod syncTerminatingRuntimePodFnType
        syncTerminatedPod         syncTerminatedPodFnType
}

func newPodSyncerFuncs(s podSyncer) podSyncerFuncs <span class="cov0" title="0">{
        return podSyncerFuncs{
                syncPod:                   s.SyncPod,
                syncTerminatingPod:        s.SyncTerminatingPod,
                syncTerminatingRuntimePod: s.SyncTerminatingRuntimePod,
                syncTerminatedPod:         s.SyncTerminatedPod,
        }
}</span>

var _ podSyncer = podSyncerFuncs{}

func (f podSyncerFuncs) SyncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod *v1.Pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, error) <span class="cov0" title="0">{
        return f.syncPod(ctx, updateType, pod, mirrorPod, podStatus)
}</span>
func (f podSyncerFuncs) SyncTerminatingPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus, gracePeriod *int64, podStatusFn func(*v1.PodStatus)) error <span class="cov0" title="0">{
        return f.syncTerminatingPod(ctx, pod, podStatus, gracePeriod, podStatusFn)
}</span>
func (f podSyncerFuncs) SyncTerminatingRuntimePod(ctx context.Context, runningPod *kubecontainer.Pod) error <span class="cov0" title="0">{
        return f.syncTerminatingRuntimePod(ctx, runningPod)
}</span>
func (f podSyncerFuncs) SyncTerminatedPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus) error <span class="cov0" title="0">{
        return f.syncTerminatedPod(ctx, pod, podStatus)
}</span>

const (
        // jitter factor for resyncInterval
        workerResyncIntervalJitterFactor = 0.5

        // jitter factor for backOffPeriod and backOffOnTransientErrorPeriod
        workerBackOffPeriodJitterFactor = 0.5

        // backoff period when transient error occurred.
        backOffOnTransientErrorPeriod = time.Second
)

// podSyncStatus tracks per-pod transitions through the three phases of pod
// worker sync (setup, terminating, terminated).
type podSyncStatus struct {
        // ctx is the context that is associated with the current pod sync.
        // TODO: remove this from the struct by having the context initialized
        // in startPodSync, the cancelFn used by UpdatePod, and cancellation of
        // a parent context for tearing down workers (if needed) on shutdown
        ctx context.Context
        // cancelFn if set is expected to cancel the current podSyncer operation.
        cancelFn context.CancelFunc

        // fullname of the pod
        fullname string

        // working is true if an update is pending or being worked by a pod worker
        // goroutine.
        working bool
        // pendingUpdate is the updated state the pod worker should observe. It is
        // cleared and moved to activeUpdate when a pod worker reads it. A new update
        // may always replace a pending update as the pod worker does not guarantee
        // that all intermediate states are synced to a worker, only the most recent.
        // This state will not be visible to downstream components until a pod worker
        // has begun processing it.
        pendingUpdate *UpdatePodOptions
        // activeUpdate is the most recent version of the pod's state that will be
        // passed to a sync*Pod function. A pod becomes visible to downstream components
        // once a worker decides to start a pod (startedAt is set). The pod and mirror
        // pod fields are accumulated if they are missing on a particular call (the last
        // known version), and the value of KillPodOptions is accumulated as pods cannot
        // have their grace period shortened. This is the source of truth for the pod spec
        // the kubelet is reconciling towards for all components that act on running pods.
        activeUpdate *UpdatePodOptions

        // syncedAt is the time at which the pod worker first observed this pod.
        syncedAt time.Time
        // startedAt is the time at which the pod worker allowed the pod to start.
        startedAt time.Time
        // terminatingAt is set once the pod is requested to be killed - note that
        // this can be set before the pod worker starts terminating the pod, see
        // terminating.
        terminatingAt time.Time
        // terminatedAt is set once the pod worker has completed a successful
        // syncTerminatingPod call and means all running containers are stopped.
        terminatedAt time.Time
        // gracePeriod is the requested gracePeriod once terminatingAt is nonzero.
        gracePeriod int64
        // notifyPostTerminating will be closed once the pod transitions to
        // terminated. After the pod is in terminated state, nothing should be
        // added to this list.
        notifyPostTerminating []chan&lt;- struct{}
        // statusPostTerminating is a list of the status changes associated
        // with kill pod requests. After the pod is in terminated state, nothing
        // should be added to this list. The worker will execute the last function
        // in this list on each termination attempt.
        statusPostTerminating []PodStatusFunc

        // startedTerminating is true once the pod worker has observed the request to
        // stop a pod (exited syncPod and observed a podWork with WorkType
        // TerminatingPod). Once this is set, it is safe for other components
        // of the kubelet to assume that no other containers may be started.
        startedTerminating bool
        // deleted is true if the pod has been marked for deletion on the apiserver
        // or has no configuration represented (was deleted before).
        deleted bool
        // evicted is true if the kill indicated this was an eviction (an evicted
        // pod can be more aggressively cleaned up).
        evicted bool
        // finished is true once the pod worker completes for a pod
        // (syncTerminatedPod exited with no errors) until SyncKnownPods is invoked
        // to remove the pod. A terminal pod (Succeeded/Failed) will have
        // termination status until the pod is deleted.
        finished bool
        // restartRequested is true if the pod worker was informed the pod is
        // expected to exist (update type of create, update, or sync) after
        // it has been killed. When known pods are synced, any pod that is
        // terminated and has restartRequested will have its history cleared.
        restartRequested bool
        // observedRuntime is true if the pod has been observed to be present in the
        // runtime. A pod that has been observed at runtime must go through either
        // SyncTerminatingRuntimePod or SyncTerminatingPod. Otherwise, we can avoid
        // invoking the terminating methods if the pod is deleted or orphaned before
        // it has been started.
        observedRuntime bool
}

func (s *podSyncStatus) IsWorking() bool              <span class="cov0" title="0">{ return s.working }</span>
func (s *podSyncStatus) IsTerminationRequested() bool <span class="cov0" title="0">{ return !s.terminatingAt.IsZero() }</span>
func (s *podSyncStatus) IsTerminationStarted() bool   <span class="cov0" title="0">{ return s.startedTerminating }</span>
func (s *podSyncStatus) IsTerminated() bool           <span class="cov0" title="0">{ return !s.terminatedAt.IsZero() }</span>
func (s *podSyncStatus) IsFinished() bool             <span class="cov0" title="0">{ return s.finished }</span>
func (s *podSyncStatus) IsEvicted() bool              <span class="cov0" title="0">{ return s.evicted }</span>
func (s *podSyncStatus) IsDeleted() bool              <span class="cov0" title="0">{ return s.deleted }</span>
func (s *podSyncStatus) IsStarted() bool              <span class="cov0" title="0">{ return !s.startedAt.IsZero() }</span>

// WorkType returns this pods' current state of the pod in pod lifecycle state machine.
func (s *podSyncStatus) WorkType() PodWorkerState <span class="cov0" title="0">{
        if s.IsTerminated() </span><span class="cov0" title="0">{
                return TerminatedPod
        }</span>
        <span class="cov0" title="0">if s.IsTerminationRequested() </span><span class="cov0" title="0">{
                return TerminatingPod
        }</span>
        <span class="cov0" title="0">return SyncPod</span>
}

// mergeLastUpdate records the most recent state from a new update. Pod and MirrorPod are
// incremented. KillPodOptions is accumulated. If RunningPod is set, Pod is synthetic and
// will *not* be used as the last pod state unless no previous pod state exists (because
// the pod worker may be responsible for terminating a pod from a previous run of the
// kubelet where no config state is visible). The contents of activeUpdate are used as the
// source of truth for components downstream of the pod workers.
func (s *podSyncStatus) mergeLastUpdate(other UpdatePodOptions) <span class="cov0" title="0">{
        opts := s.activeUpdate
        if opts == nil </span><span class="cov0" title="0">{
                opts = &amp;UpdatePodOptions{}
                s.activeUpdate = opts
        }</span>

        // UpdatePodOptions states (and UpdatePod enforces) that either Pod or RunningPod
        // is set, and we wish to preserve the most recent Pod we have observed, so only
        // overwrite our Pod when we have no Pod or when RunningPod is nil.
        <span class="cov0" title="0">if opts.Pod == nil || other.RunningPod == nil </span><span class="cov0" title="0">{
                opts.Pod = other.Pod
        }</span>
        // running pods will not persist but will be remembered for replay
        <span class="cov0" title="0">opts.RunningPod = other.RunningPod
        // if mirrorPod was not provided, remember the last one for replay
        if other.MirrorPod != nil </span><span class="cov0" title="0">{
                opts.MirrorPod = other.MirrorPod
        }</span>
        // accumulate kill pod options
        <span class="cov0" title="0">if other.KillPodOptions != nil </span><span class="cov0" title="0">{
                opts.KillPodOptions = &amp;KillPodOptions{}
                if other.KillPodOptions.Evict </span><span class="cov0" title="0">{
                        opts.KillPodOptions.Evict = true
                }</span>
                <span class="cov0" title="0">if override := other.KillPodOptions.PodTerminationGracePeriodSecondsOverride; override != nil </span><span class="cov0" title="0">{
                        value := *override
                        opts.KillPodOptions.PodTerminationGracePeriodSecondsOverride = &amp;value
                }</span>
        }
        // StartTime is not copied - that is purely for tracking latency of config propagation
        // from kubelet to pod worker.
}

// podWorkers keeps track of operations on pods and ensures each pod is
// reconciled with the container runtime and other subsystems. The worker
// also tracks which pods are in flight for starting, which pods are
// shutting down but still have running containers, and which pods have
// terminated recently and are guaranteed to have no running containers.
//
// podWorkers is the source of truth for what pods should be active on a
// node at any time, and is kept up to date with the desired state of the
// node (tracked by the kubelet pod config loops and the state in the
// kubelet's podManager) via the UpdatePod method. Components that act
// upon running pods should look to the pod worker for state instead of the
// kubelet podManager. The pod worker is periodically reconciled with the
// state of the podManager via SyncKnownPods() and is responsible for
// ensuring the completion of all observed pods no longer present in
// the podManager (no longer part of the node's desired config).
//
// A pod passed to a pod worker is either being synced (expected to be
// running), terminating (has running containers but no new containers are
// expected to start), terminated (has no running containers but may still
// have resources being consumed), or cleaned up (no resources remaining).
// Once a pod is set to be "torn down" it cannot be started again for that
// UID (corresponding to a delete or eviction) until:
//
//  1. The pod worker is finalized (syncTerminatingPod and
//     syncTerminatedPod exit without error sequentially)
//  2. The SyncKnownPods method is invoked by kubelet housekeeping and the pod
//     is not part of the known config.
//
// Pod workers provide a consistent source of information to other kubelet
// loops about the status of the pod and whether containers can be
// running. The ShouldPodContentBeRemoved() method tracks whether a pod's
// contents should still exist, which includes non-existent pods after
// SyncKnownPods() has been called once (as per the contract, all existing
// pods should be provided via UpdatePod before SyncKnownPods is invoked).
// Generally other sync loops are expected to separate "setup" and
// "teardown" responsibilities and the information methods here assist in
// each by centralizing that state. A simple visualization of the time
// intervals involved might look like:
//
// ---|                                         = kubelet config has synced at least once
// -------|                                  |- = pod exists in apiserver config
// --------|                  |---------------- = CouldHaveRunningContainers() is true
//
//        ^- pod is observed by pod worker  .
//        .                                 .
//
// ----------|       |------------------------- = syncPod is running
//
//        . ^- pod worker loop sees change and invokes syncPod
//        . .                               .
//
// --------------|                     |------- = ShouldPodContainersBeTerminating() returns true
// --------------|                     |------- = IsPodTerminationRequested() returns true (pod is known)
//
//        . .   ^- Kubelet evicts pod       .
//        . .                               .
//
// -------------------|       |---------------- = syncTerminatingPod runs then exits without error
//
//                . .        ^ pod worker loop exits syncPod, sees pod is terminating,
//                                         . .          invokes syncTerminatingPod
//                . .                               .
//
// ---|    |------------------|              .  = ShouldPodRuntimeBeRemoved() returns true (post-sync)
//
//        .                ^ syncTerminatingPod has exited successfully
//        .                               .
//
// ----------------------------|       |------- = syncTerminatedPod runs then exits without error
//
//        .                         ^ other loops can tear down
//        .                               .
//
// ------------------------------------|  |---- = status manager is waiting for SyncTerminatedPod() finished
//
//        .                         ^     .
//
// ----------|                               |- = status manager can be writing pod status
//
//        ^ status manager deletes pod because no longer exists in config
//
// Other components in the Kubelet can request a termination of the pod
// via the UpdatePod method or the killPodNow wrapper - this will ensure
// the components of the pod are stopped until the kubelet is restarted
// or permanently (if the phase of the pod is set to a terminal phase
// in the pod status change).
type podWorkers struct {
        // Protects all per worker fields.
        podLock sync.Mutex
        // podsSynced is true once the pod worker has been synced at least once,
        // which means that all working pods have been started via UpdatePod().
        podsSynced bool

        // Tracks all running per-pod goroutines - per-pod goroutine will be
        // processing updates received through its corresponding channel. Sending
        // a message on this channel will signal the corresponding goroutine to
        // consume podSyncStatuses[uid].pendingUpdate if set.
        podUpdates map[types.UID]chan struct{}
        // Tracks by UID the termination status of a pod - syncing, terminating,
        // terminated, and evicted.
        podSyncStatuses map[types.UID]*podSyncStatus

        // Tracks all uids for started static pods by full name
        startedStaticPodsByFullname map[string]types.UID
        // Tracks all uids for static pods that are waiting to start by full name
        waitingToStartStaticPodsByFullname map[string][]types.UID

        workQueue queue.WorkQueue

        // This function is run to sync the desired state of pod.
        // NOTE: This function has to be thread-safe - it can be called for
        // different pods at the same time.
        podSyncer podSyncer

        // workerChannelFn is exposed for testing to allow unit tests to impose delays
        // in channel communication. The function is invoked once each time a new worker
        // goroutine starts.
        workerChannelFn func(uid types.UID, in chan struct{}) (out &lt;-chan struct{})

        // The EventRecorder to use
        recorder record.EventRecorder

        // backOffPeriod is the duration to back off when there is a sync error.
        backOffPeriod time.Duration

        // resyncInterval is the duration to wait until the next sync.
        resyncInterval time.Duration

        // podCache stores kubecontainer.PodStatus for all pods.
        podCache kubecontainer.Cache

        // clock is used for testing timing
        clock clock.PassiveClock
}

// PodWorkersEx represents an extended version of the PodWorkers struct.
type PodWorkersEx struct {
        // Protects all per worker fields.
        podLock sync.Mutex
        // podsSynced is true once the pod worker has been synced at least once,
        // which means that all working pods have been started via UpdatePod().
        podsSynced bool

        // Tracks all running per-pod goroutines - per-pod goroutine will be
        // processing updates received through its corresponding channel. Sending
        // a message on this channel will signal the corresponding goroutine to
        // consume podSyncStatuses[uid].pendingUpdate if set.
        podUpdates map[types.UID]chan struct{}
        // Tracks by UID the termination status of a pod - syncing, terminating,
        // terminated, and evicted.
        podSyncStatuses map[types.UID]*podSyncStatus

        // Tracks all uids for started static pods by full name
        startedStaticPodsByFullname map[string]types.UID
        // Tracks all uids for static pods that are waiting to start by full name
        waitingToStartStaticPodsByFullname map[string][]types.UID

        workQueue queue.WorkQueue

        // This function is run to sync the desired state of pod.
        // NOTE: This function has to be thread-safe - it can be called for
        // different pods at the same time.
        podSyncer podSyncer

        // workerChannelFn is exposed for testing to allow unit tests to impose delays
        // in channel communication. The function is invoked once each time a new worker
        // goroutine starts.
        workerChannelFn func(uid types.UID, in chan struct{}) (out &lt;-chan struct{})

        // The EventRecorder to use
        recorder record.EventRecorder

        // backOffPeriod is the duration to back off when there is a sync error.
        backOffPeriod time.Duration

        // resyncInterval is the duration to wait until the next sync.
        resyncInterval time.Duration

        // podCache stores kubecontainer.PodStatus for all pods.
        podCache kubecontainer.Cache

        // clock is used for testing timing
        clock clock.PassiveClock
}

// GetClock returns the clock used by the podWorkers.
func (p *PodWorkersEx) GetClock() clock.PassiveClock <span class="cov0" title="0">{
        return p.clock
}</span>

// GetPodLock returns the lock used by the podWorkers.
func (p *PodWorkersEx) GetPodLock() *sync.Mutex <span class="cov0" title="0">{
        return &amp;p.podLock
}</span>

// GetPodSyncStatuses returns the podSyncStatuses used by the podWorkers.
func (p *PodWorkersEx) GetPodSyncStatuses() map[types.UID]*podSyncStatus <span class="cov0" title="0">{
        return p.podSyncStatuses
}</span>

func newPodWorkers(
        podSyncer podSyncer,
        recorder record.EventRecorder,
        workQueue queue.WorkQueue,
        resyncInterval, backOffPeriod time.Duration,
        podCache kubecontainer.Cache,
) PodWorkers <span class="cov8" title="1">{
        return &amp;podWorkers{
                podSyncStatuses:                    map[types.UID]*podSyncStatus{},
                podUpdates:                         map[types.UID]chan struct{}{},
                startedStaticPodsByFullname:        map[string]types.UID{},
                waitingToStartStaticPodsByFullname: map[string][]types.UID{},
                podSyncer:                          podSyncer,
                recorder:                           recorder,
                workQueue:                          workQueue,
                resyncInterval:                     resyncInterval,
                backOffPeriod:                      backOffPeriod,
                podCache:                           podCache,
                clock:                              clock.RealClock{},
        }
}</span>

// NewPodWorkers returns a new instance of PodWorkers.
func NewPodWorkers(
        podSyncer podSyncer,
        recorder record.EventRecorder,
        workQueue queue.WorkQueue,
        resyncInterval, backOffPeriod time.Duration,
        podCache kubecontainer.Cache,
) PodWorkers <span class="cov0" title="0">{
        return &amp;podWorkers{
                podSyncStatuses:                    map[types.UID]*podSyncStatus{},
                podUpdates:                         map[types.UID]chan struct{}{},
                startedStaticPodsByFullname:        map[string]types.UID{},
                waitingToStartStaticPodsByFullname: map[string][]types.UID{},
                podSyncer:                          podSyncer,
                recorder:                           recorder,
                workQueue:                          workQueue,
                resyncInterval:                     resyncInterval,
                backOffPeriod:                      backOffPeriod,
                podCache:                           podCache,
                clock:                              clock.RealClock{},
        }
}</span>

func (p *podWorkers) IsPodKnownTerminated(uid types.UID) bool <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()
        if status, ok := p.podSyncStatuses[uid]; ok </span><span class="cov0" title="0">{
                return status.IsTerminated()
        }</span>
        // if the pod is not known, we return false (pod worker is not aware of it)
        <span class="cov0" title="0">return false</span>
}

func (p *podWorkers) CouldHaveRunningContainers(uid types.UID) bool <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()
        if status, ok := p.podSyncStatuses[uid]; ok </span><span class="cov0" title="0">{
                return !status.IsTerminated()
        }</span>
        // once all pods are synced, any pod without sync status is known to not be running.
        <span class="cov0" title="0">return !p.podsSynced</span>
}

func (p *podWorkers) ShouldPodBeFinished(uid types.UID) bool <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()
        if status, ok := p.podSyncStatuses[uid]; ok </span><span class="cov0" title="0">{
                return status.IsFinished()
        }</span>
        // once all pods are synced, any pod without sync status is assumed to
        // have SyncTerminatedPod finished.
        <span class="cov0" title="0">return p.podsSynced</span>
}

func (p *podWorkers) IsPodTerminationRequested(uid types.UID) bool <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()
        if status, ok := p.podSyncStatuses[uid]; ok </span><span class="cov0" title="0">{
                // the pod may still be setting up at this point.
                return status.IsTerminationRequested()
        }</span>
        // an unknown pod is considered not to be terminating (use ShouldPodContainersBeTerminating in
        // cleanup loops to avoid failing to cleanup pods that have already been removed from config)
        <span class="cov0" title="0">return false</span>
}

func (p *podWorkers) ShouldPodContainersBeTerminating(uid types.UID) bool <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()
        if status, ok := p.podSyncStatuses[uid]; ok </span><span class="cov0" title="0">{
                // we wait until the pod worker goroutine observes the termination, which means syncPod will not
                // be executed again, which means no new containers can be started
                return status.IsTerminationStarted()
        }</span>
        // once we've synced, if the pod isn't known to the workers we should be tearing them
        // down
        <span class="cov0" title="0">return p.podsSynced</span>
}

func (p *podWorkers) ShouldPodRuntimeBeRemoved(uid types.UID) bool <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()
        if status, ok := p.podSyncStatuses[uid]; ok </span><span class="cov0" title="0">{
                return status.IsTerminated()
        }</span>
        // a pod that hasn't been sent to the pod worker yet should have no runtime components once we have
        // synced all content.
        <span class="cov0" title="0">return p.podsSynced</span>
}

func (p *podWorkers) ShouldPodContentBeRemoved(uid types.UID) bool <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()
        if status, ok := p.podSyncStatuses[uid]; ok </span><span class="cov0" title="0">{
                return status.IsEvicted() || (status.IsDeleted() &amp;&amp; status.IsTerminated())
        }</span>
        // a pod that hasn't been sent to the pod worker yet should have no content on disk once we have
        // synced all content.
        <span class="cov0" title="0">return p.podsSynced</span>
}

func (p *podWorkers) IsPodForMirrorPodTerminatingByFullName(podFullName string) bool <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()
        uid, started := p.startedStaticPodsByFullname[podFullName]
        if !started </span><span class="cov0" title="0">{
                return false
        }</span>
        <span class="cov0" title="0">status, exists := p.podSyncStatuses[uid]
        if !exists </span><span class="cov0" title="0">{
                return false
        }</span>
        <span class="cov0" title="0">if !status.IsTerminationRequested() || status.IsTerminated() </span><span class="cov0" title="0">{
                return false
        }</span>

        <span class="cov0" title="0">return true</span>
}

func isPodStatusCacheTerminal(status *kubecontainer.PodStatus) bool <span class="cov0" title="0">{
        for _, container := range status.ContainerStatuses </span><span class="cov0" title="0">{
                if container.State == kubecontainer.ContainerStateRunning </span><span class="cov0" title="0">{
                        return false
                }</span>
        }
        <span class="cov0" title="0">for _, sb := range status.SandboxStatuses </span><span class="cov0" title="0">{
                if sb.State == runtimeapi.PodSandboxState_SANDBOX_READY </span><span class="cov0" title="0">{
                        return false
                }</span>
        }
        <span class="cov0" title="0">return true</span>
}

// UpdatePod carries a configuration change or termination state to a pod. A pod is either runnable,
// terminating, or terminated, and will transition to terminating if: deleted on the apiserver,
// discovered to have a terminal phase (Succeeded or Failed), or evicted by the kubelet.
func (p *podWorkers) UpdatePod(options UpdatePodOptions) <span class="cov0" title="0">{
        // Handle when the pod is an orphan (no config) and we only have runtime status by running only
        // the terminating part of the lifecycle. A running pod contains only a minimal set of information
        // about the pod
        var isRuntimePod bool
        var uid types.UID
        var name, ns string
        if runningPod := options.RunningPod; runningPod != nil </span><span class="cov0" title="0">{
                if options.Pod == nil </span><span class="cov0" title="0">{
                        // the sythetic pod created here is used only as a placeholder and not tracked
                        if options.UpdateType != kubetypes.SyncPodKill </span><span class="cov0" title="0">{
                                klog.InfoS("Pod update is ignored, runtime pods can only be killed", "pod", klog.KRef(runningPod.Namespace, runningPod.Name), "podUID", runningPod.ID, "updateType", options.UpdateType)
                                return
                        }</span>
                        <span class="cov0" title="0">uid, ns, name = runningPod.ID, runningPod.Namespace, runningPod.Name
                        isRuntimePod = true</span>
                } else<span class="cov0" title="0"> {
                        options.RunningPod = nil
                        uid, ns, name = options.Pod.UID, options.Pod.Namespace, options.Pod.Name
                        klog.InfoS("Pod update included RunningPod which is only valid when Pod is not specified", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                }</span>
        } else<span class="cov0" title="0"> {
                uid, ns, name = options.Pod.UID, options.Pod.Namespace, options.Pod.Name
        }</span>

        <span class="cov0" title="0">p.podLock.Lock()
        defer p.podLock.Unlock()

        // decide what to do with this pod - we are either setting it up, tearing it down, or ignoring it
        var firstTime bool
        now := p.clock.Now()
        status, ok := p.podSyncStatuses[uid]
        if !ok </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod is being synced for the first time", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                firstTime = true
                status = &amp;podSyncStatus{
                        syncedAt: now,
                        fullname: kubecontainer.BuildPodFullName(name, ns),
                }
                // if this pod is being synced for the first time, we need to make sure it is an active pod
                if options.Pod != nil &amp;&amp; (options.Pod.Status.Phase == v1.PodFailed || options.Pod.Status.Phase == v1.PodSucceeded) </span><span class="cov0" title="0">{
                        // Check to see if the pod is not running and the pod is terminal; if this succeeds then record in the podWorker that it is terminated.
                        // This is needed because after a kubelet restart, we need to ensure terminal pods will NOT be considered active in Pod Admission. See http://issues.k8s.io/105523
                        // However, `filterOutInactivePods`, considers pods that are actively terminating as active. As a result, `IsPodKnownTerminated()` needs to return true and thus `terminatedAt` needs to be set.
                        if statusCache, err := p.podCache.Get(uid); err == nil </span><span class="cov0" title="0">{
                                if isPodStatusCacheTerminal(statusCache) </span><span class="cov0" title="0">{
                                        // At this point we know:
                                        // (1) The pod is terminal based on the config source.
                                        // (2) The pod is terminal based on the runtime cache.
                                        // This implies that this pod had already completed `SyncTerminatingPod` sometime in the past. The pod is likely being synced for the first time due to a kubelet restart.
                                        // These pods need to complete SyncTerminatedPod to ensure that all resources are cleaned and that the status manager makes the final status updates for the pod.
                                        // As a result, set finished: false, to ensure a Terminated event will be sent and `SyncTerminatedPod` will run.
                                        status = &amp;podSyncStatus{
                                                terminatedAt:       now,
                                                terminatingAt:      now,
                                                syncedAt:           now,
                                                startedTerminating: true,
                                                finished:           false,
                                                fullname:           kubecontainer.BuildPodFullName(name, ns),
                                        }
                                }</span>
                        }
                }
                <span class="cov0" title="0">p.podSyncStatuses[uid] = status</span>
        }

        // RunningPods represent an unknown pod execution and don't contain pod spec information
        // sufficient to perform any action other than termination. If we received a RunningPod
        // after a real pod has already been provided, use the most recent spec instead. Also,
        // once we observe a runtime pod we must drive it to completion, even if we weren't the
        // ones who started it.
        <span class="cov0" title="0">pod := options.Pod
        if isRuntimePod </span><span class="cov0" title="0">{
                status.observedRuntime = true
                switch </span>{
                case status.pendingUpdate != nil &amp;&amp; status.pendingUpdate.Pod != nil:<span class="cov0" title="0">
                        pod = status.pendingUpdate.Pod
                        options.Pod = pod
                        options.RunningPod = nil</span>
                case status.activeUpdate != nil &amp;&amp; status.activeUpdate.Pod != nil:<span class="cov0" title="0">
                        pod = status.activeUpdate.Pod
                        options.Pod = pod
                        options.RunningPod = nil</span>
                default:<span class="cov0" title="0">
                        // we will continue to use RunningPod.ToAPIPod() as pod here, but
                        // options.Pod will be nil and other methods must handle that appropriately.
                        pod = options.RunningPod.ToAPIPod()</span>
                }
        }

        // When we see a create update on an already terminating pod, that implies two pods with the same UID were created in
        // close temporal proximity (usually static pod but it's possible for an apiserver to extremely rarely do something
        // similar) - flag the sync status to indicate that after the pod terminates it should be reset to "not running" to
        // allow a subsequent add/update to start the pod worker again. This does not apply to the first time we see a pod,
        // such as when the kubelet restarts and we see already terminated pods for the first time.
        <span class="cov0" title="0">if !firstTime &amp;&amp; status.IsTerminationRequested() </span><span class="cov0" title="0">{
                if options.UpdateType == kubetypes.SyncPodCreate </span><span class="cov0" title="0">{
                        status.restartRequested = true
                        klog.V(4).InfoS("Pod is terminating but has been requested to restart with same UID, will be reconciled later", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                        return
                }</span>
        }

        // once a pod is terminated by UID, it cannot reenter the pod worker (until the UID is purged by housekeeping)
        <span class="cov0" title="0">if status.IsFinished() </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod is finished processing, no further updates", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                return
        }</span>

        // check for a transition to terminating
        <span class="cov0" title="0">var becameTerminating bool
        if !status.IsTerminationRequested() </span><span class="cov0" title="0">{
                switch </span>{
                case isRuntimePod:<span class="cov0" title="0">
                        klog.V(4).InfoS("Pod is orphaned and must be torn down", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                        status.deleted = true
                        status.terminatingAt = now
                        becameTerminating = true</span>
                case pod.DeletionTimestamp != nil:<span class="cov0" title="0">
                        klog.V(4).InfoS("Pod is marked for graceful deletion, begin teardown", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                        status.deleted = true
                        status.terminatingAt = now
                        becameTerminating = true</span>
                case pod.Status.Phase == v1.PodFailed, pod.Status.Phase == v1.PodSucceeded:<span class="cov0" title="0">
                        klog.V(4).InfoS("Pod is in a terminal phase (success/failed), begin teardown", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                        status.terminatingAt = now
                        becameTerminating = true</span>
                case options.UpdateType == kubetypes.SyncPodKill:<span class="cov0" title="0">
                        if options.KillPodOptions != nil &amp;&amp; options.KillPodOptions.Evict </span><span class="cov0" title="0">{
                                klog.V(4).InfoS("Pod is being evicted by the kubelet, begin teardown", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                                status.evicted = true
                        }</span> else<span class="cov0" title="0"> {
                                klog.V(4).InfoS("Pod is being removed by the kubelet, begin teardown", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                        }</span>
                        <span class="cov0" title="0">status.terminatingAt = now
                        becameTerminating = true</span>
                }
        }

        // once a pod is terminating, all updates are kills and the grace period can only decrease
        <span class="cov0" title="0">var wasGracePeriodShortened bool
        switch </span>{
        case status.IsTerminated():<span class="cov0" title="0">
                // A terminated pod may still be waiting for cleanup - if we receive a runtime pod kill request
                // due to housekeeping seeing an older cached version of the runtime pod simply ignore it until
                // after the pod worker completes.
                if isRuntimePod </span><span class="cov0" title="0">{
                        klog.V(3).InfoS("Pod is waiting for termination, ignoring runtime-only kill until after pod worker is fully terminated", "pod", klog.KRef(ns, name), "podUID", uid, "updateType", options.UpdateType)
                        return
                }</span>

                <span class="cov0" title="0">if options.KillPodOptions != nil </span><span class="cov0" title="0">{
                        if ch := options.KillPodOptions.CompletedCh; ch != nil </span><span class="cov0" title="0">{
                                close(ch)
                        }</span>
                }
                <span class="cov0" title="0">options.KillPodOptions = nil</span>

        case status.IsTerminationRequested():<span class="cov0" title="0">
                if options.KillPodOptions == nil </span><span class="cov0" title="0">{
                        options.KillPodOptions = &amp;KillPodOptions{}
                }</span>

                <span class="cov0" title="0">if ch := options.KillPodOptions.CompletedCh; ch != nil </span><span class="cov0" title="0">{
                        status.notifyPostTerminating = append(status.notifyPostTerminating, ch)
                }</span>
                <span class="cov0" title="0">if fn := options.KillPodOptions.PodStatusFunc; fn != nil </span><span class="cov0" title="0">{
                        status.statusPostTerminating = append(status.statusPostTerminating, fn)
                }</span>

                <span class="cov0" title="0">gracePeriod, gracePeriodShortened := calculateEffectiveGracePeriod(status, pod, options.KillPodOptions)

                wasGracePeriodShortened = gracePeriodShortened
                status.gracePeriod = gracePeriod
                // always set the grace period for syncTerminatingPod so we don't have to recalculate,
                // will never be zero.
                options.KillPodOptions.PodTerminationGracePeriodSecondsOverride = &amp;gracePeriod</span>

        default:<span class="cov0" title="0">
                // KillPodOptions is not valid for sync actions outside of the terminating phase
                if options.KillPodOptions != nil </span><span class="cov0" title="0">{
                        if ch := options.KillPodOptions.CompletedCh; ch != nil </span><span class="cov0" title="0">{
                                close(ch)
                        }</span>
                        <span class="cov0" title="0">options.KillPodOptions = nil</span>
                }
        }

        // start the pod worker goroutine if it doesn't exist
        <span class="cov0" title="0">podUpdates, exists := p.podUpdates[uid]
        if !exists </span><span class="cov0" title="0">{
                // buffer the channel to avoid blocking this method
                podUpdates = make(chan struct{}, 1)
                p.podUpdates[uid] = podUpdates

                // ensure that static pods start in the order they are received by UpdatePod
                if kubetypes.IsStaticPod(pod) </span><span class="cov0" title="0">{
                        p.waitingToStartStaticPodsByFullname[status.fullname] =
                                append(p.waitingToStartStaticPodsByFullname[status.fullname], uid)
                }</span>

                // allow testing of delays in the pod update channel
                <span class="cov0" title="0">var outCh &lt;-chan struct{}
                if p.workerChannelFn != nil </span><span class="cov0" title="0">{
                        outCh = p.workerChannelFn(uid, podUpdates)
                }</span> else<span class="cov0" title="0"> {
                        outCh = podUpdates
                }</span>

                // spawn a pod worker
                <span class="cov0" title="0">go func() </span><span class="cov0" title="0">{
                        // TODO: this should be a wait.Until with backoff to handle panics, and
                        // accept a context for shutdown
                        defer runtime.HandleCrash()
                        defer klog.V(3).InfoS("Pod worker has stopped", "podUID", uid)
                        p.podWorkerLoop(uid, outCh)
                }</span>()
        }

        // measure the maximum latency between a call to UpdatePod and when the pod worker reacts to it
        // by preserving the oldest StartTime
        <span class="cov0" title="0">if status.pendingUpdate != nil &amp;&amp; !status.pendingUpdate.StartTime.IsZero() &amp;&amp; status.pendingUpdate.StartTime.Before(options.StartTime) </span><span class="cov0" title="0">{
                options.StartTime = status.pendingUpdate.StartTime
        }</span>

        // notify the pod worker there is a pending update
        <span class="cov0" title="0">status.pendingUpdate = &amp;options
        status.working = true
        klog.V(4).InfoS("Notifying pod of pending update", "pod", klog.KRef(ns, name), "podUID", uid, "workType", status.WorkType())
        select </span>{
        case podUpdates &lt;- struct{}{}:<span class="cov0" title="0"></span>
        default:<span class="cov0" title="0"></span>
        }

        <span class="cov0" title="0">if (becameTerminating || wasGracePeriodShortened) &amp;&amp; status.cancelFn != nil </span><span class="cov0" title="0">{
                klog.V(3).InfoS("Cancelling current pod sync", "pod", klog.KRef(ns, name), "podUID", uid, "workType", status.WorkType())
                status.cancelFn()
                return
        }</span>
}

// calculateEffectiveGracePeriod sets the initial grace period for a newly terminating pod or allows a
// shorter grace period to be provided, returning the desired value.
func calculateEffectiveGracePeriod(status *podSyncStatus, pod *v1.Pod, options *KillPodOptions) (int64, bool) <span class="cov0" title="0">{
        // enforce the restriction that a grace period can only decrease and track whatever our value is,
        // then ensure a calculated value is passed down to lower levels
        gracePeriod := status.gracePeriod
        // this value is bedrock truth - the apiserver owns telling us this value calculated by apiserver
        if override := pod.DeletionGracePeriodSeconds; override != nil </span><span class="cov0" title="0">{
                if gracePeriod == 0 || *override &lt; gracePeriod </span><span class="cov0" title="0">{
                        gracePeriod = *override
                }</span>
        }
        // we allow other parts of the kubelet (namely eviction) to request this pod be terminated faster
        <span class="cov0" title="0">if options != nil </span><span class="cov0" title="0">{
                if override := options.PodTerminationGracePeriodSecondsOverride; override != nil </span><span class="cov0" title="0">{
                        if gracePeriod == 0 || *override &lt; gracePeriod </span><span class="cov0" title="0">{
                                gracePeriod = *override
                        }</span>
                }
        }
        // make a best effort to default this value to the pod's desired intent, in the event
        // the kubelet provided no requested value (graceful termination?)
        <span class="cov0" title="0">if gracePeriod == 0 &amp;&amp; pod.Spec.TerminationGracePeriodSeconds != nil </span><span class="cov0" title="0">{
                gracePeriod = *pod.Spec.TerminationGracePeriodSeconds
        }</span>
        // no matter what, we always supply a grace period of 1
        <span class="cov0" title="0">if gracePeriod &lt; 1 </span><span class="cov0" title="0">{
                gracePeriod = 1
        }</span>
        <span class="cov0" title="0">return gracePeriod, status.gracePeriod != 0 &amp;&amp; status.gracePeriod != gracePeriod</span>
}

// allowPodStart tries to start the pod and returns true if allowed, otherwise
// it requeues the pod and returns false. If the pod will never be able to start
// because data is missing, or the pod was terminated before start, canEverStart
// is false. This method can only be called while holding the pod lock.
func (p *podWorkers) allowPodStart(pod *v1.Pod) (canStart bool, canEverStart bool) <span class="cov0" title="0">{
        if !kubetypes.IsStaticPod(pod) </span><span class="cov0" title="0">{
                // TODO: Do we want to allow non-static pods with the same full name?
                // Note that it may disable the force deletion of pods.
                return true, true
        }</span>
        <span class="cov0" title="0">status, ok := p.podSyncStatuses[pod.UID]
        if !ok </span><span class="cov0" title="0">{
                klog.ErrorS(nil, "Pod sync status does not exist, the worker should not be running", "pod", klog.KObj(pod), "podUID", pod.UID)
                return false, false
        }</span>
        <span class="cov0" title="0">if status.IsTerminationRequested() </span><span class="cov0" title="0">{
                return false, false
        }</span>
        <span class="cov0" title="0">if !p.allowStaticPodStart(status.fullname, pod.UID) </span><span class="cov0" title="0">{
                p.workQueue.Enqueue(pod.UID, wait.Jitter(p.backOffPeriod, workerBackOffPeriodJitterFactor))
                return false, true
        }</span>
        <span class="cov0" title="0">return true, true</span>
}

// allowStaticPodStart tries to start the static pod and returns true if
// 1. there are no other started static pods with the same fullname
// 2. the uid matches that of the first valid static pod waiting to start
func (p *podWorkers) allowStaticPodStart(fullname string, uid types.UID) bool <span class="cov0" title="0">{
        startedUID, started := p.startedStaticPodsByFullname[fullname]
        if started </span><span class="cov0" title="0">{
                return startedUID == uid
        }</span>

        <span class="cov0" title="0">waitingPods := p.waitingToStartStaticPodsByFullname[fullname]
        // TODO: This is O(N) with respect to the number of updates to static pods
        // with overlapping full names, and ideally would be O(1).
        for i, waitingUID := range waitingPods </span><span class="cov0" title="0">{
                // has pod already terminated or been deleted?
                status, ok := p.podSyncStatuses[waitingUID]
                if !ok || status.IsTerminationRequested() || status.IsTerminated() </span><span class="cov0" title="0">{
                        continue</span>
                }
                // another pod is next in line
                <span class="cov0" title="0">if waitingUID != uid </span><span class="cov0" title="0">{
                        p.waitingToStartStaticPodsByFullname[fullname] = waitingPods[i:]
                        return false
                }</span>
                // we are up next, remove ourselves
                <span class="cov0" title="0">waitingPods = waitingPods[i+1:]
                break</span>
        }
        <span class="cov0" title="0">if len(waitingPods) != 0 </span><span class="cov0" title="0">{
                p.waitingToStartStaticPodsByFullname[fullname] = waitingPods
        }</span> else<span class="cov0" title="0"> {
                delete(p.waitingToStartStaticPodsByFullname, fullname)
        }</span>
        <span class="cov0" title="0">p.startedStaticPodsByFullname[fullname] = uid
        return true</span>
}

// cleanupUnstartedPod is invoked if a pod that has never been started receives a termination
// signal before it can be started. This method must be called holding the pod lock.
func (p *podWorkers) cleanupUnstartedPod(pod *v1.Pod, status *podSyncStatus) <span class="cov0" title="0">{
        p.cleanupPodUpdates(pod.UID)

        if status.terminatingAt.IsZero() </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod worker is complete but did not have terminatingAt set, likely programmer error", "pod", klog.KObj(pod), "podUID", pod.UID)
        }</span>
        <span class="cov0" title="0">if !status.terminatedAt.IsZero() </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod worker is complete and had terminatedAt set, likely programmer error", "pod", klog.KObj(pod), "podUID", pod.UID)
        }</span>
        <span class="cov0" title="0">status.finished = true
        status.working = false
        status.terminatedAt = p.clock.Now()

        if p.startedStaticPodsByFullname[status.fullname] == pod.UID </span><span class="cov0" title="0">{
                delete(p.startedStaticPodsByFullname, status.fullname)
        }</span>
}

// startPodSync is invoked by each pod worker goroutine when a message arrives on the pod update channel.
// This method consumes a pending update, initializes a context, decides whether the pod is already started
// or can be started, and updates the cached pod state so that downstream components can observe what the
// pod worker goroutine is currently attempting to do. If ok is false, there is no available event. If any
// of the boolean values is false, ensure the appropriate cleanup happens before returning.
//
// This method should ensure that either status.pendingUpdate is cleared and merged into status.activeUpdate,
// or when a pod cannot be started status.pendingUpdate remains the same. Pods that have not been started
// should never have an activeUpdate because that is exposed to downstream components on started pods.
func (p *podWorkers) startPodSync(podUID types.UID) (ctx context.Context, update podWork, canStart, canEverStart, ok bool) <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()

        // verify we are known to the pod worker still
        status, ok := p.podSyncStatuses[podUID]
        if !ok </span><span class="cov0" title="0">{
                // pod status has disappeared, the worker should exit
                klog.V(4).InfoS("Pod worker no longer has status, worker should exit", "podUID", podUID)
                return nil, update, false, false, false
        }</span>
        <span class="cov0" title="0">if !status.working </span><span class="cov0" title="0">{
                // working is used by unit tests to observe whether a worker is currently acting on this pod
                klog.V(4).InfoS("Pod should be marked as working by the pod worker, programmer error", "podUID", podUID)
        }</span>
        <span class="cov0" title="0">if status.pendingUpdate == nil </span><span class="cov0" title="0">{
                // no update available, this means we were queued without work being added or there is a
                // race condition, both of which are unexpected
                status.working = false
                klog.V(4).InfoS("Pod worker received no pending work, programmer error?", "podUID", podUID)
                return nil, update, false, false, false
        }</span>

        // consume the pending update
        <span class="cov0" title="0">update.WorkType = status.WorkType()
        update.Options = *status.pendingUpdate
        status.pendingUpdate = nil
        select </span>{
        case &lt;-p.podUpdates[podUID]:<span class="cov0" title="0"></span>
                // ensure the pod update channel is empty (it is only ever written to under lock)
        default:<span class="cov0" title="0"></span>
        }

        // initialize a context for the worker if one does not exist
        <span class="cov0" title="0">if status.ctx == nil || status.ctx.Err() == context.Canceled </span><span class="cov0" title="0">{
                status.ctx, status.cancelFn = context.WithCancel(context.Background())
        }</span>
        <span class="cov0" title="0">ctx = status.ctx

        // if we are already started, make our state visible to downstream components
        if status.IsStarted() </span><span class="cov0" title="0">{
                status.mergeLastUpdate(update.Options)
                return ctx, update, true, true, true
        }</span>

        // if we are already terminating and we only have a running pod, allow the worker
        // to "start" since we are immediately moving to terminating
        <span class="cov0" title="0">if update.Options.RunningPod != nil &amp;&amp; update.WorkType == TerminatingPod </span><span class="cov0" title="0">{
                status.mergeLastUpdate(update.Options)
                return ctx, update, true, true, true
        }</span>

        // If we receive an update where Pod is nil (running pod is set) but haven't
        // started yet, we can only terminate the pod, not start it. We should not be
        // asked to start such a pod, but guard here just in case an accident occurs.
        <span class="cov0" title="0">if update.Options.Pod == nil </span><span class="cov0" title="0">{
                status.mergeLastUpdate(update.Options)
                klog.V(4).InfoS("Running pod cannot start ever, programmer error", "pod", klog.KObj(update.Options.Pod), "podUID", podUID, "updateType", update.WorkType)
                return ctx, update, false, false, true
        }</span>

        // verify we can start
        <span class="cov0" title="0">canStart, canEverStart = p.allowPodStart(update.Options.Pod)
        switch </span>{
        case !canEverStart:<span class="cov0" title="0">
                p.cleanupUnstartedPod(update.Options.Pod, status)
                status.working = false
                if start := update.Options.StartTime; !start.IsZero() </span><span class="cov0" title="0">{
                        metrics.PodWorkerDuration.WithLabelValues("terminated").Observe(metrics.SinceInSeconds(start))
                }</span>
                <span class="cov0" title="0">klog.V(4).InfoS("Pod cannot start ever", "pod", klog.KObj(update.Options.Pod), "podUID", podUID, "updateType", update.WorkType)
                return ctx, update, canStart, canEverStart, true</span>
        case !canStart:<span class="cov0" title="0">
                // this is the only path we don't start the pod, so we need to put the change back in pendingUpdate
                status.pendingUpdate = &amp;update.Options
                status.working = false
                klog.V(4).InfoS("Pod cannot start yet", "pod", klog.KObj(update.Options.Pod), "podUID", podUID)
                return ctx, update, canStart, canEverStart, true</span>
        }

        // mark the pod as started
        <span class="cov0" title="0">status.startedAt = p.clock.Now()
        status.mergeLastUpdate(update.Options)

        // If we are admitting the pod and it is new, record the count of containers
        // TODO: We should probably move this into syncPod and add an execution count
        // to the syncPod arguments, and this should be recorded on the first sync.
        // Leaving it here complicates a particularly important loop.
        metrics.ContainersPerPodCount.Observe(float64(len(update.Options.Pod.Spec.Containers)))

        return ctx, update, true, true, true</span>
}

func podUIDAndRefForUpdate(update UpdatePodOptions) (types.UID, klog.ObjectRef) <span class="cov0" title="0">{
        if update.RunningPod != nil </span><span class="cov0" title="0">{
                return update.RunningPod.ID, klog.KObj(update.RunningPod.ToAPIPod())
        }</span>
        <span class="cov0" title="0">return update.Pod.UID, klog.KObj(update.Pod)</span>
}

// podWorkerLoop manages sequential state updates to a pod in a goroutine, exiting once the final
// state is reached. The loop is responsible for driving the pod through four main phases:
//
// 1. Wait to start, guaranteeing no two pods with the same UID or same fullname are running at the same time
// 2. Sync, orchestrating pod setup by reconciling the desired pod spec with the runtime state of the pod
// 3. Terminating, ensuring all running containers in the pod are stopped
// 4. Terminated, cleaning up any resources that must be released before the pod can be deleted
//
// The podWorkerLoop is driven by updates delivered to UpdatePod and by SyncKnownPods. If a particular
// sync method fails, p.workerQueue is updated with backoff but it is the responsibility of the kubelet
// to trigger new UpdatePod calls. SyncKnownPods will only retry pods that are no longer known to the
// caller. When a pod transitions working-&gt;terminating or terminating-&gt;terminated, the next update is
// queued immediately and no kubelet action is required.
func (p *podWorkers) podWorkerLoop(podUID types.UID, podUpdates &lt;-chan struct{}) <span class="cov0" title="0">{
        var lastSyncTime time.Time
        for range podUpdates </span><span class="cov0" title="0">{
                ctx, update, canStart, canEverStart, ok := p.startPodSync(podUID)
                // If we had no update waiting, it means someone initialized the channel without filling out pendingUpdate.
                if !ok </span><span class="cov0" title="0">{
                        continue</span>
                }
                // If the pod was terminated prior to the pod being allowed to start, we exit the loop.
                <span class="cov0" title="0">if !canEverStart </span><span class="cov0" title="0">{
                        return
                }</span>
                // If the pod is not yet ready to start, continue and wait for more updates.
                <span class="cov0" title="0">if !canStart </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov0" title="0">podUID, podRef := podUIDAndRefForUpdate(update.Options)

                klog.V(4).InfoS("Processing pod event", "pod", podRef, "podUID", podUID, "updateType", update.WorkType)
                var isTerminal bool
                err := func() error </span><span class="cov0" title="0">{
                        // The worker is responsible for ensuring the sync method sees the appropriate
                        // status updates on resyncs (the result of the last sync), transitions to
                        // terminating (no wait), or on terminated (whatever the most recent state is).
                        // Only syncing and terminating can generate pod status changes, while terminated
                        // pods ensure the most recent status makes it to the api server.
                        var status *kubecontainer.PodStatus
                        var err error
                        switch </span>{
                        case update.Options.RunningPod != nil:<span class="cov0" title="0"></span>
                                // when we receive a running pod, we don't need status at all because we are
                                // guaranteed to be terminating and we skip updates to the pod
                        default:<span class="cov0" title="0">
                                // wait until we see the next refresh from the PLEG via the cache (max 2s)
                                // TODO: this adds ~1s of latency on all transitions from sync to terminating
                                //  to terminated, and on all termination retries (including evictions). We should
                                //  improve latency by making the pleg continuous and by allowing pod status
                                //  changes to be refreshed when key events happen (killPod, sync-&gt;terminating).
                                //  Improving this latency also reduces the possibility that a terminated
                                //  container's status is garbage collected before we have a chance to update the
                                //  API server (thus losing the exit code).
                                status, err = p.podCache.GetNewerThan(update.Options.Pod.UID, lastSyncTime)

                                if err != nil </span><span class="cov0" title="0">{
                                        // This is the legacy event thrown by manage pod loop all other events are now dispatched
                                        // from syncPodFn
                                        p.recorder.Eventf(update.Options.Pod, v1.EventTypeWarning, events.FailedSync, "error determining status: %v", err)
                                        return err
                                }</span>
                        }

                        // Take the appropriate action (illegal phases are prevented by UpdatePod)
                        <span class="cov0" title="0">switch </span>{
                        case update.WorkType == TerminatedPod:<span class="cov0" title="0">
                                err = p.podSyncer.SyncTerminatedPod(ctx, update.Options.Pod, status)</span>

                        case update.WorkType == TerminatingPod:<span class="cov0" title="0">
                                var gracePeriod *int64
                                if opt := update.Options.KillPodOptions; opt != nil </span><span class="cov0" title="0">{
                                        gracePeriod = opt.PodTerminationGracePeriodSecondsOverride
                                }</span>
                                <span class="cov0" title="0">podStatusFn := p.acknowledgeTerminating(podUID)

                                // if we only have a running pod, terminate it directly
                                if update.Options.RunningPod != nil </span><span class="cov0" title="0">{
                                        err = p.podSyncer.SyncTerminatingRuntimePod(ctx, update.Options.RunningPod)
                                }</span> else<span class="cov0" title="0"> {
                                        err = p.podSyncer.SyncTerminatingPod(ctx, update.Options.Pod, status, gracePeriod, podStatusFn)
                                }</span>

                        default:<span class="cov0" title="0">
                                isTerminal, err = p.podSyncer.SyncPod(ctx, update.Options.UpdateType, update.Options.Pod, update.Options.MirrorPod, status)</span>
                        }

                        <span class="cov0" title="0">lastSyncTime = p.clock.Now()
                        return err</span>
                }()

                <span class="cov0" title="0">var phaseTransition bool
                switch </span>{
                case err == context.Canceled:<span class="cov0" title="0">
                        // when the context is cancelled we expect an update to already be queued
                        klog.V(2).InfoS("Sync exited with context cancellation error", "pod", podRef, "podUID", podUID, "updateType", update.WorkType)</span>

                case err != nil:<span class="cov0" title="0">
                        // we will queue a retry
                        klog.ErrorS(err, "Error syncing pod, skipping", "pod", podRef, "podUID", podUID)</span>

                case update.WorkType == TerminatedPod:<span class="cov0" title="0">
                        // we can shut down the worker
                        p.completeTerminated(podUID)
                        if start := update.Options.StartTime; !start.IsZero() </span><span class="cov0" title="0">{
                                metrics.PodWorkerDuration.WithLabelValues("terminated").Observe(metrics.SinceInSeconds(start))
                        }</span>
                        <span class="cov0" title="0">klog.V(4).InfoS("Processing pod event done", "pod", podRef, "podUID", podUID, "updateType", update.WorkType)
                        return</span>

                case update.WorkType == TerminatingPod:<span class="cov0" title="0">
                        // pods that don't exist in config don't need to be terminated, other loops will clean them up
                        if update.Options.RunningPod != nil </span><span class="cov0" title="0">{
                                p.completeTerminatingRuntimePod(podUID)
                                if start := update.Options.StartTime; !start.IsZero() </span><span class="cov0" title="0">{
                                        metrics.PodWorkerDuration.WithLabelValues(update.Options.UpdateType.String()).Observe(metrics.SinceInSeconds(start))
                                }</span>
                                <span class="cov0" title="0">klog.V(4).InfoS("Processing pod event done", "pod", podRef, "podUID", podUID, "updateType", update.WorkType)
                                return</span>
                        }
                        // otherwise we move to the terminating phase
                        <span class="cov0" title="0">p.completeTerminating(podUID)
                        phaseTransition = true</span>

                case isTerminal:<span class="cov0" title="0">
                        // if syncPod indicated we are now terminal, set the appropriate pod status to move to terminating
                        klog.V(4).InfoS("Pod is terminal", "pod", podRef, "podUID", podUID, "updateType", update.WorkType)
                        p.completeSync(podUID)
                        phaseTransition = true</span>
                }

                // queue a retry if necessary, then put the next event in the channel if any
                <span class="cov0" title="0">p.completeWork(podUID, phaseTransition, err)
                if start := update.Options.StartTime; !start.IsZero() </span><span class="cov0" title="0">{
                        metrics.PodWorkerDuration.WithLabelValues(update.Options.UpdateType.String()).Observe(metrics.SinceInSeconds(start))
                }</span>
                <span class="cov0" title="0">klog.V(4).InfoS("Processing pod event done", "pod", podRef, "podUID", podUID, "updateType", update.WorkType)</span>
        }
}

// acknowledgeTerminating sets the terminating flag on the pod status once the pod worker sees
// the termination state so that other components know no new containers will be started in this
// pod. It then returns the status function, if any, that applies to this pod.
func (p *podWorkers) acknowledgeTerminating(podUID types.UID) PodStatusFunc <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()

        status, ok := p.podSyncStatuses[podUID]
        if !ok </span><span class="cov0" title="0">{
                return nil
        }</span>

        <span class="cov0" title="0">if !status.terminatingAt.IsZero() &amp;&amp; !status.startedTerminating </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod worker has observed request to terminate", "podUID", podUID)
                status.startedTerminating = true
        }</span>

        <span class="cov0" title="0">if l := len(status.statusPostTerminating); l &gt; 0 </span><span class="cov0" title="0">{
                return status.statusPostTerminating[l-1]
        }</span>
        <span class="cov0" title="0">return nil</span>
}

// completeSync is invoked when syncPod completes successfully and indicates the pod is now terminal and should
// be terminated. This happens when the natural pod lifecycle completes - any pod which is not RestartAlways
// exits. Unnatural completions, such as evictions, API driven deletion or phase transition, are handled by
// UpdatePod.
func (p *podWorkers) completeSync(podUID types.UID) <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()

        klog.V(4).InfoS("Pod indicated lifecycle completed naturally and should now terminate", "podUID", podUID)

        status, ok := p.podSyncStatuses[podUID]
        if !ok </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod had no status in completeSync, programmer error?", "podUID", podUID)
                return
        }</span>

        // update the status of the pod
        <span class="cov0" title="0">if status.terminatingAt.IsZero() </span><span class="cov0" title="0">{
                status.terminatingAt = p.clock.Now()
        }</span> else<span class="cov0" title="0"> {
                klog.V(4).InfoS("Pod worker attempted to set terminatingAt twice, likely programmer error", "podUID", podUID)
        }</span>
        <span class="cov0" title="0">status.startedTerminating = true

        // the pod has now transitioned to terminating and we want to run syncTerminatingPod
        // as soon as possible, so if no update is already waiting queue a synthetic update
        p.requeueLastPodUpdate(podUID, status)</span>
}

// completeTerminating is invoked when syncTerminatingPod completes successfully, which means
// no container is running, no container will be started in the future, and we are ready for
// cleanup.  This updates the termination state which prevents future syncs and will ensure
// other kubelet loops know this pod is not running any containers.
func (p *podWorkers) completeTerminating(podUID types.UID) <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()

        klog.V(4).InfoS("Pod terminated all containers successfully", "podUID", podUID)

        status, ok := p.podSyncStatuses[podUID]
        if !ok </span><span class="cov0" title="0">{
                return
        }</span>

        // update the status of the pod
        <span class="cov0" title="0">if status.terminatingAt.IsZero() </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod worker was terminated but did not have terminatingAt set, likely programmer error", "podUID", podUID)
        }</span>
        <span class="cov0" title="0">status.terminatedAt = p.clock.Now()
        for _, ch := range status.notifyPostTerminating </span><span class="cov0" title="0">{
                close(ch)
        }</span>
        <span class="cov0" title="0">status.notifyPostTerminating = nil
        status.statusPostTerminating = nil

        // the pod has now transitioned to terminated and we want to run syncTerminatedPod
        // as soon as possible, so if no update is already waiting queue a synthetic update
        p.requeueLastPodUpdate(podUID, status)</span>
}

// completeTerminatingRuntimePod is invoked when syncTerminatingPod completes successfully,
// which means an orphaned pod (no config) is terminated and we can exit. Since orphaned
// pods have no API representation, we want to exit the loop at this point and ensure no
// status is present afterwards - the running pod is truly terminated when this is invoked.
func (p *podWorkers) completeTerminatingRuntimePod(podUID types.UID) <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()

        klog.V(4).InfoS("Pod terminated all orphaned containers successfully and worker can now stop", "podUID", podUID)

        p.cleanupPodUpdates(podUID)

        status, ok := p.podSyncStatuses[podUID]
        if !ok </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov0" title="0">if status.terminatingAt.IsZero() </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod worker was terminated but did not have terminatingAt set, likely programmer error", "podUID", podUID)
        }</span>
        <span class="cov0" title="0">status.terminatedAt = p.clock.Now()
        status.finished = true
        status.working = false

        if p.startedStaticPodsByFullname[status.fullname] == podUID </span><span class="cov0" title="0">{
                delete(p.startedStaticPodsByFullname, status.fullname)
        }</span>

        // A runtime pod is transient and not part of the desired state - once it has reached
        // terminated we can abandon tracking it.
        <span class="cov0" title="0">delete(p.podSyncStatuses, podUID)</span>
}

// completeTerminated is invoked after syncTerminatedPod completes successfully and means we
// can stop the pod worker. The pod is finalized at this point.
func (p *podWorkers) completeTerminated(podUID types.UID) <span class="cov0" title="0">{
        p.podLock.Lock()
        defer p.podLock.Unlock()

        klog.V(4).InfoS("Pod is complete and the worker can now stop", "podUID", podUID)

        p.cleanupPodUpdates(podUID)

        status, ok := p.podSyncStatuses[podUID]
        if !ok </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov0" title="0">if status.terminatingAt.IsZero() </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod worker is complete but did not have terminatingAt set, likely programmer error", "podUID", podUID)
        }</span>
        <span class="cov0" title="0">if status.terminatedAt.IsZero() </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod worker is complete but did not have terminatedAt set, likely programmer error", "podUID", podUID)
        }</span>
        <span class="cov0" title="0">status.finished = true
        status.working = false

        if p.startedStaticPodsByFullname[status.fullname] == podUID </span><span class="cov0" title="0">{
                delete(p.startedStaticPodsByFullname, status.fullname)
        }</span>
}

// completeWork requeues on error or the next sync interval and then immediately executes any pending
// work.
func (p *podWorkers) completeWork(podUID types.UID, phaseTransition bool, syncErr error) <span class="cov0" title="0">{
        // Requeue the last update if the last sync returned error.
        switch </span>{
        case phaseTransition:<span class="cov0" title="0">
                p.workQueue.Enqueue(podUID, 0)</span>
        case syncErr == nil:<span class="cov0" title="0">
                // No error; requeue at the regular resync interval.
                p.workQueue.Enqueue(podUID, wait.Jitter(p.resyncInterval, workerResyncIntervalJitterFactor))</span>
        case strings.Contains(syncErr.Error(), NetworkNotReadyErrorMsg):<span class="cov0" title="0">
                // Network is not ready; back off for short period of time and retry as network might be ready soon.
                p.workQueue.Enqueue(podUID, wait.Jitter(backOffOnTransientErrorPeriod, workerBackOffPeriodJitterFactor))</span>
        default:<span class="cov0" title="0">
                // Error occurred during the sync; back off and then retry.
                p.workQueue.Enqueue(podUID, wait.Jitter(p.backOffPeriod, workerBackOffPeriodJitterFactor))</span>
        }

        // if there is a pending update for this worker, requeue immediately, otherwise
        // clear working status
        <span class="cov0" title="0">p.podLock.Lock()
        defer p.podLock.Unlock()
        if status, ok := p.podSyncStatuses[podUID]; ok </span><span class="cov0" title="0">{
                if status.pendingUpdate != nil </span><span class="cov0" title="0">{
                        select </span>{
                        case p.podUpdates[podUID] &lt;- struct{}{}:<span class="cov0" title="0">
                                klog.V(4).InfoS("Requeueing pod due to pending update", "podUID", podUID)</span>
                        default:<span class="cov0" title="0">
                                klog.V(4).InfoS("Pending update already queued", "podUID", podUID)</span>
                        }
                } else<span class="cov0" title="0"> {
                        status.working = false
                }</span>
        }
}

// SyncKnownPods will purge any fully terminated pods that are not in the desiredPods
// list, which means SyncKnownPods must be called in a threadsafe manner from calls
// to UpdatePods for new pods. Because the podworker is dependent on UpdatePod being
// invoked to drive a pod's state machine, if a pod is missing in the desired list the
// pod worker must be responsible for delivering that update. The method returns a map
// of known workers that are not finished with a value of SyncPodTerminated,
// SyncPodKill, or SyncPodSync depending on whether the pod is terminated, terminating,
// or syncing.
func (p *podWorkers) SyncKnownPods(desiredPods []*v1.Pod) map[types.UID]PodWorkerSync <span class="cov0" title="0">{
        workers := make(map[types.UID]PodWorkerSync)
        known := make(map[types.UID]struct{})
        for _, pod := range desiredPods </span><span class="cov0" title="0">{
                known[pod.UID] = struct{}{}
        }</span>

        <span class="cov0" title="0">p.podLock.Lock()
        defer p.podLock.Unlock()

        p.podsSynced = true
        for uid, status := range p.podSyncStatuses </span><span class="cov0" title="0">{
                // We retain the worker history of any pod that is still desired according to
                // its UID. However, there are two scenarios during a sync that result in us
                // needing to purge the history:
                //
                // 1. The pod is no longer desired (the local version is orphaned)
                // 2. The pod received a kill update and then a subsequent create, which means
                //    the UID was reused in the source config (vanishingly rare for API servers,
                //    common for static pods that have specified a fixed UID)
                //
                // In the former case we wish to bound the amount of information we store for
                // deleted pods. In the latter case we wish to minimize the amount of time before
                // we restart the static pod. If we succeed at removing the worker, then we
                // omit it from the returned map of known workers, and the caller of SyncKnownPods
                // is expected to send a new UpdatePod({UpdateType: Create}).
                _, knownPod := known[uid]
                orphan := !knownPod
                if status.restartRequested || orphan </span><span class="cov0" title="0">{
                        if p.removeTerminatedWorker(uid, status, orphan) </span><span class="cov0" title="0">{
                                // no worker running, we won't return it
                                continue</span>
                        }
                }

                <span class="cov0" title="0">sync := PodWorkerSync{
                        State:  status.WorkType(),
                        Orphan: orphan,
                }
                switch </span>{
                case status.activeUpdate != nil:<span class="cov0" title="0">
                        if status.activeUpdate.Pod != nil </span><span class="cov0" title="0">{
                                sync.HasConfig = true
                                sync.Static = kubetypes.IsStaticPod(status.activeUpdate.Pod)
                        }</span>
                case status.pendingUpdate != nil:<span class="cov0" title="0">
                        if status.pendingUpdate.Pod != nil </span><span class="cov0" title="0">{
                                sync.HasConfig = true
                                sync.Static = kubetypes.IsStaticPod(status.pendingUpdate.Pod)
                        }</span>
                }
                <span class="cov0" title="0">workers[uid] = sync</span>
        }
        <span class="cov0" title="0">return workers</span>
}

// removeTerminatedWorker cleans up and removes the worker status for a worker
// that has reached a terminal state of "finished" - has successfully exited
// syncTerminatedPod. This "forgets" a pod by UID and allows another pod to be
// recreated with the same UID. The kubelet preserves state about recently
// terminated pods to prevent accidentally restarting a terminal pod, which is
// proportional to the number of pods described in the pod config. The method
// returns true if the worker was completely removed.
func (p *podWorkers) removeTerminatedWorker(uid types.UID, status *podSyncStatus, orphaned bool) bool <span class="cov0" title="0">{
        if !status.finished </span><span class="cov0" title="0">{
                // If the pod worker has not reached terminal state and the pod is still known, we wait.
                if !orphaned </span><span class="cov0" title="0">{
                        klog.V(4).InfoS("Pod worker has been requested for removal but is still not fully terminated", "podUID", uid)
                        return false
                }</span>

                // all orphaned pods are considered deleted
                <span class="cov0" title="0">status.deleted = true

                // When a pod is no longer in the desired set, the pod is considered orphaned and the
                // the pod worker becomes responsible for driving the pod to completion (there is no
                // guarantee another component will notify us of updates).
                switch </span>{
                case !status.IsStarted() &amp;&amp; !status.observedRuntime:<span class="cov0" title="0">
                        // The pod has not been started, which means we can safely clean up the pod - the
                        // pod worker will shutdown as a result of this change without executing a sync.
                        klog.V(4).InfoS("Pod is orphaned and has not been started", "podUID", uid)</span>
                case !status.IsTerminationRequested():<span class="cov0" title="0">
                        // The pod has been started but termination has not been requested - set the appropriate
                        // timestamp and notify the pod worker. Because the pod has been synced at least once,
                        // the value of status.activeUpdate will be the fallback for the next sync.
                        status.terminatingAt = p.clock.Now()
                        if status.activeUpdate != nil &amp;&amp; status.activeUpdate.Pod != nil </span><span class="cov0" title="0">{
                                status.gracePeriod, _ = calculateEffectiveGracePeriod(status, status.activeUpdate.Pod, nil)
                        }</span> else<span class="cov0" title="0"> {
                                status.gracePeriod = 1
                        }</span>
                        <span class="cov0" title="0">p.requeueLastPodUpdate(uid, status)
                        klog.V(4).InfoS("Pod is orphaned and still running, began terminating", "podUID", uid)
                        return false</span>
                default:<span class="cov0" title="0">
                        // The pod is already moving towards termination, notify the pod worker. Because the pod
                        // has been synced at least once, the value of status.activeUpdate will be the fallback for
                        // the next sync.
                        p.requeueLastPodUpdate(uid, status)
                        klog.V(4).InfoS("Pod is orphaned and still terminating, notified the pod worker", "podUID", uid)
                        return false</span>
                }
        }

        <span class="cov0" title="0">if status.restartRequested </span><span class="cov0" title="0">{
                klog.V(4).InfoS("Pod has been terminated but another pod with the same UID was created, remove history to allow restart", "podUID", uid)
        }</span> else<span class="cov0" title="0"> {
                klog.V(4).InfoS("Pod has been terminated and is no longer known to the kubelet, remove all history", "podUID", uid)
        }</span>
        <span class="cov0" title="0">delete(p.podSyncStatuses, uid)
        p.cleanupPodUpdates(uid)

        if p.startedStaticPodsByFullname[status.fullname] == uid </span><span class="cov0" title="0">{
                delete(p.startedStaticPodsByFullname, status.fullname)
        }</span>
        <span class="cov0" title="0">return true</span>
}

// killPodNow returns a KillPodFunc that can be used to kill a pod.
// It is intended to be injected into other modules that need to kill a pod.
func killPodNow(podWorkers PodWorkers, recorder record.EventRecorder) eviction.KillPodFunc <span class="cov8" title="1">{
        return func(pod *v1.Pod, isEvicted bool, gracePeriodOverride *int64, statusFn func(*v1.PodStatus)) error </span><span class="cov0" title="0">{
                // determine the grace period to use when killing the pod
                gracePeriod := int64(0)
                if gracePeriodOverride != nil </span><span class="cov0" title="0">{
                        gracePeriod = *gracePeriodOverride
                }</span> else<span class="cov0" title="0"> if pod.Spec.TerminationGracePeriodSeconds != nil </span><span class="cov0" title="0">{
                        gracePeriod = *pod.Spec.TerminationGracePeriodSeconds
                }</span>

                // we timeout and return an error if we don't get a callback within a reasonable time.
                // the default timeout is relative to the grace period (we settle on 10s to wait for kubelet-&gt;runtime traffic to complete in sigkill)
                <span class="cov0" title="0">timeout := gracePeriod + (gracePeriod / 2)
                minTimeout := int64(10)
                if timeout &lt; minTimeout </span><span class="cov0" title="0">{
                        timeout = minTimeout
                }</span>
                <span class="cov0" title="0">timeoutDuration := time.Duration(timeout) * time.Second

                // open a channel we block against until we get a result
                ch := make(chan struct{}, 1)
                podWorkers.UpdatePod(UpdatePodOptions{
                        Pod:        pod,
                        UpdateType: kubetypes.SyncPodKill,
                        KillPodOptions: &amp;KillPodOptions{
                                CompletedCh:                              ch,
                                Evict:                                    isEvicted,
                                PodStatusFunc:                            statusFn,
                                PodTerminationGracePeriodSecondsOverride: gracePeriodOverride,
                        },
                })

                // wait for either a response, or a timeout
                select </span>{
                case &lt;-ch:<span class="cov0" title="0">
                        return nil</span>
                case &lt;-time.After(timeoutDuration):<span class="cov0" title="0">
                        recorder.Eventf(pod, v1.EventTypeWarning, events.ExceededGracePeriod, "Container runtime did not kill the pod within specified grace period.")
                        return fmt.Errorf("timeout waiting to kill pod")</span>
                }
        }
}

// KillPodNow returns a KillPodFunc that can be used to kill a pod.
func KillPodNow(podWorkers PodWorkers, recorder record.EventRecorder) eviction.KillPodFunc <span class="cov0" title="0">{
        return func(pod *v1.Pod, isEvicted bool, gracePeriodOverride *int64, statusFn func(*v1.PodStatus)) error </span><span class="cov0" title="0">{
                // determine the grace period to use when killing the pod
                gracePeriod := int64(0)
                if gracePeriodOverride != nil </span><span class="cov0" title="0">{
                        gracePeriod = *gracePeriodOverride
                }</span> else<span class="cov0" title="0"> if pod.Spec.TerminationGracePeriodSeconds != nil </span><span class="cov0" title="0">{
                        gracePeriod = *pod.Spec.TerminationGracePeriodSeconds
                }</span>

                // we timeout and return an error if we don't get a callback within a reasonable time.
                // the default timeout is relative to the grace period (we settle on 10s to wait for kubelet-&gt;runtime traffic to complete in sigkill)
                <span class="cov0" title="0">timeout := gracePeriod + (gracePeriod / 2)
                minTimeout := int64(10)
                if timeout &lt; minTimeout </span><span class="cov0" title="0">{
                        timeout = minTimeout
                }</span>
                <span class="cov0" title="0">timeoutDuration := time.Duration(timeout) * time.Second

                // open a channel we block against until we get a result
                ch := make(chan struct{}, 1)
                podWorkers.UpdatePod(UpdatePodOptions{
                        Pod:        pod,
                        UpdateType: kubetypes.SyncPodKill,
                        KillPodOptions: &amp;KillPodOptions{
                                CompletedCh:                              ch,
                                Evict:                                    isEvicted,
                                PodStatusFunc:                            statusFn,
                                PodTerminationGracePeriodSecondsOverride: gracePeriodOverride,
                        },
                })

                // wait for either a response, or a timeout
                select </span>{
                case &lt;-ch:<span class="cov0" title="0">
                        return nil</span>
                case &lt;-time.After(timeoutDuration):<span class="cov0" title="0">
                        recorder.Eventf(pod, v1.EventTypeWarning, events.ExceededGracePeriod, "Container runtime did not kill the pod within specified grace period.")
                        return fmt.Errorf("timeout waiting to kill pod")</span>
                }
        }
}

// cleanupPodUpdates closes the podUpdates channel and removes it from
// podUpdates map so that the corresponding pod worker can stop. It also
// removes any undelivered work. This method must be called holding the
// pod lock.
func (p *podWorkers) cleanupPodUpdates(uid types.UID) <span class="cov0" title="0">{
        if ch, ok := p.podUpdates[uid]; ok </span><span class="cov0" title="0">{
                close(ch)
        }</span>
        <span class="cov0" title="0">delete(p.podUpdates, uid)</span>
}

// requeueLastPodUpdate creates a new pending pod update from the most recently
// executed update if no update is already queued, and then notifies the pod
// worker goroutine of the update. This method must be called while holding
// the pod lock.
func (p *podWorkers) requeueLastPodUpdate(podUID types.UID, status *podSyncStatus) <span class="cov0" title="0">{
        // if there is already an update queued, we can use that instead, or if
        // we have no previously executed update, we cannot replay it.
        if status.pendingUpdate != nil || status.activeUpdate == nil </span><span class="cov0" title="0">{
                return
        }</span>
        <span class="cov0" title="0">copied := *status.activeUpdate
        status.pendingUpdate = &amp;copied

        // notify the pod worker
        status.working = true
        select </span>{
        case p.podUpdates[podUID] &lt;- struct{}{}:<span class="cov0" title="0"></span>
        default:<span class="cov0" title="0"></span>
        }
}
</pre>
		
		<pre class="file" id="file14" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "fmt"
        "sync"

        "github.com/golang/groupcache/lru"
        "k8s.io/apimachinery/pkg/types"
        kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
)

// ReasonCache stores the failure reason of the latest container start
// in a string, keyed by &lt;pod_UID&gt;_&lt;container_name&gt;. The goal is to
// propagate this reason to the container status. This endeavor is
// "best-effort" for two reasons:
//  1. The cache is not persisted.
//  2. We use an LRU cache to avoid extra garbage collection work. This
//     means that some entries may be recycled before a pod has been
//     deleted.
//
// TODO(random-liu): Use more reliable cache which could collect garbage of failed pod.
// TODO(random-liu): Move reason cache to somewhere better.
type ReasonCache struct {
        lock  sync.Mutex
        cache *lru.Cache
}

// ReasonItem is the cached item in ReasonCache
type ReasonItem struct {
        Err     error
        Message string
}

// maxReasonCacheEntries is the cache entry number in lru cache. 1000 is a proper number
// for our 100 pods per node target. If we support more pods per node in the future, we
// may want to increase the number.
const maxReasonCacheEntries = 1000

// NewReasonCache creates an instance of 'ReasonCache'.
func NewReasonCache() *ReasonCache <span class="cov8" title="1">{
        return &amp;ReasonCache{cache: lru.New(maxReasonCacheEntries)}
}</span>

func (c *ReasonCache) composeKey(uid types.UID, name string) string <span class="cov8" title="1">{
        return fmt.Sprintf("%s_%s", uid, name)
}</span>

// add adds error reason into the cache
func (c *ReasonCache) add(uid types.UID, name string, reason error, message string) <span class="cov8" title="1">{
        c.lock.Lock()
        defer c.lock.Unlock()
        c.cache.Add(c.composeKey(uid, name), ReasonItem{reason, message})
}</span>

func (c *ReasonCache) Add(uid types.UID, name string, reason error, message string) <span class="cov0" title="0">{
        c.lock.Lock()
        defer c.lock.Unlock()
        c.cache.Add(c.composeKey(uid, name), ReasonItem{reason, message})
}</span>

// Update updates the reason cache with the SyncPodResult. Only SyncResult with
// StartContainer action will change the cache.
func (c *ReasonCache) Update(uid types.UID, result kubecontainer.PodSyncResult) <span class="cov8" title="1">{
        for _, r := range result.SyncResults </span><span class="cov8" title="1">{
                if r.Action != kubecontainer.StartContainer </span><span class="cov8" title="1">{
                        continue</span>
                }
                <span class="cov8" title="1">name := r.Target.(string)
                if r.Error != nil </span><span class="cov0" title="0">{
                        c.add(uid, name, r.Error, r.Message)
                }</span> else<span class="cov8" title="1"> {
                        c.Remove(uid, name)
                }</span>
        }
}

// Remove removes error reason from the cache
func (c *ReasonCache) Remove(uid types.UID, name string) <span class="cov8" title="1">{
        c.lock.Lock()
        defer c.lock.Unlock()
        c.cache.Remove(c.composeKey(uid, name))
}</span>

// Get gets error reason from the cache. The return values are error reason, error message and
// whether an error reason is found in the cache. If no error reason is found, empty string will
// be returned for error reason and error message.
func (c *ReasonCache) Get(uid types.UID, name string) (*ReasonItem, bool) <span class="cov8" title="1">{
        c.lock.Lock()
        defer c.lock.Unlock()
        value, ok := c.cache.Get(c.composeKey(uid, name))
        if !ok </span><span class="cov8" title="1">{
                return nil, false
        }</span>
        <span class="cov8" title="1">info := value.(ReasonItem)
        return &amp;info, true</span>
}
</pre>
		
		<pre class="file" id="file15" style="display: none">/*
Copyright 2014 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "context"
        "fmt"
        "os"
        "time"

        v1 "k8s.io/api/core/v1"
        "k8s.io/klog/v2"
        kubecontainer "k8s.io/kubernetes/pkg/kubelet/container"
        kubetypes "k8s.io/kubernetes/pkg/kubelet/types"
        "k8s.io/kubernetes/pkg/kubelet/util/format"
)

const (
        runOnceManifestDelay     = 1 * time.Second
        runOnceMaxRetries        = 10
        runOnceRetryDelay        = 1 * time.Second
        runOnceRetryDelayBackoff = 2
)

// RunPodResult defines the running results of a Pod.
type RunPodResult struct {
        Pod *v1.Pod
        Err error
}

// RunOnce polls from one configuration update and run the associated pods.
func (kl *Kubelet) RunOnce(updates &lt;-chan kubetypes.PodUpdate) ([]RunPodResult, error) <span class="cov0" title="0">{
        ctx := context.Background()
        // Setup filesystem directories.
        if err := kl.setupDataDirs(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // If the container logs directory does not exist, create it.
        <span class="cov0" title="0">if _, err := os.Stat(ContainerLogsDir); err != nil </span><span class="cov0" title="0">{
                if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Failed to create directory", "path", ContainerLogsDir)
                }</span>
        }

        <span class="cov0" title="0">select </span>{
        case u := &lt;-updates:<span class="cov0" title="0">
                klog.InfoS("Processing manifest with pods", "numPods", len(u.Pods))
                result, err := kl.runOnce(ctx, u.Pods, runOnceRetryDelay)
                klog.InfoS("Finished processing pods", "numPods", len(u.Pods))
                return result, err</span>
        case &lt;-time.After(runOnceManifestDelay):<span class="cov0" title="0">
                return nil, fmt.Errorf("no pod manifest update after %v", runOnceManifestDelay)</span>
        }
}

// runOnce runs a given set of pods and returns their status.
func (kl *Kubelet) runOnce(ctx context.Context, pods []*v1.Pod, retryDelay time.Duration) (results []RunPodResult, err error) <span class="cov0" title="0">{
        ch := make(chan RunPodResult)
        admitted := []*v1.Pod{}
        for _, pod := range pods </span><span class="cov0" title="0">{
                // Check if we can admit the pod.
                if ok, reason, message := kl.canAdmitPod(admitted, pod); !ok </span><span class="cov0" title="0">{
                        kl.rejectPod(pod, reason, message)
                        results = append(results, RunPodResult{pod, nil})
                        continue</span>
                }

                <span class="cov0" title="0">admitted = append(admitted, pod)
                go func(pod *v1.Pod) </span><span class="cov0" title="0">{
                        err := kl.runPod(ctx, pod, retryDelay)
                        ch &lt;- RunPodResult{pod, err}
                }</span>(pod)
        }

        <span class="cov0" title="0">klog.InfoS("Waiting for pods", "numPods", len(admitted))
        failedPods := []string{}
        for i := 0; i &lt; len(admitted); i++ </span><span class="cov0" title="0">{
                res := &lt;-ch
                results = append(results, res)
                if res.Err != nil </span><span class="cov0" title="0">{
                        failedContainerName, err := kl.getFailedContainers(ctx, res.Pod)
                        if err != nil </span><span class="cov0" title="0">{
                                klog.InfoS("Unable to get failed containers' names for pod", "pod", klog.KObj(res.Pod), "err", err)
                        }</span> else<span class="cov0" title="0"> {
                                klog.InfoS("Unable to start pod because container failed", "pod", klog.KObj(res.Pod), "containerName", failedContainerName)
                        }</span>
                        <span class="cov0" title="0">failedPods = append(failedPods, format.Pod(res.Pod))</span>
                } else<span class="cov0" title="0"> {
                        klog.InfoS("Started pod", "pod", klog.KObj(res.Pod))
                }</span>
        }
        <span class="cov0" title="0">if len(failedPods) &gt; 0 </span><span class="cov0" title="0">{
                return results, fmt.Errorf("error running pods: %v", failedPods)
        }</span>
        <span class="cov0" title="0">klog.InfoS("Pods started", "numPods", len(pods))
        return results, err</span>
}

// runPod runs a single pod and waits until all containers are running.
func (kl *Kubelet) runPod(ctx context.Context, pod *v1.Pod, retryDelay time.Duration) error <span class="cov0" title="0">{
        var isTerminal bool
        delay := retryDelay
        retry := 0
        for !isTerminal </span><span class="cov0" title="0">{
                status, err := kl.containerRuntime.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("unable to get status for pod %q: %v", format.Pod(pod), err)
                }</span>

                <span class="cov0" title="0">if kl.isPodRunning(pod, status) </span><span class="cov0" title="0">{
                        klog.InfoS("Pod's containers running", "pod", klog.KObj(pod))
                        return nil
                }</span>
                <span class="cov0" title="0">klog.InfoS("Pod's containers not running: syncing", "pod", klog.KObj(pod))

                klog.InfoS("Creating a mirror pod for static pod", "pod", klog.KObj(pod))
                if err := kl.mirrorPodClient.CreateMirrorPod(pod); err != nil </span><span class="cov0" title="0">{
                        klog.ErrorS(err, "Failed creating a mirror pod", "pod", klog.KObj(pod))
                }</span>
                <span class="cov0" title="0">mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)
                if isTerminal, err = kl.SyncPod(ctx, kubetypes.SyncPodUpdate, pod, mirrorPod, status); err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("error syncing pod %q: %v", format.Pod(pod), err)
                }</span>
                <span class="cov0" title="0">if retry &gt;= runOnceMaxRetries </span><span class="cov0" title="0">{
                        return fmt.Errorf("timeout error: pod %q containers not running after %d retries", format.Pod(pod), runOnceMaxRetries)
                }</span>
                // TODO(proppy): health checking would be better than waiting + checking the state at the next iteration.
                <span class="cov0" title="0">klog.InfoS("Pod's containers synced, waiting", "pod", klog.KObj(pod), "duration", delay)
                time.Sleep(delay)
                retry++
                delay *= runOnceRetryDelayBackoff</span>
        }
        <span class="cov0" title="0">return nil</span>
}

// isPodRunning returns true if all containers of a manifest are running.
func (kl *Kubelet) isPodRunning(pod *v1.Pod, status *kubecontainer.PodStatus) bool <span class="cov0" title="0">{
        for _, c := range pod.Spec.Containers </span><span class="cov0" title="0">{
                cs := status.FindContainerStatusByName(c.Name)
                if cs == nil || cs.State != kubecontainer.ContainerStateRunning </span><span class="cov0" title="0">{
                        klog.InfoS("Container not running", "pod", klog.KObj(pod), "containerName", c.Name)
                        return false
                }</span>
        }
        <span class="cov0" title="0">return true</span>
}

// getFailedContainer returns failed container name for pod.
func (kl *Kubelet) getFailedContainers(ctx context.Context, pod *v1.Pod) ([]string, error) <span class="cov0" title="0">{
        status, err := kl.containerRuntime.GetPodStatus(ctx, pod.UID, pod.Name, pod.Namespace)
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("unable to get status for pod %q: %v", format.Pod(pod), err)
        }</span>
        <span class="cov0" title="0">var containerNames []string
        for _, cs := range status.ContainerStatuses </span><span class="cov0" title="0">{
                if cs.State != kubecontainer.ContainerStateRunning &amp;&amp; cs.ExitCode != 0 </span><span class="cov0" title="0">{
                        containerNames = append(containerNames, cs.Name)
                }</span>
        }
        <span class="cov0" title="0">return containerNames, nil</span>
}
</pre>
		
		<pre class="file" id="file16" style="display: none">/*
Copyright 2015 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "errors"
        "fmt"
        "sync"
        "time"

        utilerrors "k8s.io/apimachinery/pkg/util/errors"
)

type runtimeState struct {
        sync.RWMutex
        lastBaseRuntimeSync      time.Time
        baseRuntimeSyncThreshold time.Duration
        networkError             error
        runtimeError             error
        storageError             error
        cidr                     string
        healthChecks             []*healthCheck
}

// A health check function should be efficient and not rely on external
// components (e.g., container runtime).
type healthCheckFnType func() (bool, error)

type healthCheck struct {
        name string
        fn   healthCheckFnType
}

func (s *runtimeState) addHealthCheck(name string, f healthCheckFnType) <span class="cov8" title="1">{
        s.Lock()
        defer s.Unlock()
        s.healthChecks = append(s.healthChecks, &amp;healthCheck{name: name, fn: f})
}</span>

func (s *runtimeState) setRuntimeSync(t time.Time) <span class="cov8" title="1">{
        s.Lock()
        defer s.Unlock()
        s.lastBaseRuntimeSync = t
}</span>

func (s *runtimeState) SetRuntimeSync(t time.Time) <span class="cov0" title="0">{
        s.Lock()
        defer s.Unlock()
        s.lastBaseRuntimeSync = t
}</span>

func (s *runtimeState) setNetworkState(err error) <span class="cov8" title="1">{
        s.Lock()
        defer s.Unlock()
        s.networkError = err
}</span>

func (s *runtimeState) SetNetworkState(err error) <span class="cov0" title="0">{
        s.Lock()
        defer s.Unlock()
        s.runtimeError = err
}</span>

func (s *runtimeState) setRuntimeState(err error) <span class="cov0" title="0">{
        s.Lock()
        defer s.Unlock()
        s.runtimeError = err
}</span>

func (s *runtimeState) SetRuntimeState(err error) <span class="cov0" title="0">{
        s.Lock()
        defer s.Unlock()
        s.runtimeError = err
}</span>

func (s *runtimeState) setStorageState(err error) <span class="cov0" title="0">{
        s.Lock()
        defer s.Unlock()
        s.storageError = err
}</span>

func (s *runtimeState) setPodCIDR(cidr string) <span class="cov0" title="0">{
        s.Lock()
        defer s.Unlock()
        s.cidr = cidr
}</span>

func (s *runtimeState) podCIDR() string <span class="cov8" title="1">{
        s.RLock()
        defer s.RUnlock()
        return s.cidr
}</span>

func (s *runtimeState) runtimeErrors() error <span class="cov8" title="1">{
        s.RLock()
        defer s.RUnlock()
        errs := []error{}
        if s.lastBaseRuntimeSync.IsZero() </span><span class="cov8" title="1">{
                errs = append(errs, errors.New("container runtime status check may not have completed yet"))
        }</span> else<span class="cov8" title="1"> if !s.lastBaseRuntimeSync.Add(s.baseRuntimeSyncThreshold).After(time.Now()) </span><span class="cov0" title="0">{
                errs = append(errs, errors.New("container runtime is down"))
        }</span>
        <span class="cov8" title="1">for _, hc := range s.healthChecks </span><span class="cov0" title="0">{
                if ok, err := hc.fn(); !ok </span><span class="cov0" title="0">{
                        errs = append(errs, fmt.Errorf("%s is not healthy: %v", hc.name, err))
                }</span>
        }
        <span class="cov8" title="1">if s.runtimeError != nil </span><span class="cov0" title="0">{
                errs = append(errs, s.runtimeError)
        }</span>

        <span class="cov8" title="1">return utilerrors.NewAggregate(errs)</span>
}

func (s *runtimeState) networkErrors() error <span class="cov8" title="1">{
        s.RLock()
        defer s.RUnlock()
        errs := []error{}
        if s.networkError != nil </span><span class="cov8" title="1">{
                errs = append(errs, s.networkError)
        }</span>
        <span class="cov8" title="1">return utilerrors.NewAggregate(errs)</span>
}

func (s *runtimeState) storageErrors() error <span class="cov8" title="1">{
        s.RLock()
        defer s.RUnlock()
        errs := []error{}
        if s.storageError != nil </span><span class="cov0" title="0">{
                errs = append(errs, s.storageError)
        }</span>
        <span class="cov8" title="1">return utilerrors.NewAggregate(errs)</span>
}

func newRuntimeState(runtimeSyncThreshold time.Duration) *runtimeState <span class="cov8" title="1">{
        return &amp;runtimeState{
                lastBaseRuntimeSync:      time.Time{},
                baseRuntimeSyncThreshold: runtimeSyncThreshold,
                networkError:             errors.New("network unknown"),
        }
}</span>

// NewRuntimeState creates a new runtimeState
func NewRuntimeState(runtimeSyncThreshold time.Duration) *runtimeState <span class="cov0" title="0">{
        fmt.Println("Inside NewRuntimeState", runtimeSyncThreshold)

        if runtimeSyncThreshold == 0 </span><span class="cov0" title="0">{
                fmt.Println("Warning: runtimeSyncThreshold is zero")
        }</span>
        <span class="cov0" title="0">newState := &amp;runtimeState{
                lastBaseRuntimeSync:      time.Time{},
                baseRuntimeSyncThreshold: runtimeSyncThreshold,
                networkError:             errors.New("network unknown"),
        }

        fmt.Printf("NewRuntimeState created: %+v\n", newState)

        return newState</span>
}
</pre>
		
		<pre class="file" id="file17" style="display: none">/*
Copyright 2016 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package kubelet

import (
        "fmt"
        "net"
        "runtime"

        "k8s.io/klog/v2"
        "k8s.io/mount-utils"
        utilexec "k8s.io/utils/exec"

        authenticationv1 "k8s.io/api/authentication/v1"
        v1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
        "k8s.io/apimachinery/pkg/types"
        "k8s.io/apimachinery/pkg/util/wait"
        "k8s.io/client-go/informers"
        clientset "k8s.io/client-go/kubernetes"
        storagelisters "k8s.io/client-go/listers/storage/v1"
        "k8s.io/client-go/tools/cache"
        "k8s.io/client-go/tools/record"
        cloudprovider "k8s.io/cloud-provider"
        "k8s.io/kubernetes/pkg/kubelet/clustertrustbundle"
        "k8s.io/kubernetes/pkg/kubelet/configmap"
        "k8s.io/kubernetes/pkg/kubelet/secret"
        "k8s.io/kubernetes/pkg/kubelet/token"
        "k8s.io/kubernetes/pkg/volume"
        "k8s.io/kubernetes/pkg/volume/util"
        "k8s.io/kubernetes/pkg/volume/util/hostutil"
        "k8s.io/kubernetes/pkg/volume/util/subpath"
)

// NewInitializedVolumePluginMgr returns a new instance of
// volume.VolumePluginMgr initialized with kubelets implementation of the
// volume.VolumeHost interface.
//
// kubelet - used by VolumeHost methods to expose kubelet specific parameters
// plugins - used to initialize volumePluginMgr
func NewInitializedVolumePluginMgr(
        kubelet *Kubelet,
        secretManager secret.Manager,
        configMapManager configmap.Manager,
        tokenManager *token.Manager,
        clusterTrustBundleManager clustertrustbundle.Manager,
        plugins []volume.VolumePlugin,
        prober volume.DynamicPluginProber) (*volume.VolumePluginMgr, error) <span class="cov8" title="1">{

        // Initialize csiDriverLister before calling InitPlugins
        var informerFactory informers.SharedInformerFactory
        var csiDriverLister storagelisters.CSIDriverLister
        var csiDriversSynced cache.InformerSynced
        const resyncPeriod = 0
        // Don't initialize if kubeClient is nil
        if kubelet.kubeClient != nil </span><span class="cov8" title="1">{
                informerFactory = informers.NewSharedInformerFactory(kubelet.kubeClient, resyncPeriod)
                csiDriverInformer := informerFactory.Storage().V1().CSIDrivers()
                csiDriverLister = csiDriverInformer.Lister()
                csiDriversSynced = csiDriverInformer.Informer().HasSynced

        }</span> else<span class="cov8" title="1"> {
                klog.InfoS("KubeClient is nil. Skip initialization of CSIDriverLister")
        }</span>

        <span class="cov8" title="1">kvh := &amp;kubeletVolumeHost{
                kubelet:                   kubelet,
                volumePluginMgr:           volume.VolumePluginMgr{},
                secretManager:             secretManager,
                configMapManager:          configMapManager,
                tokenManager:              tokenManager,
                clusterTrustBundleManager: clusterTrustBundleManager,
                informerFactory:           informerFactory,
                csiDriverLister:           csiDriverLister,
                csiDriversSynced:          csiDriversSynced,
                exec:                      utilexec.New(),
        }

        if err := kvh.volumePluginMgr.InitPlugins(plugins, prober, kvh); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf(
                        "could not initialize volume plugins for KubeletVolumePluginMgr: %v",
                        err)
        }</span>

        <span class="cov8" title="1">return &amp;kvh.volumePluginMgr, nil</span>
}

// Compile-time check to ensure kubeletVolumeHost implements the VolumeHost interface
var _ volume.VolumeHost = &amp;kubeletVolumeHost{}
var _ volume.KubeletVolumeHost = &amp;kubeletVolumeHost{}

func (kvh *kubeletVolumeHost) GetPluginDir(pluginName string) string <span class="cov0" title="0">{
        return kvh.kubelet.getPluginDir(pluginName)
}</span>

type kubeletVolumeHost struct {
        kubelet                   *Kubelet
        volumePluginMgr           volume.VolumePluginMgr
        secretManager             secret.Manager
        tokenManager              *token.Manager
        configMapManager          configmap.Manager
        clusterTrustBundleManager clustertrustbundle.Manager
        informerFactory           informers.SharedInformerFactory
        csiDriverLister           storagelisters.CSIDriverLister
        csiDriversSynced          cache.InformerSynced
        exec                      utilexec.Interface
}

func (kvh *kubeletVolumeHost) SetKubeletError(err error) <span class="cov0" title="0">{
        kvh.kubelet.runtimeState.setStorageState(err)
}</span>

func (kvh *kubeletVolumeHost) GetVolumeDevicePluginDir(pluginName string) string <span class="cov0" title="0">{
        return kvh.kubelet.getVolumeDevicePluginDir(pluginName)
}</span>

func (kvh *kubeletVolumeHost) GetPodsDir() string <span class="cov0" title="0">{
        return kvh.kubelet.getPodsDir()
}</span>

func (kvh *kubeletVolumeHost) GetPodVolumeDir(podUID types.UID, pluginName string, volumeName string) string <span class="cov0" title="0">{
        dir := kvh.kubelet.getPodVolumeDir(podUID, pluginName, volumeName)
        if runtime.GOOS == "windows" </span><span class="cov0" title="0">{
                dir = util.GetWindowsPath(dir)
        }</span>
        <span class="cov0" title="0">return dir</span>
}

func (kvh *kubeletVolumeHost) GetPodVolumeDeviceDir(podUID types.UID, pluginName string) string <span class="cov0" title="0">{
        return kvh.kubelet.getPodVolumeDeviceDir(podUID, pluginName)
}</span>

func (kvh *kubeletVolumeHost) GetPodPluginDir(podUID types.UID, pluginName string) string <span class="cov0" title="0">{
        return kvh.kubelet.getPodPluginDir(podUID, pluginName)
}</span>

func (kvh *kubeletVolumeHost) GetKubeClient() clientset.Interface <span class="cov0" title="0">{
        return kvh.kubelet.kubeClient
}</span>

func (kvh *kubeletVolumeHost) GetSubpather() subpath.Interface <span class="cov0" title="0">{
        return kvh.kubelet.subpather
}</span>

func (kvh *kubeletVolumeHost) GetHostUtil() hostutil.HostUtils <span class="cov0" title="0">{
        return kvh.kubelet.hostutil
}</span>

func (kvh *kubeletVolumeHost) GetInformerFactory() informers.SharedInformerFactory <span class="cov0" title="0">{
        return kvh.informerFactory
}</span>

func (kvh *kubeletVolumeHost) CSIDriverLister() storagelisters.CSIDriverLister <span class="cov0" title="0">{
        return kvh.csiDriverLister
}</span>

func (kvh *kubeletVolumeHost) CSIDriversSynced() cache.InformerSynced <span class="cov0" title="0">{
        return kvh.csiDriversSynced
}</span>

// WaitForCacheSync is a helper function that waits for cache sync for CSIDriverLister
func (kvh *kubeletVolumeHost) WaitForCacheSync() error <span class="cov0" title="0">{
        if kvh.csiDriversSynced == nil </span><span class="cov0" title="0">{
                klog.ErrorS(nil, "CsiDriversSynced not found on KubeletVolumeHost")
                return fmt.Errorf("csiDriversSynced not found on KubeletVolumeHost")
        }</span>

        <span class="cov0" title="0">synced := []cache.InformerSynced{kvh.csiDriversSynced}
        if !cache.WaitForCacheSync(wait.NeverStop, synced...) </span><span class="cov0" title="0">{
                klog.InfoS("Failed to wait for cache sync for CSIDriverLister")
                return fmt.Errorf("failed to wait for cache sync for CSIDriverLister")
        }</span>

        <span class="cov0" title="0">return nil</span>
}

func (kvh *kubeletVolumeHost) NewWrapperMounter(
        volName string,
        spec volume.Spec,
        pod *v1.Pod,
        opts volume.VolumeOptions) (volume.Mounter, error) <span class="cov0" title="0">{
        // The name of wrapper volume is set to "wrapped_{wrapped_volume_name}"
        wrapperVolumeName := "wrapped_" + volName
        if spec.Volume != nil </span><span class="cov0" title="0">{
                spec.Volume.Name = wrapperVolumeName
        }</span>

        <span class="cov0" title="0">return kvh.kubelet.newVolumeMounterFromPlugins(&amp;spec, pod, opts)</span>
}

func (kvh *kubeletVolumeHost) NewWrapperUnmounter(volName string, spec volume.Spec, podUID types.UID) (volume.Unmounter, error) <span class="cov0" title="0">{
        // The name of wrapper volume is set to "wrapped_{wrapped_volume_name}"
        wrapperVolumeName := "wrapped_" + volName
        if spec.Volume != nil </span><span class="cov0" title="0">{
                spec.Volume.Name = wrapperVolumeName
        }</span>

        <span class="cov0" title="0">plugin, err := kvh.kubelet.volumePluginMgr.FindPluginBySpec(&amp;spec)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov0" title="0">return plugin.NewUnmounter(spec.Name(), podUID)</span>
}

func (kvh *kubeletVolumeHost) GetCloudProvider() cloudprovider.Interface <span class="cov0" title="0">{
        return kvh.kubelet.cloud
}</span>

func (kvh *kubeletVolumeHost) GetMounter(pluginName string) mount.Interface <span class="cov0" title="0">{
        return kvh.kubelet.mounter
}</span>

func (kvh *kubeletVolumeHost) GetHostName() string <span class="cov0" title="0">{
        return kvh.kubelet.hostname
}</span>

func (kvh *kubeletVolumeHost) GetHostIP() (net.IP, error) <span class="cov0" title="0">{
        hostIPs, err := kvh.kubelet.GetHostIPs()
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov0" title="0">return hostIPs[0], err</span>
}

func (kvh *kubeletVolumeHost) GetNodeAllocatable() (v1.ResourceList, error) <span class="cov0" title="0">{
        node, err := kvh.kubelet.getNodeAnyWay()
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("error retrieving node: %v", err)
        }</span>
        <span class="cov0" title="0">return node.Status.Allocatable, nil</span>
}

func (kvh *kubeletVolumeHost) GetSecretFunc() func(namespace, name string) (*v1.Secret, error) <span class="cov0" title="0">{
        if kvh.secretManager != nil </span><span class="cov0" title="0">{
                return kvh.secretManager.GetSecret
        }</span>
        <span class="cov0" title="0">return func(namespace, name string) (*v1.Secret, error) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("not supported due to running kubelet in standalone mode")
        }</span>
}

func (kvh *kubeletVolumeHost) GetConfigMapFunc() func(namespace, name string) (*v1.ConfigMap, error) <span class="cov0" title="0">{
        if kvh.configMapManager != nil </span><span class="cov0" title="0">{
                return kvh.configMapManager.GetConfigMap
        }</span>
        <span class="cov0" title="0">return func(namespace, name string) (*v1.ConfigMap, error) </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("not supported due to running kubelet in standalone mode")
        }</span>
}

func (kvh *kubeletVolumeHost) GetServiceAccountTokenFunc() func(namespace, name string, tr *authenticationv1.TokenRequest) (*authenticationv1.TokenRequest, error) <span class="cov0" title="0">{
        return kvh.tokenManager.GetServiceAccountToken
}</span>

func (kvh *kubeletVolumeHost) DeleteServiceAccountTokenFunc() func(podUID types.UID) <span class="cov0" title="0">{
        return kvh.tokenManager.DeleteServiceAccountToken
}</span>

func (kvh *kubeletVolumeHost) GetTrustAnchorsByName(name string, allowMissing bool) ([]byte, error) <span class="cov0" title="0">{
        return kvh.clusterTrustBundleManager.GetTrustAnchorsByName(name, allowMissing)
}</span>

func (kvh *kubeletVolumeHost) GetTrustAnchorsBySigner(signerName string, labelSelector *metav1.LabelSelector, allowMissing bool) ([]byte, error) <span class="cov0" title="0">{
        return kvh.clusterTrustBundleManager.GetTrustAnchorsBySigner(signerName, labelSelector, allowMissing)
}</span>

func (kvh *kubeletVolumeHost) GetNodeLabels() (map[string]string, error) <span class="cov0" title="0">{
        node, err := kvh.kubelet.GetNode()
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("error retrieving node: %v", err)
        }</span>
        <span class="cov0" title="0">return node.Labels, nil</span>
}

func (kvh *kubeletVolumeHost) GetAttachedVolumesFromNodeStatus() (map[v1.UniqueVolumeName]string, error) <span class="cov0" title="0">{
        node, err := kvh.kubelet.GetNode()
        if err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("error retrieving node: %v", err)
        }</span>
        <span class="cov0" title="0">attachedVolumes := node.Status.VolumesAttached
        result := map[v1.UniqueVolumeName]string{}
        for i := range attachedVolumes </span><span class="cov0" title="0">{
                attachedVolume := attachedVolumes[i]
                result[attachedVolume.Name] = attachedVolume.DevicePath
        }</span>
        <span class="cov0" title="0">return result, nil</span>
}

func (kvh *kubeletVolumeHost) GetNodeName() types.NodeName <span class="cov0" title="0">{
        return kvh.kubelet.nodeName
}</span>

func (kvh *kubeletVolumeHost) GetEventRecorder() record.EventRecorder <span class="cov0" title="0">{
        return kvh.kubelet.recorder
}</span>

func (kvh *kubeletVolumeHost) GetExec(pluginName string) utilexec.Interface <span class="cov0" title="0">{
        return kvh.exec
}</span>
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
